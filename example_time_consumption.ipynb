{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, tensorboard_trace_handler, ProfilerActivity, schedule\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import torch.distributed as dist\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time \n",
    "import os \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,X,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,c_in = 4,h_dim1 = 16, h_dim2 = 16, c_out = 1,device = 'cpu'):\n",
    "        super(MLP,self).__init__()\n",
    "        self.linear1 = nn.Linear(c_in,h_dim1)\n",
    "        self.linear2 = nn.Linear(h_dim1,h_dim2)\n",
    "        self.linear3 = nn.Linear(h_dim2,c_out)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.device = device\n",
    "    def forward(self,x):\n",
    "        return(self.linear3(self.relu(self.linear2(self.relu(self.linear1(x))))))\n",
    "    \n",
    "def load_profile_dataloader(dataset,B,num_workers,persistent_workers,pin_memory,prefetch_factor,drop_last,dataparallel=False,prof = False):\n",
    "    if prof:\n",
    "        activities = [ProfilerActivity.CPU, ProfilerActivity.CUDA] if torch.cuda.is_available() else [ProfilerActivity.CPU]\n",
    "        prof =  profile(activities=activities,\n",
    "                        schedule=schedule(wait=1, warmup=1, active=12, repeat=1),\n",
    "                        on_trace_ready=tensorboard_trace_handler('./profiler/trial_profiler'),\n",
    "                        profile_memory=True,\n",
    "                        record_shapes=False, \n",
    "                        with_stack=False,\n",
    "                        with_flops=False\n",
    "                        )\n",
    "    else:\n",
    "        prof = None\n",
    "    \n",
    "    if dataparallel:\n",
    "        sampler = torch.utils.data.distributed.DistributedSampler(dataset,\n",
    "                                                                num_replicas=2,\n",
    "                                                                rank=0,\n",
    "                                                                shuffle=True)\n",
    "    else: \n",
    "        sampler = None\n",
    "        \n",
    "    \n",
    "\n",
    "    dataloader = DataLoader(dataset,batch_size=B,shuffle =False if dataparallel else True ,\n",
    "                            num_workers=num_workers, \n",
    "                            persistent_workers= False if num_workers == 0 else persistent_workers,\n",
    "                            pin_memory=pin_memory,\n",
    "                            prefetch_factor=None if num_workers==0 else prefetch_factor, #2,3,4,5...\n",
    "                            drop_last=drop_last,\n",
    "                           sampler = sampler \n",
    "                           )\n",
    "    \n",
    "    return(prof,dataloader,sampler)\n",
    "\n",
    "\n",
    "def load_model_loss_opt(c_in,h_dim1,h_dim2,c_out,device,dataparallel = False):\n",
    "    loss = nn.MSELoss()\n",
    "    model = MLP(c_in,h_dim1, h_dim2, c_out,device).to(device)\n",
    "    if dataparallel:\n",
    "        dist.init_process_group(backend='nccl',# init_method='env://',\n",
    "                                world_size=2, \n",
    "                                rank=0)\n",
    "        model = DistributedDataParallel(model,device_ids = [0])\n",
    "    optimizer = torch.optim.SGD(model.parameters(), 1e-3)\n",
    "\n",
    "    return(loss,model,optimizer)\n",
    "\n",
    "\n",
    "def training(model,optimizer,loss,prof,epochs,dataloader,device,scaler):\n",
    "    t_epochs,t_batchs, t_coms, t_forwards, t_backwards,total_time = 0,0,0,0,0,0\n",
    "    for epoch in range(epochs):\n",
    "        if sampler is not None:\n",
    "            sampler.set_epoch(epoch)\n",
    "        epoch1 = time.time()\n",
    "        t_epoch = time.time()\n",
    "        for x,y in dataloader:\n",
    "            t_batch = time.time()\n",
    "            t_com = time.time()\n",
    "            x,y = x.to(device),y.to(device)\n",
    "            t_coms += time.time()-t_com\n",
    "\n",
    "            t_forward = time.time()\n",
    "            pred = model(x)\n",
    "            t_forwards += time.time()-t_forward\n",
    "            \n",
    "            if len(y.size()) != len(pred.size()): \n",
    "                pred = pred.squeeze()\n",
    "\n",
    "            t_backward = time.time()\n",
    "            l = loss(pred,y)\n",
    "            t_backwards += time.time() - t_backward\n",
    "            if scaler is not None:\n",
    "                scaler.scale(l).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                l.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            t_batchs += time.time()-t_batch\n",
    "        t_epochs += time.time()-t_epoch\n",
    "        if epoch == 0:\n",
    "            t_epochs = 0\n",
    "            epochs1 = time.time()-epoch1\n",
    "        if prof:\n",
    "            prof.step()\n",
    "        return(t_epochs,epochs1,t_coms,t_forwards,t_backwards)\n",
    "\n",
    "def train_model(model,optimizer,loss,prof,epochs,dataloader,sampler,device):\n",
    "    total_t = time.time()\n",
    "    if sampler is not None:\n",
    "        scaler = GradScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "    if prof is not None:\n",
    "        with prof:\n",
    "            (t_epochs,epochs1,t_coms,t_forwards,t_backwards) = training(model,optimizer,loss,prof,epochs,dataloader,device,scaler)\n",
    "    else:\n",
    "        (t_epochs,epochs1,t_coms,t_forwards,t_backwards) = training(model,optimizer,loss,prof,epochs,dataloader,device,scaler)\n",
    "        \n",
    "    total_time = time.time() - total_t\n",
    "\n",
    "    print(f\"Total time: {total_time} \\nTime per epoch: {(t_epochs)/(epochs-1)} \\\n",
    "        \\nTime first epoch: {(epochs1)} \\nTime Communication: {t_coms}\\\n",
    "            \\nTime forwards: {t_forwards} \\nTime Backward: {t_backwards}\\\n",
    "            \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test avec different worker, sur model moyen (petit), input shape proche des miens : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Num workers: 0\n",
      "Total time: 0.8179500102996826 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 0.8179304599761963 \n",
      "Time Communication: 0.041181087493896484            \n",
      "Time forwards: 0.15056371688842773 \n",
      "Time Backward: 0.04733085632324219            \n",
      "\n",
      "Num workers: 1\n",
      "Total time: 1.6966233253479004 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 1.6965746879577637 \n",
      "Time Communication: 0.062432289123535156            \n",
      "Time forwards: 0.1364271640777588 \n",
      "Time Backward: 0.045671701431274414            \n",
      "\n",
      "Num workers: 2\n",
      "Total time: 1.8864109516143799 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 1.8863301277160645 \n",
      "Time Communication: 0.12665390968322754            \n",
      "Time forwards: 0.1561121940612793 \n",
      "Time Backward: 0.04915332794189453            \n",
      "\n",
      "Num workers: 4\n",
      "Total time: 1.9492802619934082 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 1.9491872787475586 \n",
      "Time Communication: 0.11153483390808105            \n",
      "Time forwards: 0.15110492706298828 \n",
      "Time Backward: 0.05306577682495117            \n",
      "\n",
      "Num workers: 6\n",
      "Total time: 2.040165662765503 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 2.040065288543701 \n",
      "Time Communication: 0.1405165195465088            \n",
      "Time forwards: 0.15297627449035645 \n",
      "Time Backward: 0.050089359283447266            \n",
      "\n",
      "Num workers: 8\n",
      "Total time: 2.0956063270568848 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 2.0955162048339844 \n",
      "Time Communication: 0.13002610206604004            \n",
      "Time forwards: 0.14043331146240234 \n",
      "Time Backward: 0.04544210433959961            \n"
     ]
    }
   ],
   "source": [
    "B = 8\n",
    "T = 6000\n",
    "L = 8\n",
    "N = 40\n",
    "epochs = 300\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "c_in,h_dim1,h_dim2,c_out = L, 64, 64, 1\n",
    "\n",
    "persistent_workers = False\n",
    "pin_memory = False\n",
    "prefetch_factor = None\n",
    "drop_last = False\n",
    "\n",
    "X,Y=  torch.randn(T,N,L),torch.randn(T,N)\n",
    "#inputs = list(zip(X,Y))\n",
    "inputs = CustomDataset(X,Y)\n",
    "\n",
    "for num_workers in [0,1,2,4,6,8]:\n",
    "    print('\\nNum workers:',num_workers)\n",
    "    (prof,dataloader,sampler) = load_profile_dataloader(inputs,B,num_workers,persistent_workers,pin_memory,prefetch_factor,drop_last)\n",
    "    (loss,model,optimizer) = load_model_loss_opt(c_in,h_dim1,h_dim2,c_out,device)\n",
    "    train_model(model,optimizer,loss,prof,epochs,dataloader,sampler,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essaie avec toute les données chargées initialement en mémoire : \n",
    "Ici, impossible de les charger en mémoire en amont pour du num_worker > 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Num workers: 0\n",
      "Total time: 0.634476900100708 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 0.634458065032959 \n",
      "Time Communication: 0.0035734176635742188            \n",
      "Time forwards: 0.11711645126342773 \n",
      "Time Backward: 0.038048505783081055            \n"
     ]
    }
   ],
   "source": [
    "B = 8\n",
    "T = 6000\n",
    "L = 8\n",
    "N = 40\n",
    "epochs = 300\n",
    "\n",
    "persistent_workers = False\n",
    "pin_memory = False\n",
    "prefetch_factor = None\n",
    "drop_last = False\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "c_in,h_dim1,h_dim2,c_out = L, 64, 64, 1\n",
    "\n",
    "X,Y=  torch.randn(T,N,L).to(device),torch.randn(T,N,1).to(device)\n",
    "inputs = CustomDataset(X,Y)\n",
    "\n",
    "for num_workers in [0]:\n",
    "    print('\\nNum workers:',num_workers)\n",
    "    (prof,dataloader,sampler) = load_profile_dataloader(inputs,B,num_workers,persistent_workers,pin_memory,prefetch_factor,drop_last)\n",
    "    (loss,model,optimizer) = load_model_loss_opt(c_in,h_dim1,h_dim2,c_out,device)\n",
    "    train_model(model,optimizer,loss,prof,epochs,dataloader,sampler,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajout de : persistent_worker, pin_memory, prefetch_factor = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Num workers: 0 and prefetch_factor:  None\n",
      "Total time: 0.7066452503204346 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 0.7066271305084229 \n",
      "Time Communication: 0.03134489059448242            \n",
      "Time forwards: 0.11990141868591309 \n",
      "Time Backward: 0.03914356231689453            \n",
      "\n",
      "Num workers: 0 and prefetch_factor:  2\n",
      "Total time: 0.6967222690582275 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 0.6967051029205322 \n",
      "Time Communication: 0.031247854232788086            \n",
      "Time forwards: 0.1203000545501709 \n",
      "Time Backward: 0.03901362419128418            \n",
      "\n",
      "Num workers: 0 and prefetch_factor:  4\n",
      "Total time: 0.6892411708831787 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 0.6892240047454834 \n",
      "Time Communication: 0.031081199645996094            \n",
      "Time forwards: 0.11827659606933594 \n",
      "Time Backward: 0.0383143424987793            \n",
      "\n",
      "Num workers: 0 and prefetch_factor:  8\n",
      "Total time: 0.6936221122741699 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 0.6936051845550537 \n",
      "Time Communication: 0.030835866928100586            \n",
      "Time forwards: 0.11847186088562012 \n",
      "Time Backward: 0.038513898849487305            \n",
      "\n",
      "Num workers: 1 and prefetch_factor:  None\n",
      "Total time: 1.2808942794799805 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 1.2808711528778076 \n",
      "Time Communication: 0.10613560676574707            \n",
      "Time forwards: 0.142730712890625 \n",
      "Time Backward: 0.04854464530944824            \n",
      "\n",
      "Num workers: 1 and prefetch_factor:  2\n",
      "Total time: 1.2782480716705322 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 1.278212308883667 \n",
      "Time Communication: 0.043892860412597656            \n",
      "Time forwards: 0.14061403274536133 \n",
      "Time Backward: 0.049520015716552734            \n",
      "\n",
      "Num workers: 1 and prefetch_factor:  4\n",
      "Total time: 1.2846832275390625 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 1.2846577167510986 \n",
      "Time Communication: 0.10344767570495605            \n",
      "Time forwards: 0.14364266395568848 \n",
      "Time Backward: 0.0494537353515625            \n",
      "\n",
      "Num workers: 1 and prefetch_factor:  8\n",
      "Total time: 1.3007986545562744 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 1.3007662296295166 \n",
      "Time Communication: 0.07886457443237305            \n",
      "Time forwards: 0.14497876167297363 \n",
      "Time Backward: 0.051529645919799805            \n"
     ]
    }
   ],
   "source": [
    "persistent_workers = True\n",
    "pin_memory = True\n",
    "drop_last = True\n",
    "\n",
    "X,Y=  torch.randn(T,N,L),torch.randn(T,N,1)\n",
    "inputs = CustomDataset(X,Y)\n",
    "\n",
    "for num_workers in [0,1]:\n",
    "    for prefetch_factor in [None,2,4,8]:\n",
    "        print('\\nNum workers:',num_workers,'and prefetch_factor: ',prefetch_factor)\n",
    "        (prof,dataloader,sampler) = load_profile_dataloader(inputs,B,num_workers,persistent_workers,pin_memory,prefetch_factor,drop_last)\n",
    "        (loss,model,optimizer) = load_model_loss_opt(c_in,h_dim1,h_dim2,c_out,device)\n",
    "        train_model(model,optimizer,loss,prof,epochs,dataloader,sampler,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of best config: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers= 1 #4\n",
    "persistent_workers = True\n",
    "pin_memory = True\n",
    "prefetch_factor = 4\n",
    "drop_last = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial with Dataparallel : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "\n",
    "def find_free_port():\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.bind(('', 0))\n",
    "    addr, port = s.getsockname()\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "free_port = find_free_port()\n",
    "os.environ['MASTER_ADDR'] = '137.121.170.69'\n",
    "os.environ['MASTER_PORT'] = 8888 #Ne fonctionne pas #str(free_port) \n",
    "print(f\"Using port {free_port} for MASTER_PORT\")\n",
    "\n",
    "dataparallel = True\n",
    "\n",
    "(prof,dataloader,sampler) = load_profile_dataloader(inputs,B,num_workers,persistent_workers,pin_memory,prefetch_factor,drop_last,dataparallel)\n",
    "(loss,model,optimizer) = load_model_loss_opt(c_in,h_dim1,h_dim2,c_out,device,dataparallel)\n",
    "train_model(model,optimizer,loss,prof,epochs,dataloader,sampler,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-2.0.1_py-3.10.5]",
   "language": "python",
   "name": "conda-env-pytorch-2.0.1_py-3.10.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
