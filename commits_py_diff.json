[
{"commit":"699384692fca79a8cb0acf8daedb8ff9f1fa6ed5","author":"Fr-ocasting","date":"2025-04-24T14:53:35+02:00","message":"Changement de 'blocks' qui était pris en compte dans l'HP tuning mais pas dans les revalidation..."},"diff":"diff --git a/HP_tuning/hyperparameter_tuning_ray.py b/HP_tuning/hyperparameter_tuning_ray.py\n
index 1377354..78a1e26 100644\n
--- a/HP_tuning/hyperparameter_tuning_ray.py\n
+++ b/HP_tuning/hyperparameter_tuning_ray.py\n
@@ -19 +19,2 @@ from HP_tuning.ray_config import get_ray_config\n
-from high_level_DL_method import load_model,load_optimizer_and_scheduler\n
+from high_level_DL_method import load_optimizer_and_scheduler\n
+from dl_models.full_model import full_model\n
@@ -86 +87 @@ def load_trainer(config, dataset, args):\n
-    model = load_model(dataset, args)\n
+    model = full_model(dataset, args).to(args.device)\n
diff --git a/K_fold_validation/K_fold_validation.py b/K_fold_validation/K_fold_validation.py\n
index 53cee98..7cb87f1 100644\n
--- a/K_fold_validation/K_fold_validation.py\n
+++ b/K_fold_validation/K_fold_validation.py\n
@@ -113,0 +114,2 @@ class KFoldSplitter(object):\n
+        print(f'----------------------------------------')\n
+        print(f'Loading the initial dataset for K-fold splitting')\n
@@ -115 +116,0 @@ class KFoldSplitter(object):\n
-        #print('Considered Spatial-Unit: ',args.set_spatial_units)\n
@@ -127,0 +129,2 @@ class KFoldSplitter(object):\n
+            print(f'\\n----------------------------------------')\n
+            print(f'Loading the dataset for fold n°{k}')\n
diff --git a/build_inputs/load_contextual_data.py b/build_inputs/load_contextual_data.py\n
index 88d755b..85f9b3a 100644\n
--- a/build_inputs/load_contextual_data.py\n
+++ b/build_inputs/load_contextual_data.py\n
@@ -85 +85 @@ def tackle_input_data(invalid_dates,intersect_coverage_period,args,normalize):\n
-        print('>>>Tackle dataset_name:',dataset_name)\n
+        print('\\n>>>Tackle Contextual dataset: ',dataset_name)\n
@@ -206,2 +205,0 @@ def tackle_contextual(target_ds,invalid_dates,intersect_coverage_period,args,nor\n
-        print('vision_input_type', args.vision_input_type)\n
-        print('vision_model_name', args.vision_model_name)\n
@@ -236,0 +235,2 @@ def tackle_contextual(target_ds,invalid_dates,intersect_coverage_period,args,nor\n
+                print('   vision_input_type', args.vision_input_type)\n
+                print('   vision_model_name', args.vision_model_name)\n
@@ -239,0 +240,5 @@ def tackle_contextual(target_ds,invalid_dates,intersect_coverage_period,args,nor\n
+        print(f\"   Init Dataset: '{[c_i.raw_values.size()for _,c_i in contextual_ds.items()]}. {[torch.isnan(c_i.raw_values).sum()for _,c_i in contextual_ds.items()]} Nan values\")\n
+        print('   TRAIN contextual_ds:',[c_i.U_train.size() for _,c_i in contextual_ds.items()])\n
+        print('   VALID contextual_ds:',[c_i.U_valid.size() for  _,c_i in contextual_ds.items()]) if hasattr(target_ds,'U_valid') else None\n
+        print('   TEST contextual_ds:',[c_i.U_test.size() for  _,c_i in contextual_ds.items()]) if hasattr(target_ds,'U_test') else None\n
+\n
@@ -244,0 +250,2 @@ def tackle_contextual(target_ds,invalid_dates,intersect_coverage_period,args,nor\n
+\n
+\n
diff --git a/build_inputs/load_datasets_to_predict.py b/build_inputs/load_datasets_to_predict.py\n
index d451741..4d0f6ac 100644\n
--- a/build_inputs/load_datasets_to_predict.py\n
+++ b/build_inputs/load_datasets_to_predict.py\n
@@ -48 +48 @@ def get_intersect_of_coverage_periods(args,coverage_period):\n
-    for ds_name in args.dataset_for_coverage:\n
+    for ds_name in (args.dataset_for_coverage + [ds_name for ds_name in args.dataset_names if not ds_name in args.dataset_for_coverage]):\n
@@ -66,0 +67 @@ def get_intersect_of_coverage_periods(args,coverage_period):\n
+    print(f\"Coverage Period: {len(intersect_coverage_period)} elts between {min(intersect_coverage_period)} and {max(intersect_coverage_period)}\") \n
@@ -82,0 +84 @@ def load_datasets_to_predict(args,coverage_period,normalize=True):\n
+    print('\\n>>>Tackle Target dataset:',args.target_data)\n
@@ -91 +93,4 @@ def load_datasets_to_predict(args,coverage_period,normalize=True):\n
-    print(f\"\\nInit Dataset: '{preprocessed_ds.raw_values.size()} with {preprocessed_ds.raw_values.numel()} Total nb of elements and {torch.isnan(preprocessed_ds.raw_values).sum()} Nan values\")\n
+    print(f\"   Init Dataset: '{preprocessed_ds.raw_values.size()}. {torch.isnan(preprocessed_ds.raw_values).sum()} Nan values\")\n
+    print('   TRAIN contextual_ds:',preprocessed_ds.U_train.size())\n
+    print('   VALID contextual_ds:',preprocessed_ds.U_valid.size()) if hasattr(preprocessed_ds,'U_valid') else None\n
+    print('   TEST contextual_ds:',preprocessed_ds.U_test.size()) if hasattr(preprocessed_ds,'U_test') else None\n
diff --git a/build_inputs/load_preprocessed_dataset.py b/build_inputs/load_preprocessed_dataset.py\n
index 5012925..c046dd4 100644\n
--- a/build_inputs/load_preprocessed_dataset.py\n
+++ b/build_inputs/load_preprocessed_dataset.py\n
@@ -189 +189 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-\n
+        \n
@@ -232,3 +231,0 @@ def load_complete_ds(args,coverage_period = None,normalize = True):\n
-    print('target_ds.U_valid',target_ds.U_valid.size())\n
-    if contextual_ds is not None:\n
-        print('contextual_ds.U_valid:',[contextual_ds_i.U_valid.size() for  name_i,contextual_ds_i in contextual_ds.items()])\n
diff --git a/constants/config.py b/constants/config.py\n
index 3b9c532..8596ae6 100644\n
--- a/constants/config.py\n
+++ b/constants/config.py\n
@@ -232,3 +231,0 @@ def update_modif(args,name_gpu='cuda'):\n
-    \n
-    print(f\">>>>Model: {args.model_name}; K_fold = {args.K_fold}; Loss function: {args.loss_function_type} \") \n
-    print(\">>>> Prediction sur une UNIQUE STATION et non pas les 40 \") if args.single_station else None\n
diff --git a/dl_models/STGCN/get_gso.py b/dl_models/STGCN/get_gso.py\n
index 7213473..43af510 100644\n
--- a/dl_models/STGCN/get_gso.py\n
+++ b/dl_models/STGCN/get_gso.py\n
@@ -52,0 +53,9 @@ def get_output_kernel_size(args):\n
+def load_blocks(stblock_num,temporal_h_dim,spatial_h_dim,output_h_dim):\n
+    blocks = []\n
+    blocks.append([1])\n
+    for l in range(stblock_num):\n
+        blocks.append([temporal_h_dim, spatial_h_dim, temporal_h_dim])\n
+    blocks.append([output_h_dim])\n
+    return(blocks)\n
+\n
+\n
@@ -54 +63,2 @@ def get_block_dims(args,Ko):\n
-    blocks = args.blocks.copy()\n
+    blocks = load_blocks(args.stblock_num,args.temporal_h_dim, args.spatial_h_dim,args.output_h_dim)\n
+    #blocks = args.blocks.copy()\n
@@ -65,2 +75,3 @@ def get_block_dims(args,Ko):\n
-    args.blocks[0][0] = args.C\n
-    return(blocks)\n
+    #blocks[0][0] = args.C\n
+    args.blocks = blocks\n
+    return(args)\n
diff --git a/dl_models/STGCN/load_config.py b/dl_models/STGCN/load_config.py\n
index 41a6b62..8405fd6 100644\n
--- a/dl_models/STGCN/load_config.py\n
+++ b/dl_models/STGCN/load_config.py\n
@@ -66,0 +67 @@ args = parser.parse_args(args=[])\n
+\"\"\"\n
@@ -77,0 +79 @@ args.blocks = blocks\n
+\"\"\"\n
diff --git a/dl_models/full_model.py b/dl_models/full_model.py\n
index 1172072..1d9fce0 100644\n
--- a/dl_models/full_model.py\n
+++ b/dl_models/full_model.py\n
@@ -370 +369,0 @@ def load_model(dataset, args):\n
-\n
@@ -435,2 +434 @@ def load_model(dataset, args):\n
-        blocks = get_block_dims(args,Ko)\n
-\n
+        args = get_block_dims(args,Ko)\n
@@ -438 +436 @@ def load_model(dataset, args):\n
-        model = STGCN(args,gso=gso, blocks = blocks,Ko = Ko).to(args.device)\n
+        model = STGCN(args,gso=gso, blocks = args.blocks,Ko = Ko).to(args.device)\n
diff --git a/examples/benchmark.py b/examples/benchmark.py\n
index 2a5a00c..689b9bd 100644\n
--- a/examples/benchmark.py\n
+++ b/examples/benchmark.py\n
@@ -14 +14,2 @@ from K_fold_validation.K_fold_validation import KFoldSplitter\n
-from high_level_DL_method import load_model,load_optimizer_and_scheduler\n
+from high_level_DL_method import load_optimizer_and_scheduler\n
+from dl_models.full_model import load_model\n
@@ -46,0 +48 @@ def local_get_args(model_name,args_init,dataset_names,dataset_for_coverage,modif\n
+    \"\"\"\n
@@ -48 +50 @@ def local_get_args(model_name,args_init,dataset_names,dataset_for_coverage,modif\n
-        from dl_models.STGCN.load_config import load_blocks\n
+        from dl_models.STGCN.get_gso import load_blocks\n
@@ -50,0 +53 @@ def local_get_args(model_name,args_init,dataset_names,dataset_for_coverage,modif\n
+    \"\"\"\n
@@ -70,0 +74,2 @@ def train_on_ds(ds,args,trial_id,save_folder,df_loss):\n
+    print(f\">>>>Model: {args.model_name}; K_fold = {args.K_fold}; Loss function: {args.loss_function_type} \") \n
+    print(\">>>> Prediction sur une UNIQUE STATION et non pas les 40 \") if args.single_station else None\n
diff --git a/examples/load_best_config.py b/examples/load_best_config.py\n
index e2da85d..5f6d4db 100644\n
--- a/examples/load_best_config.py\n
+++ b/examples/load_best_config.py\n
@@ -17 +17,2 @@ from constants.paths import SAVE_DIRECTORY\n
-from high_level_DL_method import load_model,load_optimizer_and_scheduler\n
+from high_level_DL_method import load_optimizer_and_scheduler\n
+from dl_models.full_model import full_model\n
@@ -95 +96 @@ def get_trainer_and_ds_from_saved_trial(trial_id,add_name_id,save_folder,modific\n
-    model = load_model(ds, args)\n
+    model = full_model(ds, args).to(args.device)\n
diff --git a/examples/test_tmps.py b/examples/test_tmps.py\n
index 588199e..57a4417 100644\n
--- a/examples/test_tmps.py\n
+++ b/examples/test_tmps.py\n
@@ -18 +18 @@ from examples.train_model_on_k_fold_validation import train_model_on_k_fold_vali\n
-if True:\n
+if False:\n
@@ -134,2 +134,2 @@ if False:\n
-if False: \n
-    save_folder = 'K_fold_validation/training_with_HP_tuning/re_validation'\n
+if True: \n
+    save_folder = 'K_fold_validation/training_with_HP_tuning/subway_in_subway_out'\n
@@ -142,0 +143,6 @@ if False:\n
+                    'target_data':'subway_in',\n
+                    'use_target_as_context': False,\n
+                    'freq':'15min',\n
+                    'minmaxnorm':True,\n
+                    'standardize': False,\n
+                    'learnable_adj_matrix' : False,\n
@@ -144,3 +149,0 @@ if False:\n
-    \n
-    L_epsilon = ['station_epsilon100','station_epsilon300']\n
-    L_Apps = ['Google_Maps','Instagram','Deezer']\n
@@ -149,9 +152 @@ if False:\n
-    Combination_Apps,CombinationTags = [],[]\n
-    for i in range(len(L_Apps)):\n
-        Combination_Apps = Combination_Apps+[list(x) for x in itertools.combinations(L_Apps,i+1)]\n
-    for i in range(len(L_epsilon)):\n
-        CombinationTags = CombinationTags +[list(x) for x in itertools.combinations(L_epsilon,i+1)]\n
-\n
-    for NetMob_selected_apps,NetMob_selected_tags in list(itertools.product(Combination_Apps,CombinationTags)):\n
-        name_config = f\"NETMOB_eps{'_'.join([x.split('epsilon')[-1] for x in NetMob_selected_tags])}_{'_'.join(NetMob_selected_apps)}\"\n
-        config_diffs.update({name_config:{'dataset_names':['subway_in','netmob_POIs'],\n
+    config_diffs.update({'subway_in_subway_out':{'dataset_names':['subway_in','subway_out'],\n
@@ -160,5 +154,0 @@ if False:\n
-                                            'freq':'15min',\n
-                                            'NetMob_selected_apps':  NetMob_selected_apps,\n
-                                            'NetMob_transfer_mode' :  ['DL'],\n
-                                            'NetMob_selected_tags': NetMob_selected_tags,\n
-                                            'NetMob_expanded' : '',\n
@@ -173,15 +163,6 @@ if False:\n
-    for add_name_id,config_diff_i in config_diffs.items():\n
-        config_diff_i.update(modification)\n
-        if False:\n
-            trainer,args,training_mode_list,metric_list = train_valid_1_model(args,trial_id,save_folder,modification=config_diff_i)\n
-\n
-            # Keep track on metrics :\n
-            df_metrics_per_config.index = [f'{training_mode}_{metric}' for training_mode in training_mode_list for metric in metric_list]\n
-            df_metrics_per_config[add_name_id] = [trainer.performance[f'{training_mode}_metrics'][metric] for training_mode in training_mode_list for metric in metric_list]\n
-\n
-            df_metrics_per_config.to_csv('../save/results/NetMob_as_Channel.csv')\n
-        if True:\n
-            train_model_on_k_fold_validation(trial_id,load_config =True,\n
-                                    save_folder=save_folder,\n
-                                    modification=config_diff_i,\n
-                                    add_name_id=add_name_id)\n
+    for add_name_id,config_diff in config_diffs.items():\n
+        config_diff.update(modification)\n
+        train_model_on_k_fold_validation(trial_id,load_config =True,\n
+                                            save_folder=save_folder,\n
+                                            modification=config_diff,\n
+                                            add_name_id=add_name_id)\n
diff --git a/examples/train_model_on_k_fold_validation.py b/examples/train_model_on_k_fold_validation.py\n
index 0f66a40..1b372bb 100644\n
--- a/examples/train_model_on_k_fold_validation.py\n
+++ b/examples/train_model_on_k_fold_validation.py\n
@@ -16 +16,2 @@ from trainer import Trainer\n
-from high_level_DL_method import load_model,load_optimizer_and_scheduler\n
+from high_level_DL_method import load_optimizer_and_scheduler\n
+from dl_models.full_model import full_model\n
@@ -87 +88 @@ def load_trainer(args,trial_id,save_folder=None,modification={},fold_to_evaluate\n
-    model = load_model(ds, args)\n
+    model = full_model(ds, args).to(args.device)\n
diff --git a/high_level_DL_method.py b/high_level_DL_method.py\n
index 5060e83..4c2e15c 100644\n
--- a/high_level_DL_method.py\n
+++ b/high_level_DL_method.py\n
@@ -9,6 +8,0 @@ from K_fold_validation.K_fold_validation import KFoldSplitter\n
-def load_model(dataset, args):\n
-    model = full_model(dataset, args).to(args.device)\n
-    print('number of total parameters: {}'.format(sum([p.numel() for p in model.parameters()])))\n
-    print('number of trainable parameters: {}'.format(sum([p.numel() for p in model.parameters() if p.requires_grad])))\n
-    return(model)\n
-\n
@@ -32,2 +26,4 @@ def load_everything(args):\n
-    model = load_model(subway_ds,args)\n
-\n
+    model = full_model(subway_ds, args).to(args.device)\n
+    print('number of total parameters: {}'.format(sum([p.numel() for p in model.parameters()])))\n
+    print('number of trainable parameters: {}'.format(sum([p.numel() for p in model.parameters() if p.requires_grad])))\n
+    \n
diff --git a/load_inputs/subway_in.py b/load_inputs/subway_in.py\n
index cc34afe..45a4218 100644\n
--- a/load_inputs/subway_in.py\n
+++ b/load_inputs/subway_in.py\n
@@ -74 +74 @@ def load_DataSet(args,ROOT,FOLDER_PATH,coverage_period = None,filename=None):\n
-        print('Considered Spatial-Unit: ',args.set_spatial_units)\n
+        print('   Number of Considered Spatial-Unit: ',len(args.set_spatial_units))\n
@@ -102 +102,11 @@ def load_subway_in_df(args,ROOT,FOLDER_PATH,filename,coverage_period):\n
-    df = pd.read_csv(f\"{ROOT}/{FOLDER_PATH}/{filename}.csv\",index_col = 0)\n
+    \n
+    print(f\"   Load data from: {ROOT}/{FOLDER_PATH}/{filename}.csv\")\n
+    try:\n
+        df = pd.read_csv(f\"{ROOT}/{FOLDER_PATH}/{filename}.csv\",index_col = 0)\n
+    except FileNotFoundError:\n
+        print(f\"   ERROR : File {ROOT}/{FOLDER_PATH}/{filename}.csv has not been found.\")\n
+        return None\n
+    except Exception as e:\n
+        print(f\"   ERROR while loading {ROOT}/{FOLDER_PATH}/{filename}.csv: {e}\")\n
+        return None\n
+\n
diff --git a/load_inputs/subway_indiv.py b/load_inputs/subway_indiv.py\n
index 788631b..4af165f 100644\n
--- a/load_inputs/subway_indiv.py\n
+++ b/load_inputs/subway_indiv.py\n
@@ -27 +27 @@ START = '2019-11-01' # Exemple basé sur head()\n
-END = '2020-04-01'\n
+END = '2020-04-30 23:30:00'\n
@@ -58 +58 @@ def load_data(ROOT, FOLDER_PATH, invalid_dates, coverage_period, args, normalize\n
-    print(f\"Chargement des données depuis : {data_file_path}\")\n
+    print(f\"   Load data from: {data_file_path}\")\n
@@ -62,2 +62,2 @@ def load_data(ROOT, FOLDER_PATH, invalid_dates, coverage_period, args, normalize\n
-        print(f\"ERREUR : Le fichier {data_file_path} n'a pas été trouvé.\")\n
-        print(f\"Vérifiez que la fréquence '{target_freq}' existe pour {FILE_BASE_NAME} et que les chemins sont corrects.\")\n
+        print(f\"   ERROR : File {data_file_path} has not been found.\")\n
+        print(f\"   Check if '{target_freq}' exists in {FILE_BASE_NAME} and than paths are well set.\")\n
@@ -66 +66 @@ def load_data(ROOT, FOLDER_PATH, invalid_dates, coverage_period, args, normalize\n
-        print(f\"ERREUR lors du chargement du fichier {file_name}.csv: {e}\")\n
+        print(f\"   ERROR while loading {file_name}.csv: {e}\")\n
@@ -70,0 +71 @@ def load_data(ROOT, FOLDER_PATH, invalid_dates, coverage_period, args, normalize\n
+        \n
@@ -72,0 +74,4 @@ def load_data(ROOT, FOLDER_PATH, invalid_dates, coverage_period, args, normalize\n
+        reindex = pd.date_range(start=START, end=END, freq=args.freq)[:-1]\n
+        df = df.reindex(reindex).fillna(0)\n
+\n
+\n
@@ -76,2 +81,2 @@ def load_data(ROOT, FOLDER_PATH, invalid_dates, coverage_period, args, normalize\n
-             print(f\"ERROR : Not any remainig data in {file_name}.csv\")\n
-             print(f\"Check the current coverage period on the trial: ({min(coverage_period)} - {max(coverage_period)})\")\n
+             print(f\"   ERROR : Not any remainig data in {file_name}.csv\")\n
+             print(f\"   Check the current coverage period on the trial: ({min(coverage_period)} - {max(coverage_period)})\")\n
@@ -79 +84 @@ def load_data(ROOT, FOLDER_PATH, invalid_dates, coverage_period, args, normalize\n
-                print(f\"And the maximum coverage period of {file_name}: ({df.index.min()} - {df.index.max()})\")\n
+                print(f\"   And the maximum coverage period of {file_name}: ({df.index.min()} - {df.index.max()})\")\n
@@ -81 +86 @@ def load_data(ROOT, FOLDER_PATH, invalid_dates, coverage_period, args, normalize\n
-                print(f\"DataFrame df is empty, no data in {file_name}.csv\")\n
+                print(f\"   DataFrame df is empty, no data in {file_name}.csv\")\n
@@ -85 +90 @@ def load_data(ROOT, FOLDER_PATH, invalid_dates, coverage_period, args, normalize\n
-        print(f\"ERREUR: Colonne manquante dans {file_name}.csv : {e}. Vérifiez DATE_COL, LOCATION_COL, VALUE_COL.\")\n
+        print(f\"   ERROR: Missing column within {file_name}.csv : {e}. Check if columns DATE_COL, LOCATION_COL, VALUE_COL exists.\")\n
@@ -88 +93 @@ def load_data(ROOT, FOLDER_PATH, invalid_dates, coverage_period, args, normalize\n
-        print(f\"ERREUR pendant le prétraitement des données {file_name}.csv: {e}\")\n
+        print(f\"   ERROR while pre-processing {file_name}.csv: {e}\")\n
@@ -94 +99 @@ def load_data(ROOT, FOLDER_PATH, invalid_dates, coverage_period, args, normalize\n
-    print(\"Création et prétraitement de l'objet PersonnalInput...\")\n
+    #print(\"   Création et prétraitement de l'objet PersonnalInput...\")\n
@@ -104 +109 @@ def load_data(ROOT, FOLDER_PATH, invalid_dates, coverage_period, args, normalize\n
-    print(f\"Chargement et prétraitement de {FILE_BASE_NAME} terminés.\")\n
+    #print(f\"   Chargement et prétraitement de {FILE_BASE_NAME} terminés.\")\n
diff --git a/profiler/chrono.py b/profiler/chrono.py\n
index 6e22b67..e5a0c01 100644\n
--- a/profiler/chrono.py\n
+++ b/profiler/chrono.py\n
@@ -229,15 +228,0 @@ class Chronometer:\n
-        if self.stop_proc and self.start_proc: print(\">>> Training complete in: \" + str(self.stop_proc - self.start_proc))\n
-        if len(self.time_perf_train) > 0: print(\">>> Training performance time: min {} avg {} seconds (+/- {})\".format(np.min(self.time_perf_train[1:]), np.median(self.time_perf_train[1:]), np.std(self.time_perf_train[1:])))\n
-        if len(self.time_perf_load) > 0: print(\">>> Loading performance time: min {} avg {} seconds (+/- {})\".format(np.min(self.time_perf_load[1:]), np.mean(self.time_perf_load[1:]), np.std(self.time_perf_load[1:])))\n
-        if len(self.time_perf_forward) > 0: print(\">>> Forward performance time: {} seconds (+/- {})\".format(np.mean(self.time_perf_forward[1:]), np.std(self.time_perf_forward[1:])))\n
-        if len(self.time_perf_backward) > 0: print(\">>> Backward performance time: {} seconds (+/- {})\".format(np.mean(self.time_perf_backward[1:]), np.std(self.time_perf_backward[1:])))\n
-\n
-        if len(self.time_plotting) > 0: print(\">>> Plotting performance time: {} seconds (+/- {})\".format(np.mean(self.time_plotting[1:]), np.std(self.time_plotting[1:])))\n
-        if len(self.time_saving_model) > 0: print(\">>> Saving performance time: {} seconds (+/- {})\".format(np.mean(self.time_saving_model[1:]), np.std(self.time_saving_model[1:])))\n
-        if len(self.time_tracking_pi) > 0: print(\">>> PI-tracking performance time: {} seconds (+/- {})\".format(np.mean(self.time_tracking_pi[1:]), np.std(self.time_tracking_pi[1:])))\n
-        if len(self.time_scheduler) > 0: print(\">>> Scheduler-update performance time: {} seconds (+/- {})\".format(np.mean(self.time_scheduler[1:]), np.std(self.time_scheduler[1:])))\n
-\n
-        #if len(self.power) > 0: print(\">>> Peak Power during training: {} W)\".format(np.max(self.power)))\n
-        if self.val_time: print(\">>> Validation time: {}\".format(self.val_time))\n
-\n
-\n
@@ -253 +238 @@ class Chronometer:\n
-        total_names = ['Loading','Forward','Backward','Plotting','CheckPoint Saving','Tracking PI','Update Scheduler','Read all data on GPU']\n
+        sum_total_times = np.sum(total_times)\n
@@ -254,0 +240,13 @@ class Chronometer:\n
+        total_prop = {'time_perf_train':np.sum(self.time_perf_train)/sum_total_times if len(self.time_perf_train) >0 else 0,\n
+                       'time_perf_load':np.sum(self.time_perf_load)/sum_total_times if len(self.time_perf_load) >0 else 0,\n
+                       'time_perf_forward':np.sum(self.time_perf_forward)/sum_total_times if len(self.time_perf_forward) >0 else 0,\n
+                       'time_perf_backward':np.sum(self.time_perf_backward)/sum_total_times if len(self.time_perf_backward) >0 else 0,\n
+                       'time_plotting':np.sum(self.time_plotting)/sum_total_times if len(self.time_plotting) >0 else 0,\n
+                       'time_saving_model':np.sum(self.time_saving_model)/sum_total_times if len(self.time_saving_model) >0 else 0,\n
+                       'time_tracking_pi':np.sum(self.time_tracking_pi)/sum_total_times if len(self.time_tracking_pi) >0 else 0,\n
+                       'time_scheduler':np.sum(self.time_scheduler)/sum_total_times if len(self.time_scheduler) >0 else 0,\n
+                       'time_prefetch':np.sum(self.time_prefetch)/sum_total_times if len(self.time_prefetch) >0 else 0,\n
+                       }\n
+\n
+        \"\"\"\n
+        total_names = ['Loading','Forward','Backward','Plotting','CheckPoint Saving','Tracking PI','Update Scheduler','Read all data on GPU']\n
@@ -257,0 +256 @@ class Chronometer:\n
+        \"\"\"\n
@@ -259,0 +259,32 @@ class Chronometer:\n
+        if self.stop_proc and self.start_proc: print(\">>> Training complete in: \" + str(self.stop_proc - self.start_proc))\n
+        if len(self.time_perf_train) > 1: print(\">>> Training ({:.2%}) performance time: min {:.2f} avg {:.2e} seconds (+/- {:.2e})\".format(total_prop['time_perf_train'],np.min(self.time_perf_train[1:]), np.median(self.time_perf_train[1:]), np.std(self.time_perf_train[1:])))\n
+        if len(self.time_perf_train) == 1: print(\">>> Training ({:.2%}) performance time: min {:.2f}\".format(total_prop['time_perf_train'],np.min(self.time_perf_train[:])))\n
+        if len(self.time_perf_load) > 1: print(\">>> Loading ({:.2%}) performance time: min {:.2f} avg {:.2e} seconds (+/- {:.2e})\".format(total_prop['time_perf_load'],np.min(self.time_perf_load[1:]), np.mean(self.time_perf_load[1:]), np.std(self.time_perf_load[1:])))\n
+        if len(self.time_perf_load) == 1: print(\">>> Loading ({:.2%}) performance time: min {:.2f}\".format(total_prop['time_perf_load'],np.min(self.time_perf_load[:])))\n
+\n
+        dic_correspondance = {'time_perf_forward': 'Forward',\n
+                              'time_perf_backward': 'Backward',\n
+                              'time_plotting': 'Plotting',\n
+                              'time_saving_model': 'Saving',\n
+                              'time_tracking_pi': 'PI-tracking',\n
+                              'time_scheduler': 'Scheduler update'}\n
+        \n
+        for attr, name in dic_correspondance.items():\n
+            if (len(getattr(self, attr)) > 1): \n
+                print(f\">>> {name}  ({'{:.2%}'.format(total_prop[attr])}) performance time: {'{:.2e}'.format(np.mean(getattr(self, attr)[1:]))} seconds (+/- {'{:.2e}'.format(np.std(getattr(self, attr)[1:]))})\")\n
+            if (len(getattr(self, attr)) == 1):\n
+                print(f\">>> {name}  ({'{:.2%}'.format(total_prop[attr])}) performance time: {'{:.2e}'.format(getattr(self, attr)[0])} seconds\")\n
+\n
+\n
+        \"\"\"\n
+        if len(self.time_perf_forward) > 0: print(\">>> Forward performance time: {} seconds (+/- {})\".format(np.mean(self.time_perf_forward[1:]), np.std(self.time_perf_forward[1:])))\n
+        if len(self.time_perf_backward) > 0: print(\">>> Backward performance time: {} seconds (+/- {})\".format(np.mean(self.time_perf_backward[1:]), np.std(self.time_perf_backward[1:])))\n
+        if len(self.time_plotting) > 0: print(\">>> Plotting performance time: {} seconds (+/- {})\".format(np.mean(self.time_plotting[1:]), np.std(self.time_plotting[1:])))\n
+        if len(self.time_saving_model) > 0: print(\">>> Saving performance time: {} seconds (+/- {})\".format(np.mean(self.time_saving_model[1:]), np.std(self.time_saving_model[1:])))\n
+        if len(self.time_tracking_pi) > 0: print(\">>> PI-tracking performance time: {} seconds (+/- {})\".format(np.mean(self.time_tracking_pi[1:]), np.std(self.time_tracking_pi[1:])))\n
+        if len(self.time_scheduler) > 0: print(\">>> Scheduler-update performance time: {} seconds (+/- {})\".format(np.mean(self.time_scheduler[1:]), np.std(self.time_scheduler[1:])))\n
+        \"\"\"\n
+\n
+        #if len(self.power) > 0: print(\">>> Peak Power during training: {} W)\".format(np.max(self.power)))\n
+        if self.val_time: print(\">>> Validation time: {}\".format(self.val_time))\n
+\n
diff --git a/utils/compare_models.py b/utils/compare_models.py\n
index 0021635..e3bd4ec 100644\n
--- a/utils/compare_models.py\n
+++ b/utils/compare_models.py\n
@@ -15 +15,3 @@ from constants.paths import SAVE_DIRECTORY\n
-from high_level_DL_method import load_model,load_optimizer_and_scheduler\n
+from high_level_DL_method import load_optimizer_and_scheduler\n
+from dl_models.full_model import full_model\n
+\n
@@ -79 +81 @@ def get_trainer_and_ds_from_saved_trial(trial_id,add_name_id,save_folder,modific\n
-    model = load_model(ds, args)\n
+    model = full_model(ds, args).to(args.device)\n"
{"commit":"038ba6a7f44a6b40f6ff23e542cd8878fdd65485","author":"Fr-ocasting","date":"2025-04-23T17:32:01+02:00","message":"bordel de data qui correspondent pas"},"diff":"diff --git a/build_inputs/load_contextual_data.py b/build_inputs/load_contextual_data.py\n
index f1f477a..88d755b 100644\n
--- a/build_inputs/load_contextual_data.py\n
+++ b/build_inputs/load_contextual_data.py\n
@@ -224,0 +225 @@ def tackle_contextual(target_ds,invalid_dates,intersect_coverage_period,args,nor\n
+                        raise NotImplementedError(\"NE PREND PAS EN COMPTE CORRECTEMENT NETMOB POIS AVEC D AUTRES DONNEE CONTUEXTUELLES\")\n
@@ -226 +227 @@ def tackle_contextual(target_ds,invalid_dates,intersect_coverage_period,args,nor\n
-                        add_C = latent_dim*contextual_ds.C\n
+                        add_C = latent_dim*sum([contextual_ds_i.C for name_i,contextual_ds_i in contextual_ds.items()])\n
diff --git a/build_inputs/load_preprocessed_dataset.py b/build_inputs/load_preprocessed_dataset.py\n
index c47f69b..5012925 100644\n
--- a/build_inputs/load_preprocessed_dataset.py\n
+++ b/build_inputs/load_preprocessed_dataset.py\n
@@ -63 +63 @@ def update_contextual_tensor(dataset_name,args,need_local_spatial_attn,ds_to_pre\n
-    return contextual_tensors,ds_to_predict,contextual_tensors,ds_which_need_spatial_attn,positions,pos_node_attributes,dict_node_attr2dataset,node_attr_which_need_attn\n
+    return contextual_tensors,ds_to_predict,ds_which_need_spatial_attn,positions,pos_node_attributes,dict_node_attr2dataset,node_attr_which_need_attn\n
@@ -83 +82,0 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-    contextual_tensors,positions = {},{}\n
@@ -85 +84,2 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-    # Add calednar data to the contextual tensors:\n
+\n
+    # Create 'contextual_tensors' and 'positions' and Add calendar data to the contextual tensors:\n
@@ -89 +89,3 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-                                } \n
+                                }\n
+    pos_calendar = [list(contextual_tensors.keys()).index(f'calendar_{calendar_type}') for calendar_type in dict_calendar_U_train.keys()]\n
+    positions= {'calendar': pos_calendar}\n
@@ -101,5 +102,0 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-\n
-        if dataset_name == 'calendar':\n
-            #pos_calendar = list(contextual_tensors.keys()).index(f'calendar_{args.args_embedding.calendar_class}')\n
-            pos_calendar = [list(contextual_tensors.keys()).index(f'calendar_{calendar_type}') for calendar_type in dict_calendar_U_train.keys()]\n
-            positions['calendar'] = pos_calendar\n
@@ -107 +104 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-        elif (dataset_name == 'netmob_image_per_station') or (dataset_name == 'netmob_bidon') or (dataset_name == 'netmob_video_lyon'):\n
+        if (dataset_name == 'netmob_image_per_station') or (dataset_name == 'netmob_bidon') or (dataset_name == 'netmob_video_lyon'):\n
@@ -109 +106 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-             contextual_tensors,target_ds,contextual_tensors,ds_which_need_spatial_attn,positions,pos_node_attributes,dict_node_attr2dataset,node_attr_which_need_attn = update_contextual_tensor(dataset_name,args,need_local_spatial_attn,\n
+             contextual_tensors,target_ds,ds_which_need_spatial_attn,positions,pos_node_attributes,dict_node_attr2dataset,node_attr_which_need_attn = update_contextual_tensor(dataset_name,args,need_local_spatial_attn,\n
@@ -117 +114 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-            contextual_tensors,target_ds,contextual_tensors,ds_which_need_spatial_attn,positions,pos_node_attributes,dict_node_attr2dataset,node_attr_which_need_attn = update_contextual_tensor(dataset_name,args,need_local_spatial_attn,\n
+            contextual_tensors,target_ds,ds_which_need_spatial_attn,positions,pos_node_attributes,dict_node_attr2dataset,node_attr_which_need_attn = update_contextual_tensor(dataset_name,args,need_local_spatial_attn,\n
@@ -143 +140 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-            contextual_tensors,target_ds,contextual_tensors,ds_which_need_spatial_attn,positions,pos_node_attributes,dict_node_attr2dataset,node_attr_which_need_attn = update_contextual_tensor(dataset_name,args,need_local_spatial_attn,\n
+            contextual_tensors,target_ds,ds_which_need_spatial_attn,positions,pos_node_attributes,dict_node_attr2dataset,node_attr_which_need_attn = update_contextual_tensor(dataset_name,args,need_local_spatial_attn,\n
@@ -153 +150 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-            contextual_tensors,target_ds,contextual_tensors,ds_which_need_spatial_attn,positions,pos_node_attributes,dict_node_attr2dataset,node_attr_which_need_attn = update_contextual_tensor(dataset_name,args,need_local_spatial_attn,\n
+            contextual_tensors,target_ds,ds_which_need_spatial_attn,positions,pos_node_attributes,dict_node_attr2dataset,node_attr_which_need_attn = update_contextual_tensor(dataset_name,args,need_local_spatial_attn,\n
@@ -163 +160 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-            contextual_tensors,target_ds,contextual_tensors,ds_which_need_spatial_attn,positions,pos_node_attributes,dict_node_attr2dataset,node_attr_which_need_attn = update_contextual_tensor(dataset_name,args,need_local_spatial_attn,\n
+            contextual_tensors,target_ds,ds_which_need_spatial_attn,positions,pos_node_attributes,dict_node_attr2dataset,node_attr_which_need_attn = update_contextual_tensor(dataset_name,args,need_local_spatial_attn,\n
diff --git a/build_inputs/load_raw_data.py b/build_inputs/load_raw_data.py\n
index a822ba5..6de291a 100644\n
--- a/build_inputs/load_raw_data.py\n
+++ b/build_inputs/load_raw_data.py\n
@@ -21 +21,21 @@ def load_subway_15_min(txt_path, dates = None):\n
-    df = pd.read_csv(txt_path, delimiter =\"\\t\",low_memory=False).rename(columns = {' Date jour (CAS tr)':'date','Heure (CAS tr)':'hour','Nb entrées total (CAS tr)':'in','Nb sorties total (CAS tr)':'out'})\n
+    txt_name = txt_path.split('/')[-1].split('.')[0]\n
+    if  \"Métro 15 minutes 2021\" == txt_name:\n
+        df = pd.read_csv(txt_path, delimiter =\",\",low_memory=False).rename(columns = {'Date jour (CAS tr)':'date',' Date jour (CAS tr)':'date','Heure (CAS tr)':'hour','Nb entrées total (CAS tr)':'in','Nb sorties total (CAS tr)':'out'})\n
+        df = df[df['hour'] != ':'].copy()\n
+        df['date'] = pd.to_datetime(df['date'])\n
+        df['hour'] = df['hour'].apply(lambda h: f\"{int(h.split(':')[0]) - 24}:{h.split(':')[1].zfill(2)}\" if int(h.split(':')[0]) >= 24 else h)\n
+        df['hour_timedelta'] = df['hour'].apply(lambda h: timedelta(hours=int(h.split(':')[0]), minutes=int(h.split(':')[1]) )) \n
+        df['datetime'] = df['date'] + df['hour_timedelta'] \n
+\n
+    elif \"Métro 15 minutes 2019 2020\" == txt_name:\n
+        df = pd.read_csv(txt_path, delimiter =\"\\t\",low_memory=False).rename(columns = {'Date jour (CAS tr)':'date',' Date jour (CAS tr)':'date','Heure (CAS tr)':'hour','Nb entrées total (CAS tr)':'in','Nb sorties total (CAS tr)':'out'})\n
+        # Add '20' like '01_01_2020' instead of '01_01_20'\n
+        df['date'] = df['date'].transform(lambda d : d+'20' if len(d)<10 else d)\n
+        T_2020 = pd.to_datetime(df['date'] + ' ' + df['hour'], format='%d/%m/%Y %H:%M', errors='coerce')\n
+        df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['hour'], format='%d/%m/%Y %H:%M:%S', errors='coerce').fillna(T_2020)\n
+    elif \"Métro 15 minutes 2021 jusqu'au 31 mai.txt\" == txt_name:\n
+        df = pd.read_csv(txt_path, delimiter =\",\",low_memory=False).rename(columns = {'Date jour (CAS tr)':'date',' Date jour (CAS tr)':'date','Heure (CAS tr)':'hour','Nb entrées total (CAS tr)':'in','Nb sorties total (CAS tr)':'out'})\n
+        df = df.dropna()\n
+        raise NotImplementedError(f\"Reading of {txt_path} has not been implemented.\")\n
+    else:\n
+        raise NotImplementedError(f\"Reading of {txt_path} has not been implemented.\")\n
@@ -31,5 +50,0 @@ def load_subway_15_min(txt_path, dates = None):\n
-    # Add '20' like '01_01_2020' instead of '01_01_20'\n
-    df['date'] = df['date'].transform(lambda d : d+'20' if len(d)<10 else d)\n
-    T_2020 = pd.to_datetime(df['date'] + ' ' + df['hour'], format='%d/%m/%Y %H:%M', errors='coerce')\n
-    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['hour'], format='%d/%m/%Y %H:%M:%S', errors='coerce').fillna(T_2020)\n
-\n
diff --git a/dataset.py b/dataset.py\n
index a3f3e6c..3d11059 100644\n
--- a/dataset.py\n
+++ b/dataset.py\n
@@ -475,3 +475,3 @@ class DataSet(object):\n
-        contextual_train  = {name: self.contextual_tensors[name]['train'] for name in self.contextual_tensors.keys()} #[self.contextual_tensors[name]['train'] for name in self.contextual_tensors.keys()]\n
-        contextual_valid  = {name: self.contextual_tensors[name]['valid'] if 'valid' in self.contextual_tensors[name].keys() else None for name in self.contextual_tensors.keys()}   # [self.contextual_tensors[name]['valid'] for name in self.contextual_tensors.keys()]\n
-        contextual_test  =  {name: self.contextual_tensors[name]['test'] if 'test' in self.contextual_tensors[name].keys() else None for name in self.contextual_tensors.keys()} # [self.contextual_tensors[name]['test'] for name in self.contextual_tensors.keys()] \n
+        contextual_train = {name_i: contextual_i['train'] for name_i,contextual_i in self.contextual_tensors.items()}  \n
+        contextual_valid ={name_i: contextual_i['valid'] for name_i,contextual_i in self.contextual_tensors.items() if 'valid' in contextual_i.keys()}     # {name: self.contextual_tensors[name]['valid'] if 'valid' in self.contextual_tensors[name].keys() else None for name in self.contextual_tensors.keys()}   # [self.contextual_tensors[name]['valid'] for name in self.contextual_tensors.keys()]\n
+        contextual_test  = {name_i: contextual_i['test'] for name_i,contextual_i in self.contextual_tensors.items() if 'test' in contextual_i.keys()}    #  {name: self.contextual_tensors[name]['test'] if 'test' in self.contextual_tensors[name].keys() else None for name in self.contextual_tensors.keys()} # [self.contextual_tensors[name]['test'] for name in self.contextual_tensors.keys()] \n
diff --git a/load_inputs/subway_in.py b/load_inputs/subway_in.py\n
index eb1e1c8..cc34afe 100644\n
--- a/load_inputs/subway_in.py\n
+++ b/load_inputs/subway_in.py\n
@@ -113 +113 @@ def load_subway_in_df(args,ROOT,FOLDER_PATH,filename,coverage_period):\n
-    \n
+    # Temporal Restriction: \n
@@ -114,0 +115,2 @@ def load_subway_in_df(args,ROOT,FOLDER_PATH,filename,coverage_period):\n
+\n
+    # Restrain to specific stations:\n
@@ -116,2 +118,5 @@ def load_subway_in_df(args,ROOT,FOLDER_PATH,filename,coverage_period):\n
-    df_correspondance.set_index('Station').reindex(df.columns)\n
-    df.columns = df_correspondance.COD_TRG\n
+    df_correspondance.sort_values(by = 'Station',inplace = True)\n
+    df = df.rename(columns = {row.Station: row.COD_TRG for _,row in df_correspondance.iterrows()})\n
+    df = df[df_correspondance.COD_TRG]\n
+    # ...\n
+\n
diff --git a/load_inputs/subway_indiv.py b/load_inputs/subway_indiv.py\n
index 2363ad6..788631b 100644\n
--- a/load_inputs/subway_indiv.py\n
+++ b/load_inputs/subway_indiv.py\n
@@ -26 +26 @@ NATIVE_FREQ = '3min'\n
-START = '2019-10-01' # Exemple basé sur head()\n
+START = '2019-11-01' # Exemple basé sur head()\n
@@ -71,3 +70,0 @@ def load_data(ROOT, FOLDER_PATH, invalid_dates, coverage_period, args, normalize\n
-        print('df.head(): ',df.head())\n
-        print('Len coverage_period: ',len(coverage_period))\n
-\n
@@ -75 +71,0 @@ def load_data(ROOT, FOLDER_PATH, invalid_dates, coverage_period, args, normalize\n
-        print('df.head(): ',df.head())\n
@@ -77 +72,0 @@ def load_data(ROOT, FOLDER_PATH, invalid_dates, coverage_period, args, normalize\n
-        print('df.head(): ',df.head())\n
@@ -80,4 +74,0 @@ def load_data(ROOT, FOLDER_PATH, invalid_dates, coverage_period, args, normalize\n
-        print(f\"Check the current coverage period on the trial: ({min(coverage_period)} - {max(coverage_period)})\")\n
-        print(f\"And the maximum coverage period of {file_name}: ({df.index.min()} - {df.index.max()})\")\n
-        print('df_reindexed.head(): ',df_reindexed.head())\n
-\n"
{"commit":"1dd7de5f6cb194238bf34705aabd8d0001db88db","author":"Fr-ocasting","date":"2025-04-23T12:02:29+02:00","message":"continuation de la modif"},"diff":"diff --git a/build_inputs/load_contextual_data.py b/build_inputs/load_contextual_data.py\n
index 42150a2..f1f477a 100644\n
--- a/build_inputs/load_contextual_data.py\n
+++ b/build_inputs/load_contextual_data.py\n
@@ -49 +49 @@ def replace_heure_d_ete(tensor,start = 572, end = 576):\n
-def load_input_and_preprocess(dims,normalize,invalid_dates,args,netmob_T,dataset=None,df_dates=None):\n
+def load_input_and_preprocess(dims,normalize,invalid_dates,args,contextual_T,dataset=None,df_dates=None):\n
@@ -51,3 +51,3 @@ def load_input_and_preprocess(dims,normalize,invalid_dates,args,netmob_T,dataset\n
-    print('\\nInit NetMob Dataset: ', netmob_T.size())\n
-    print('Number of Nan Value: ',torch.isnan(netmob_T).sum())\n
-    print('Total Number of Elements: ', netmob_T.numel(),'\\n')\n
+    print('\\nInit contextual_ds Dataset: ', contextual_T.size())\n
+    print('Number of Nan Value: ',torch.isnan(contextual_T).sum())\n
+    print('Total Number of Elements: ', contextual_T.numel(),'\\n')\n
@@ -58 +58 @@ def load_input_and_preprocess(dims,normalize,invalid_dates,args,netmob_T,dataset\n
-    NetMob_ds = PersonnalInput(invalid_dates,args, tensor = netmob_T, dates = df_dates,\n
+    contrextual_ds = PersonnalInput(invalid_dates,args, tensor = contextual_T, dates = df_dates,\n
@@ -69 +69 @@ def load_input_and_preprocess(dims,normalize,invalid_dates,args,netmob_T,dataset\n
-    NetMob_ds.preprocess(args.train_prop,args.valid_prop,args.test_prop,args.train_valid_test_split_method,normalize)\n
+    contrextual_ds.preprocess(args.train_prop,args.valid_prop,args.test_prop,args.train_valid_test_split_method,normalize)\n
@@ -71 +71 @@ def load_input_and_preprocess(dims,normalize,invalid_dates,args,netmob_T,dataset\n
-    return NetMob_ds\n
+    return contrextual_ds\n
@@ -74 +74 @@ def tackle_input_data(invalid_dates,intersect_coverage_period,args,normalize):\n
-    ''' Load the NetMob input data\n
+    ''' Load the contextual data\n
@@ -83,15 +83,75 @@ def tackle_input_data(invalid_dates,intersect_coverage_period,args,normalize):\n
-\n
-    if 'netmob_video_lyon' in args.dataset_names:\n
-    # if vision_input_type == 'unique_image_through_lyon':\n
-        #contextual_ds = load_netmob_lyon_map(dataset,invalid_dates,args,columns = columns,normalize = normalize)\n
-        from load_inputs.netmob_video_lyon import load_data\n
-        contextual_ds = load_data(parent_dir,invalid_dates,intersect_coverage_period,args,restricted,normalize= True)\n
-        args.vision_input_type = 'unique_image_through_lyon'\n
-        netmob_dataset_name = 'netmob_video_lyon'\n
-\n
-\n
-    elif 'netmob_image_per_station' in args.dataset_names:\n
-        from load_inputs.netmob_image_per_station import load_data\n
-        contextual_ds = load_data(parent_dir,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize = normalize) \n
-        args.vision_input_type = 'image_per_stations'\n
-        netmob_dataset_name = 'netmob_image_per_station'\n
+    contextual_ds = {}\n
+    for dataset_name in args.contextual_dataset_names:\n
+        print('>>>Tackle dataset_name:',dataset_name)\n
+\n
+        \"\"\" NEW\"\"\"\n
+        module_path = f\"load_inputs.{dataset_name}\"\n
+        module = importlib.import_module(module_path)\n
+        importlib.reload(module)\n
+        contextual_ds_i = module.load_data(parent_dir,FOLDER_PATH,\n
+                                           coverage_period = intersect_coverage_period,\n
+                                           invalid_dates=invalid_dates,\n
+                                           args=args,\n
+                                           normalize=normalize\n
+                                           )\n
+        contextual_ds_i.name = dataset_name\n
+        contextual_ds[dataset_name] = contextual_ds_i\n
+        \"\"\" END NEW\"\"\"\n
+\n
+        \"\"\"\n
+        if 'netmob_video_lyon' in args.dataset_names:\n
+        # if vision_input_type == 'unique_image_through_lyon':\n
+            #contextual_ds = load_netmob_lyon_map(dataset,invalid_dates,args,columns = columns,normalize = normalize)\n
+            from load_inputs.netmob_video_lyon import load_data\n
+            contextual_ds_i = load_data(parent_dir,invalid_dates,intersect_coverage_period,args,restricted,normalize= True)\n
+            args.vision_input_type = 'unique_image_through_lyon'\n
+            netmob_dataset_name = 'netmob_video_lyon'\n
+\n
+\n
+        elif 'netmob_image_per_station' in args.dataset_names:\n
+            from load_inputs.netmob_image_per_station import load_data\n
+            contextual_ds_i = load_data(parent_dir,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize = normalize) \n
+            args.vision_input_type = 'image_per_stations'\n
+            netmob_dataset_name = 'netmob_image_per_station'\n
+            \n
+        elif \"netmob_POIs\" in args.dataset_names:\n
+            from load_inputs.netmob_POIs import load_data\n
+            contextual_ds_i = load_data(parent_dir,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize= normalize)\n
+            args.vision_input_type = 'POIs'\n
+            netmob_dataset_name = 'netmob_POIs'\n
+\n
+        elif \"netmob_POIs_per_station\" in args.dataset_names:\n
+            from load_inputs.netmob_POIs_per_station import load_data\n
+            contextual_ds_i = load_data(parent_dir,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize= normalize)\n
+            args.vision_input_type = 'POIs'\n
+            netmob_dataset_name = 'netmob_POIs_per_station'\n
+\n
+        elif 'subway_out' in args.dataset_names:\n
+            from load_inputs.subway_out import load_data      \n
+            contextual_ds_i = load_data(parent_dir,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize)  \n
+            args.vision_input_type = 'POIs'\n
+            netmob_dataset_name = 'subway_out'\n
+\n
+        elif 'subway_in' in args.dataset_names:\n
+            from load_inputs.subway_in import load_data\n
+            contextual_ds_i = load_data(parent_dir,FOLDER_PATH,args,coverage_period=intersect_coverage_period,normalize=True)\n
+            args.vision_input_type = 'POIs'\n
+            netmob_dataset_name = 'subway_in'\n
+\n
+        elif 'subway_indiv' in args.dataset_names:\n
+            from load_inputs.subway_in import load_data\n
+            contextual_ds_i = load_data(parent_dir,FOLDER_PATH,args,coverage_period=intersect_coverage_period,normalize=True)\n
+            args.vision_input_type = 'POIs'\n
+            netmob_dataset_name = 'subway_indiv'  \n
+\n
+        elif 'subway_out_per_station' in args.dataset_names:\n
+            from load_inputs.subway_out_per_station import load_data      \n
+            contextual_ds_i = load_data(parent_dir,FOLDER_PATH,coverage_period = intersect_coverage_period,invalid_dates=invalid_dates,args=args,normalize=normalize)  \n
+            args.vision_input_type = 'POIs'\n
+            netmob_dataset_name = 'subway_out_per_station'\n
+        else :\n
+            raise NotImplementedError(f'load data has not been implemented for the netmob file here {args.dataset_names}')\n
+        return(contextual_ds,args,netmob_dataset_name)\n
+        \"\"\"\n
+\n
+        return(contextual_ds,args)\n
@@ -99,58 +158,0 @@ def tackle_input_data(invalid_dates,intersect_coverage_period,args,normalize):\n
-    elif \"netmob_POIs\" in args.dataset_names:\n
-        from load_inputs.netmob_POIs import load_data\n
-        contextual_ds = load_data(parent_dir,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize= normalize)\n
-        args.vision_input_type = 'POIs'\n
-        netmob_dataset_name = 'netmob_POIs'\n
-\n
-    elif \"netmob_POIs_per_station\" in args.dataset_names:\n
-        from load_inputs.netmob_POIs_per_station import load_data\n
-        contextual_ds = load_data(parent_dir,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize= normalize)\n
-        args.vision_input_type = 'POIs'\n
-        netmob_dataset_name = 'netmob_POIs_per_station'\n
-\n
-    elif 'subway_out' in args.dataset_names:\n
-        from load_inputs.subway_out import load_data      \n
-        contextual_ds = load_data(parent_dir,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize)  \n
-        args.vision_input_type = 'POIs'\n
-        netmob_dataset_name = 'subway_out'\n
-\n
-    elif 'subway_in' in args.dataset_names:\n
-        from load_inputs.subway_in import load_data\n
-        contextual_ds = load_data(args,coverage_period=intersect_coverage_period,normalize=True)\n
-        args.vision_input_type = 'POIs'\n
-        netmob_dataset_name = 'subway_in'\n
-\n
-    elif 'subway_indiv' in args.dataset_names:\n
-        from load_inputs.subway_in import load_data\n
-        contextual_ds = load_data(args,coverage_period=intersect_coverage_period,normalize=True)\n
-        args.vision_input_type = 'POIs'\n
-        netmob_dataset_name = 'subway_indiv'  \n
-\n
-    elif 'subway_out_per_station' in args.dataset_names:\n
-        from load_inputs.subway_out_per_station import load_data      \n
-        contextual_ds = load_data(args,parent_dir,FOLDER_PATH,intersect_coverage_period,normalize,invalid_dates)  \n
-        args.vision_input_type = 'POIs'\n
-        netmob_dataset_name = 'subway_out_per_station'\n
-    else :\n
-        raise NotImplementedError(f'load data has not been implemented for the netmob file here {args.dataset_names}')\n
-    \"\"\"\n
-    elif 'subway_in' in args.dataset_names:\n
-        from load_inputs.subway_in import load_data\n
-        if 'subway_in'  == dataset.target_data:\n
-            contextual_ds,_,_,_ = load_datasets_to_predict(args,coverage_period=intersect_coverage_period,normalize=True)\n
-        else:\n
-            raise NotImplementedError\n
-            contextual_ds = load_data(args,ROOT,FOLDER_PATH,coverage_period = None,filename=None) \n
-        args.vision_input_type = 'POIs'\n
-        netmob_dataset_name = 'subway_in'\n
-\n
-    elif 'subway_indiv' in args.dataset_names:\n
-        from load_inputs.subway_in import load_data\n
-        if 'subway_indiv'  == dataset.target_data:\n
-            contextual_ds,_,_,_ = load_datasets_to_predict(args,coverage_period=intersect_coverage_period,normalize=True)\n
-        else:\n
-            raise NotImplementedError\n
-            contextual_ds = load_data(args,ROOT,FOLDER_PATH,coverage_period = None,filename=None) \n
-        args.vision_input_type = 'POIs'\n
-        netmob_dataset_name = 'subway_indiv'\n
-    \"\"\"\n
@@ -158 +160 @@ def tackle_input_data(invalid_dates,intersect_coverage_period,args,normalize):\n
-    return(contextual_ds,args,netmob_dataset_name)\n
+\n
@@ -190,0 +193,7 @@ def tackle_contextual(target_ds,invalid_dates,intersect_coverage_period,args,nor\n
+    # Define contextual tensors\n
+    contextual_dataset_names = [ds_name for ds_name in args.dataset_names if ds_name != target_ds.target_data]\n
+    if args.use_target_as_context:\n
+        if target_ds.target_data not in contextual_dataset_names:\n
+            contextual_dataset_names.append(target_ds.target_data)\n
+    args.contextual_dataset_names = contextual_dataset_names\n
+\n
@@ -192,7 +201,3 @@ def tackle_contextual(target_ds,invalid_dates,intersect_coverage_period,args,nor\n
-    if len(args.dataset_names) > 1: \n
-        more_than_2_contextual_data = (target_ds.target_data in args.dataset_names) and len(args.dataset_names)>2\n
-        more_than_2_contextual_data = more_than_2_contextual_data or ((target_ds.target_data not in args.dataset_names)  and len(args.dataset_names)>1)\n
-        if more_than_2_contextual_data:\n
-            raise NotImplementedError(f\"More than 2 contextual data have been defined. Please check the code.\")\n
-        # TACKLE THE INPUT DATA \n
-        NetMob_ds,args,netmob_dataset_name = tackle_input_data(invalid_dates,intersect_coverage_period,args,normalize)\n
+    if len(args.contextual_dataset_names) > 0: \n
+\n
+        contextual_ds,args = tackle_input_data(invalid_dates,intersect_coverage_period,args,normalize)\n
@@ -205 +210 @@ def tackle_contextual(target_ds,invalid_dates,intersect_coverage_period,args,nor\n
-                raise ValueError(\"You are using 'NetMob' data but you did not defined 'args.vision_model_name'. It needs to be set \")\n
+                raise ValueError(\"You are using contextual data but you did not defined 'args.vision_model_name'. It needs to be set \")\n
@@ -208 +213 @@ def tackle_contextual(target_ds,invalid_dates,intersect_coverage_period,args,nor\n
-                if (args.compute_node_attr_with_attn) or ('per_station' in netmob_dataset_name): \n
+                if (args.compute_node_attr_with_attn) or (sum(['per_station' in contextual_ds_i.name for name_i,contextual_ds_i in contextual_ds.items()])>0): \n
@@ -211,0 +217 @@ def tackle_contextual(target_ds,invalid_dates,intersect_coverage_period,args,nor\n
+                    raise NotImplementedError(f\"VA PRENDRE EN COMPTE UNE SPATIAL ATTENTION MAIS NE SAIS PAS POUR QUEL DONNEE LA FAIRE\")\n
@@ -214,2 +220,2 @@ def tackle_contextual(target_ds,invalid_dates,intersect_coverage_period,args,nor\n
-                if type(NetMob_ds)==list:\n
-                    add_C = latent_dim*NetMob_ds[0].C\n
+                if type(contextual_ds)==list:\n
+                    add_C = latent_dim*contextual_ds[0].C\n
@@ -220 +226 @@ def tackle_contextual(target_ds,invalid_dates,intersect_coverage_period,args,nor\n
-                        add_C = latent_dim*NetMob_ds.C\n
+                        add_C = latent_dim*contextual_ds.C\n
@@ -230,2 +236,2 @@ def tackle_contextual(target_ds,invalid_dates,intersect_coverage_period,args,nor\n
-                args_vision = Namespace(**{'dataset_name': netmob_dataset_name, 'model_name':args.vision_model_name,'input_type':args.vision_input_type})\n
-                args_vision = tackle_config_of_feature_extractor_module(NetMob_ds,args_vision)\n
+                args_vision = Namespace(**{'dataset_name': [contextual_ds_i.name for  name_i,contextual_ds_i in contextual_ds.items()], 'model_name':args.vision_model_name,'input_type':args.vision_input_type})\n
+                args_vision = tackle_config_of_feature_extractor_module(contextual_ds,args_vision)\n
@@ -235 +241 @@ def tackle_contextual(target_ds,invalid_dates,intersect_coverage_period,args,nor\n
-        NetMob_ds = None\n
+        contextual_ds = None\n
@@ -238 +244 @@ def tackle_contextual(target_ds,invalid_dates,intersect_coverage_period,args,nor\n
-    return args,NetMob_ds\n
\\ No newline at end of file\n
+    return args,contextual_ds\n
\\ No newline at end of file\n
diff --git a/build_inputs/load_datasets_to_predict.py b/build_inputs/load_datasets_to_predict.py\n
index fa08ffb..d451741 100644\n
--- a/build_inputs/load_datasets_to_predict.py\n
+++ b/build_inputs/load_datasets_to_predict.py\n
@@ -85,5 +85,6 @@ def load_datasets_to_predict(args,coverage_period,normalize=True):\n
-    \"\"\"\n
-    dataset = module_data.load_data(args,parent_dir,FOLDER_PATH,intersect_coverage_period)\n
-    preprocesed_ds = preprocess_dataset(dataset,args,union_invalid_dates,normalize)\n
-    \"\"\"\n
-    preprocessed_ds = module_data.load_data(ROOT,FOLDER_PATH,union_invalid_dates,intersect_coverage_period,args,normalize= True)\n
+    \n
+    preprocessed_ds = module_data.load_data(ROOT,FOLDER_PATH,\n
+                                            invalid_dates = union_invalid_dates,\n
+                                            coverage_period = intersect_coverage_period,\n
+                                            args = args,\n
+                                            normalize= True)\n
diff --git a/build_inputs/load_preprocessed_dataset.py b/build_inputs/load_preprocessed_dataset.py\n
index 783f2c0..c47f69b 100644\n
--- a/build_inputs/load_preprocessed_dataset.py\n
+++ b/build_inputs/load_preprocessed_dataset.py\n
@@ -13,0 +14,5 @@ from build_inputs.load_calendar import load_calendar,get_args_embedding\n
+from utils.utilities import filter_args\n
+from utils.utilities import get_time_step_per_hour\n
+from dataset import DataSet\n
+from dataset import PersonnalInput\n
+\n
@@ -17,0 +23,2 @@ import pandas as pd\n
+\n
+\n
@@ -74,0 +82 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
+\n
@@ -77 +85 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-    # Define contextual tensor for Calendar Information:\n
+    # Add calednar data to the contextual tensors:\n
@@ -89 +96,0 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-    contextual_dataset_names = [dataset_name for dataset_name in args.dataset_names if dataset_name != target_ds.target_data]\n
@@ -91,3 +98,2 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-    # Particular case if we use a dataset as contextual data AND target_ds.target_data:\n
-    if len([ds_name for ds_name in args.dataset_names if target_ds.target_data == ds_name])>1: \n
-        contextual_dataset_names.append(target_ds.target_data)\n
+    for dataset_name in args.contextual_dataset_names:\n
+        contextual_ds_i = contextual_ds[dataset_name]\n
@@ -96,6 +101,0 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-    #if ('netmob_POIs' in contextual_dataset_names) and ('subway_out' in contextual_dataset_names):\n
-    if len(contextual_dataset_names)>2:\n
-        raise NotImplementedError('Semblerait que contextual_ds est soit subway_out / soit NetMob Pois. ça à l air de merder si on a les deux, en tout cas on a pas encore implémenter, donc à vérifier.')\n
-\n
-\n
-    for dataset_name in contextual_dataset_names:\n
@@ -110 +110 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-                                                                                                                             target_ds,contextual_tensors,contextual_ds,\n
+                                                                                                                             target_ds,contextual_tensors,contextual_ds_i,\n
@@ -114,12 +113,0 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-             \"\"\"\n
-             contextual_tensors.update({'netmob': {'train': contextual_ds.U_train,\n
-                                            'valid': contextual_ds.U_valid if hasattr(contextual_ds,'U_valid') else None,\n
-                                            'test': contextual_ds.U_test  if hasattr(contextual_ds,'U_test') else None}\n
-                                            }\n
-                                            )\n
-            \n
-             pos_netmob = list(contextual_tensors.keys()).index('netmob')\n
-             ds_which_need_spatial_attn.append(dataset_name)\n
-             positions[dataset_name] = pos_netmob\n
-             target_ds.normalizers.update({dataset_name:contextual_ds.normalizer})\n
-             \"\"\"\n
@@ -130 +118 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-                                                                                                                             target_ds,contextual_tensors,contextual_ds,\n
+                                                                                                                             target_ds,contextual_tensors,contextual_ds_i,\n
@@ -135,4 +123,4 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-                    decomposition = fill_and_decompose_df(contextual_ds.raw_values,\n
-                                                        contextual_ds.tensor_limits_keeper.df_verif_train,\n
-                                                        contextual_ds.time_step_per_hour,\n
-                                                        contextual_ds.spatial_unit,\n
+                    decomposition = fill_and_decompose_df(contextual_ds_i.raw_values,\n
+                                                        contextual_ds_i.tensor_limits_keeper.df_verif_train,\n
+                                                        contextual_ds_i.time_step_per_hour,\n
+                                                        contextual_ds_i.spatial_unit,\n
@@ -140 +128 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-                                                        periods = contextual_ds.periods)\n
+                                                        periods = contextual_ds_i.periods)\n
@@ -142 +130 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-                    df_noises = df_noises[contextual_ds.spatial_unit]\n
+                    df_noises = df_noises[contextual_ds_i.spatial_unit]\n
@@ -144 +132 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-                    df_verif_train = contextual_ds.tensor_limits_keeper.df_verif_train\n
+                    df_verif_train = contextual_ds_i.tensor_limits_keeper.df_verif_train\n
@@ -146 +134 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-                    reindex_dates = pd.date_range(dates_used_in_train.min(),dates_used_in_train.max(),freq=f\"{1/contextual_ds.time_step_per_hour}h\")\n
+                    reindex_dates = pd.date_range(dates_used_in_train.min(),dates_used_in_train.max(),freq=f\"{1/contextual_ds_i.time_step_per_hour}h\")\n
@@ -148 +136 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-                    df_noises = pd.DataFrame({col : [1]*len(reindex_dates) for col in contextual_ds.spatial_unit},index =reindex_dates )\n
+                    df_noises = pd.DataFrame({col : [1]*len(reindex_dates) for col in contextual_ds_i.spatial_unit},index =reindex_dates )\n
@@ -156 +144 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-                                                                                                                             target_ds,contextual_tensors,contextual_ds,\n
+                                                                                                                             target_ds,contextual_tensors,contextual_ds_i,\n
@@ -166 +154 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-                                                                                                                             target_ds,contextual_tensors,contextual_ds,\n
+                                                                                                                             target_ds,contextual_tensors,contextual_ds_i,\n
@@ -176 +164 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-                                                                                                                             target_ds,contextual_tensors,contextual_ds,\n
+                                                                                                                             target_ds,contextual_tensors,contextual_ds_i,\n
@@ -180 +167,0 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-\n
@@ -184,4 +171,4 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-                    decomposition = fill_and_decompose_df(contextual_ds[0].raw_values,\n
-                                                        contextual_ds[0].tensor_limits_keeper.df_verif_train,\n
-                                                        contextual_ds[0].time_step_per_hour,\n
-                                                        contextual_ds[0].spatial_unit,\n
+                    decomposition = fill_and_decompose_df(contextual_ds_i[0].raw_values,\n
+                                                        contextual_ds_i[0].tensor_limits_keeper.df_verif_train,\n
+                                                        contextual_ds_i[0].time_step_per_hour,\n
+                                                        contextual_ds_i[0].spatial_unit,\n
@@ -189 +176 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-                                                        periods = contextual_ds[0].periods)\n
+                                                        periods = contextual_ds_i[0].periods)\n
@@ -191 +178 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-                    df_noises = df_noises[contextual_ds[0].spatial_unit]\n
+                    df_noises = df_noises[contextual_ds_i[0].spatial_unit]\n
@@ -193 +180 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-                    df_verif_train = contextual_ds[0].tensor_limits_keeper.df_verif_train\n
+                    df_verif_train = contextual_ds_i[0].tensor_limits_keeper.df_verif_train\n
@@ -195 +182 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-                    reindex_dates = pd.date_range(dates_used_in_train.min(),dates_used_in_train.max(),freq=f\"{1/contextual_ds[0].time_step_per_hour}h\")\n
+                    reindex_dates = pd.date_range(dates_used_in_train.min(),dates_used_in_train.max(),freq=f\"{1/contextual_ds_i[0].time_step_per_hour}h\")\n
@@ -197 +184 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
-                    df_noises = pd.DataFrame({col : [1]*len(reindex_dates) for col in contextual_ds[0].spatial_unit},index =reindex_dates )\n
+                    df_noises = pd.DataFrame({col : [1]*len(reindex_dates) for col in contextual_ds_i[0].spatial_unit},index =reindex_dates )\n
@@ -220,0 +208,17 @@ def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_\n
+def load_input_and_preprocess(dims,normalize,invalid_dates,args,data_T,coverage_period):\n
+    df_dates = pd.DataFrame(coverage_period)\n
+    df_dates.columns = ['date']\n
+    args_DataSet = filter_args(DataSet, args)\n
+\n
+    preprocessed_ds = PersonnalInput(invalid_dates,args, tensor = data_T, dates = df_dates,\n
+                            time_step_per_hour = get_time_step_per_hour(args.freq),\n
+                            #minmaxnorm = dataset.minmaxnorm,\n
+                            #standardize = dataset.standardize,\n
+                           dims =dims,\n
+                           **args_DataSet)\n
+    \n
+    preprocessed_ds.preprocess(args.train_prop,args.valid_prop,args.test_prop,args.train_valid_test_split_method,normalize)\n
+\n
+    return preprocessed_ds\n
+\n
+\n
@@ -233 +237 @@ def load_complete_ds(args,coverage_period = None,normalize = True):\n
-        print('contextual_ds.U_valid',contextual_ds.U_valid.size())\n
+        print('contextual_ds.U_valid:',[contextual_ds_i.U_valid.size() for  name_i,contextual_ds_i in contextual_ds.items()])\n
diff --git a/dataset.py b/dataset.py\n
index e416326..a3f3e6c 100644\n
--- a/dataset.py\n
+++ b/dataset.py\n
@@ -593,0 +594 @@ class PersonnalInput(DataSet):\n
+        self.name = ''\n
diff --git a/load_inputs/buses_indiv.py b/load_inputs/buses_indiv.py\n
new file mode 100644\n
index 0000000..e69de29\n
diff --git a/load_inputs/criter.py b/load_inputs/criter.py\n
index 7fddf11..7f8060d 100644\n
--- a/load_inputs/criter.py\n
+++ b/load_inputs/criter.py\n
@@ -16,0 +17 @@ from dataset import DataSet, PersonnalInput\n
+from build_inputs.load_preprocessed_dataset import load_input_and_preprocess\n
@@ -49 +50 @@ VALUE_COL = 'DEBIT_HEURE' # Ou une autre colonne pertinente retournée par load_\n
-def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_period, args, normalize=True):\n
+def load_data(ROOT, FOLDER_PATH, invalid_dates, coverage_period, args, normalize=True):\n
@@ -57,3 +58,3 @@ def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_peri\n
-    # Déterminer les années couvertes par intersect_coverage_period\n
-    min_date = intersect_coverage_period.min()\n
-    max_date = intersect_coverage_period.max()\n
+    # Déterminer les années couvertes par coverage_period\n
+    min_date = coverage_period.min()\n
+    max_date = coverage_period.max()\n
@@ -133,2 +134,2 @@ def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_peri\n
-        print(f\"Filtrage temporel CRITER sur {len(intersect_coverage_period)} dates...\")\n
-        df_filtered = df_pivoted[df_pivoted.index.isin(intersect_coverage_period)].copy()\n
+        print(f\"Filtrage temporel CRITER sur {len(coverage_period)} dates...\")\n
+        df_filtered = df_pivoted[df_pivoted.index.isin(coverage_period)].copy()\n
@@ -154,11 +155,2 @@ def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_peri\n
-    # --- Création et Prétraitement avec PersonnalInput ---\n
-    print(\"Création et prétraitement de l'objet PersonnalInput pour CRITER...\")\n
-    processed_input = load_input_and_preprocess(\n
-        dims=[0],\n
-        normalize=normalize,\n
-        invalid_dates=invalid_dates,\n
-        args=args,\n
-        data_T=data_T,\n
-        dataset=dataset,\n
-        df_dates=local_df_dates\n
-    )\n
+    print(\"Création et prétraitement de l'objet PersonnalInput...\")\n
+    dims = [0] # if [0] then Normalisation on temporal dim\n
@@ -166 +158 @@ def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_peri\n
-    if processed_input is None: return None\n
+    processed_input = load_input_and_preprocess(dims = dims,normalize=normalize,invalid_dates=invalid_dates,args=args,data_T=data_T,coverage_period=coverage_period)\n
@@ -171 +163 @@ def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_peri\n
-    processed_input.periods = None\n
+    processed_input.periods = None # Pas de périodicité spécifique définie ici\n
@@ -176,27 +167,0 @@ def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_peri\n
-\n
-# Definition de load_input_and_preprocess (identique à subway_indiv.py)\n
-def load_input_and_preprocess(dims, normalize, invalid_dates, args, data_T, dataset, df_dates):\n
-    \"\"\"\n
-    Fonction helper pour instancier PersonnalInput à partir d'un Tensor\n
-    et appeler preprocess.\n
-    \"\"\"\n
-    args_DataSet = filter_args(DataSet, args)\n
-    try:\n
-        personal_instance = PersonnalInput(\n
-            invalid_dates, args, tensor=data_T, dates=df_dates,\n
-            time_step_per_hour=getattr(dataset, 'time_step_per_hour', None),\n
-            dims=dims, **args_DataSet\n
-        )\n
-        print(\"Appel de la méthode preprocess...\")\n
-        personal_instance.preprocess(\n
-            args.train_prop, args.valid_prop, args.test_prop,\n
-            args.train_valid_test_split_method, normalize=normalize\n
-        )\n
-        print(\"Méthode preprocess terminée.\")\n
-        return personal_instance\n
-    except Exception as e:\n
-        print(f\"ERREUR lors de l'instanciation ou preprocess de PersonnalInput : {e}\")\n
-        import traceback\n
-        traceback.print_exc()\n
-        return None\n
-\n
diff --git a/load_inputs/netmob_POIs.py b/load_inputs/netmob_POIs.py\n
index be90e08..99f6a1e 100644\n
--- a/load_inputs/netmob_POIs.py\n
+++ b/load_inputs/netmob_POIs.py\n
@@ -11,2 +10,0 @@ from datetime import datetime\n
-from dataset import DataSet\n
-from dataset import PersonnalInput\n
@@ -15,4 +13,2 @@ import numpy as np\n
-import pickle\n
-from utils.utilities import filter_args\n
-from build_inputs.load_contextual_data import find_positions,replace_heure_d_ete\n
-from utils.utilities import get_time_step_per_hour\n
+from build_inputs.load_contextual_data import replace_heure_d_ete\n
+from build_inputs.load_preprocessed_dataset import load_input_and_preprocess\n
@@ -83 +79 @@ def load_data(ROOT,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,norm\n
-    NetMob_POI = load_input_and_preprocess(dims = dims,normalize=normalize,invalid_dates=invalid_dates,args=args,netmob_T=netmob_T,intersect_coverage_period = intersect_coverage_period) \n
+    NetMob_POI = load_input_and_preprocess(dims = dims,normalize=normalize,invalid_dates=invalid_dates,args=args,data_T=netmob_T,coverage_period = intersect_coverage_period) \n
@@ -106,20 +101,0 @@ def load_data_npy(ROOT,FOLDER_PATH,args):\n
-def load_input_and_preprocess(dims,normalize,invalid_dates,args,netmob_T,intersect_coverage_period):\n
-    df_dates = pd.DataFrame(intersect_coverage_period)\n
-    df_dates.columns = ['date']\n
-    args_DataSet = filter_args(DataSet, args)\n
-    #print('Netmb_T.size: ',netmob_T.size())\n
-    #print('df_dates: ',dataset.df_dates)\n
-    #print('Theoric df-dates length:',len(pd.date_range(start=START, end=END, freq=args.freq)[:-1]))\n
-    #blabla\n
-    NetMob_ds = PersonnalInput(invalid_dates,args, tensor = netmob_T, dates = df_dates,\n
-                            time_step_per_hour = get_time_step_per_hour(args.freq),\n
-                            #minmaxnorm = dataset.minmaxnorm,\n
-                            #standardize = dataset.standardize,\n
-                           dims =dims,\n
-                           **args_DataSet)\n
-    \n
-    NetMob_ds.preprocess(args.train_prop,args.valid_prop,args.test_prop,args.train_valid_test_split_method,normalize)\n
-\n
-    return NetMob_ds\n
-\n
-\n
diff --git a/load_inputs/subway_in.py b/load_inputs/subway_in.py\n
index 9993517..eb1e1c8 100644\n
--- a/load_inputs/subway_in.py\n
+++ b/load_inputs/subway_in.py\n
@@ -10 +10 @@ if parent_dir not in sys.path:\n
-from dataset import DataSet,PersonnalInput\n
+from dataset import DataSet\n
@@ -12,0 +13 @@ from utils.utilities import filter_args,get_time_step_per_hour\n
+from build_inputs.load_preprocessed_dataset import load_input_and_preprocess\n
@@ -38,2 +39,2 @@ n_vertex = 40\n
-def load_data(ROOT,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize= True,filename=None):\n
-    dataset = load_DataSet(args,ROOT,FOLDER_PATH,coverage_period = intersect_coverage_period,filename=filename)\n
+def load_data(ROOT,FOLDER_PATH,invalid_dates,coverage_period,args,normalize= True,filename=None):\n
+    dataset = load_DataSet(args,ROOT,FOLDER_PATH,coverage_period = coverage_period,filename=filename)\n
@@ -42,9 +42,0 @@ def load_data(ROOT,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,norm\n
-    preprocesed_ds = PersonnalInput(invalid_dates, args,tensor = dataset.raw_values, dates = dataset.df_dates, \n
-                                    spatial_unit = dataset.spatial_unit,\n
-                                    indices_spatial_unit = dataset.indices_spatial_unit,\n
-                                    time_step_per_hour = dataset.time_step_per_hour,\n
-                                    city = dataset.city,\n
-                                    dims=dataset.dims,\n
-                                    periods = dataset.periods,\n
-                                     **args_DataSet\n
-                                     )\n
@@ -52 +44,10 @@ def load_data(ROOT,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,norm\n
-    preprocesed_ds.preprocess(args.train_prop,args.valid_prop,args.test_prop,args.train_valid_test_split_method,normalize)\n
+\n
+    preprocesed_ds = load_input_and_preprocess(dims = dataset.dims,normalize=normalize,invalid_dates=invalid_dates,args=args,data_T=dataset.raw_values,coverage_period=coverage_period)\n
+    \n
+    preprocesed_ds.spatial_unit = dataset.spatial_unit\n
+    preprocesed_ds.dims = dataset.dims\n
+    preprocesed_ds.periods = dataset.periods\n
+    preprocesed_ds.time_step_per_hour = dataset.time_step_per_hour\n
+    preprocesed_ds.indices_spatial_unit = dataset.indices_spatial_unit\n
+    preprocesed_ds.city = dataset.city\n
+\n
@@ -160,0 +162 @@ def get_trigram_correspondance():\n
+\n
@@ -162 +164 @@ def get_trigram_correspondance():\n
-                     'SAN','SAX','VMY','JEA',\n
+                     'SAN','SAX','VMY','JEA', #JEA = Vieux Lyon\n
@@ -163,0 +166 @@ def get_trigram_correspondance():\n
+\n
@@ -171,0 +175 @@ def get_trigram_correspondance():\n
+\n
@@ -174,0 +179 @@ def get_trigram_correspondance():\n
+\n
@@ -177,0 +183,13 @@ def get_trigram_correspondance():\n
+    \n
+    df['INDIV'] = ['AMPERE', 'BELLECOUR', 'BROTTEAUX','CORDELIERS',\n
+                   'CUIRE', 'CUSSET','FLACHET','GORGE DE LOUP',\n
+                   'GRANGE BLANCHE', 'GRATTE CIEL', 'PLACE GUICHARD', 'GUILLOTIERE',\n
+                    'HENON', 'HOTEL DE VILLE', 'LAENNEC','MASSENA',      \n
+\n
+                    'MERMOZ PINEL', 'MONPLAISIR LUMIERE','PARILLY','PERRACHE',\n
+                    'SANS SOUCI', 'SAXE GAMBETTA','VALMY','VIEUX LYON',\n
+                    'LAURENT BONNEVAY', 'CHARPENNES', 'GARE DE VAISE', 'GARE DE VENISSIEUX',\n
+\n
+                    'JEAN MACE','GARIBALDI','FOCH', 'REPUBLIQUE',\n
+                    'STADE DE GERLAND', 'DEBOURG', 'PLACE JEAN JAURES','CROIX PAQUET',\n
+                    'CROIX ROUSSE', 'PART DIEU',  'VAULX-EN-VELIN LA SOIE', 'OULLINS GARE',]\n
diff --git a/load_inputs/subway_indiv.py b/load_inputs/subway_indiv.py\n
index bda9224..2363ad6 100644\n
--- a/load_inputs/subway_indiv.py\n
+++ b/load_inputs/subway_indiv.py\n
@@ -17 +17 @@ from utils.utilities import filter_args # Assurez-vous que ce chemin est correct\n
-\n
+from build_inputs.load_preprocessed_dataset import load_input_and_preprocess\n
@@ -21 +21 @@ FILE_BASE_NAME = 'subway_indiv'\n
-DATA_SUBFOLDER = 'validation_individuelle' # Sous-dossier dans FOLDER_PATH\n
+DATA_SUBFOLDER = 'agg_data/validation_individuelle' # Sous-dossier dans FOLDER_PATH\n
@@ -27,0 +28 @@ END = '2020-04-01'\n
+\n
@@ -37,2 +38 @@ VALUE_COL = 'Flow'\n
-\n
-def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_period, args, normalize=True):\n
+def load_data(ROOT, FOLDER_PATH, invalid_dates, coverage_period, args, normalize=True):\n
@@ -47 +47 @@ def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_peri\n
-        intersect_coverage_period (pd.DatetimeIndex): Période temporelle à conserver.\n
+        coverage_period (pd.DatetimeIndex): Période temporelle à conserver.\n
@@ -71,34 +71,2 @@ def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_peri\n
-        # Convertir en datetime\n
-        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n
-\n
-        # Pivoter le DataFrame\n
-        print(\"Pivotage du DataFrame...\")\n
-        df_pivoted = df.pivot_table(index=DATE_COL, columns=LOCATION_COL, values=VALUE_COL, aggfunc='sum')\n
-\n
-        # Remplacer les NaN (dus au pivot) par 0\n
-        df_pivoted = df_pivoted.fillna(0)\n
-\n
-        # S'assurer que l'index est un DatetimeIndex\n
-        df_pivoted.index = pd.to_datetime(df_pivoted.index)\n
-\n
-        # Ré-échantillonage (si args.freq est différent de la freq native du fichier ET plus grossier - rare ici car on charge le fichier exact)\n
-        # Note: Normalement, on charge le fichier correspondant à args.freq, donc pas besoin de resample ici.\n
-        # Laissez ce bloc commenté sauf si vous avez besoin de charger une fréquence plus fine et de la ré-échantillonner.\n
-        # current_freq = pd.infer_freq(df_pivoted.index) # Ou extraire de NATIVE_FREQ/target_freq\n
-        # if target_freq != current_freq and pd.to_timedelta(target_freq) > pd.to_timedelta(current_freq):\n
-        #    print(f\"Ré-échantillonage de {current_freq} vers {target_freq}...\")\n
-        #    df_pivoted = df_pivoted.resample(target_freq).sum() # ou .mean() selon la donnée\n
-\n
-        # Filtrage temporel basé sur l'intersection\n
-        print(f\"Filtrage temporel sur {len(intersect_coverage_period)} dates...\")\n
-        # Assurez-vous que les deux index sont bien des DatetimeIndex\n
-        df_filtered = df_pivoted[df_pivoted.index.isin(intersect_coverage_period)].copy()\n
-        local_df_dates = pd.DataFrame(df_filtered.index, columns=['date'])\n
-\n
-        if df_filtered.empty:\n
-             print(f\"ERREUR : Aucune donnée restante après filtrage temporel pour {file_name}.csv\")\n
-             print(f\"Vérifiez la couverture de intersect_coverage_period ({intersect_coverage_period.min()} - {intersect_coverage_period.max()})\")\n
-             print(f\"et la couverture du fichier chargé ({df_pivoted.index.min()} - {df_pivoted.index.max()})\")\n
-             return None\n
-\n
-        print(f\"Données filtrées. Dimensions: {df_filtered.shape}\")\n
+        print('df.head(): ',df.head())\n
+        print('Len coverage_period: ',len(coverage_period))\n
@@ -106,2 +74,18 @@ def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_peri\n
-        # Conversion en Tensor\n
-        data_T = torch.tensor(df_filtered.values).float()\n
+        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n
+        print('df.head(): ',df.head())\n
+        df = df.set_index(DATE_COL)\n
+        print('df.head(): ',df.head())\n
+        df_reindexed = df[df.index.isin(coverage_period)].copy()\n
+\n
+        print(f\"Check the current coverage period on the trial: ({min(coverage_period)} - {max(coverage_period)})\")\n
+        print(f\"And the maximum coverage period of {file_name}: ({df.index.min()} - {df.index.max()})\")\n
+        print('df_reindexed.head(): ',df_reindexed.head())\n
+\n
+        if df_reindexed.empty:\n
+             print(f\"ERROR : Not any remainig data in {file_name}.csv\")\n
+             print(f\"Check the current coverage period on the trial: ({min(coverage_period)} - {max(coverage_period)})\")\n
+             if df is not None:\n
+                print(f\"And the maximum coverage period of {file_name}: ({df.index.min()} - {df.index.max()})\")\n
+             else:\n
+                print(f\"DataFrame df is empty, no data in {file_name}.csv\")\n
+        data_T = torch.tensor(df_reindexed.values).float()\n
@@ -120,12 +104,3 @@ def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_peri\n
-    processed_input = load_input_and_preprocess(\n
-        dims=[0], # Normalisation sur la dimension temporelle par défaut\n
-        normalize=normalize,\n
-        invalid_dates=invalid_dates, # Utilise les invalid_dates globales passées\n
-        args=args,\n
-        data_T=data_T,\n
-        dataset=dataset, # Passe le dataset principal pour contexte\n
-        df_dates=local_df_dates\n
-    )\n
-\n
-    if processed_input is None:\n
-        return None\n
+    dims = [0] # if [0] then Normalisation on temporal dim\n
+\n
+    processed_input = load_input_and_preprocess(dims = dims,normalize=normalize,invalid_dates=invalid_dates,args=args,data_T=data_T,coverage_period=coverage_period)\n
@@ -134 +109 @@ def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_peri\n
-    processed_input.spatial_unit = df_filtered.columns.tolist()\n
+    processed_input.spatial_unit = df_reindexed.columns.tolist()\n
@@ -141,42 +115,0 @@ def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_peri\n
-def load_input_and_preprocess(dims, normalize, invalid_dates, args, data_T, dataset, df_dates):\n
-    \"\"\"\n
-    Fonction helper pour instancier PersonnalInput à partir d'un Tensor\n
-    et appeler preprocess.\n
-    \"\"\"\n
-    # Filtrer les arguments de args qui sont pertinents pour DataSet/PersonnalInput\n
-    args_DataSet = filter_args(DataSet, args)\n
-\n
-    try:\n
-        # Instancier PersonnalInput avec le Tensor\n
-        # Note: on passe le time_step_per_hour et potentiellement d'autres\n
-        # attributs de contexte depuis le 'dataset' principal.\n
-        personal_instance = PersonnalInput(\n
-            invalid_dates, # Les dates invalides spécifiques à cette donnée (ou globales)\n
-            args,          # L'objet args complet\n
-            tensor=data_T,\n
-            dates=df_dates,\n
-            time_step_per_hour=getattr(dataset, 'time_step_per_hour', None), # Hérite du dataset principal\n
-            dims=dims,     # Dimensions pour la normalisation éventuelle\n
-            # minmaxnorm=getattr(dataset, 'minmaxnorm', None),   # Hérite si besoin\n
-            # standardize=getattr(dataset, 'standardize', None), # Hérite si besoin\n
-            **args_DataSet # Arguments filtrés de args pour DataSet\n
-        )\n
-\n
-        # Appeler la méthode preprocess de l'instance\n
-        print(\"Appel de la méthode preprocess...\")\n
-        personal_instance.preprocess(\n
-            args.train_prop,\n
-            args.valid_prop,\n
-            args.test_prop,\n
-            args.train_valid_test_split_method,\n
-            normalize=normalize\n
-        )\n
-        print(\"Méthode preprocess terminée.\")\n
-        return personal_instance\n
-\n
-    except Exception as e:\n
-        print(f\"ERREUR lors de l'instanciation ou preprocess de PersonnalInput : {e}\")\n
-        import traceback\n
-        traceback.print_exc()\n
-        return None\n
-\n
diff --git a/load_inputs/subway_out.py b/load_inputs/subway_out.py\n
index 91f58c2..d848286 100644\n
--- a/load_inputs/subway_out.py\n
+++ b/load_inputs/subway_out.py\n
@@ -12 +12 @@ from load_inputs.subway_in import load_data as load_data_from_subway_in_py\n
-from load_inputs.netmob_POIs import load_input_and_preprocess\n
+from build_inputs.load_preprocessed_dataset import load_input_and_preprocess\n
@@ -38 +38 @@ C = 1\n
-def load_data(ROOT,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize):\n
+def load_data(ROOT,FOLDER_PATH,invalid_dates,coverage_period,args,normalize):\n
@@ -40 +40 @@ def load_data(ROOT,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,norm\n
-    subway_out = load_data_from_subway_in_py(ROOT,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize= normalize,filename = FILE_NAME)\n
+    subway_out = load_data_from_subway_in_py(ROOT,FOLDER_PATH,invalid_dates,coverage_period,args,normalize= normalize,filename = FILE_NAME)\n
@@ -48 +48 @@ def load_data(ROOT,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,norm\n
-    preprocessed_personal_input = load_input_and_preprocess(dims = dims,normalize=normalize,invalid_dates=invalid_dates,args=args,netmob_T=T_subway_out,intersect_coverage_period=intersect_coverage_period)\n
+    preprocessed_personal_input = load_input_and_preprocess(dims = dims,normalize=normalize,invalid_dates=invalid_dates,args=args,data_T=T_subway_out,coverage_period=coverage_period)\n
diff --git a/load_inputs/tramway_indiv.py b/load_inputs/tramway_indiv.py\n
index 7a2b4ee..e69de29 100644\n
--- a/load_inputs/tramway_indiv.py\n
+++ b/load_inputs/tramway_indiv.py\n
@@ -1,147 +0,0 @@\n
-import sys\n
-import os\n
-import pandas as pd\n
-import torch\n
-import numpy as np\n
-from datetime import datetime\n
-\n
-# --- Gestion de l'arborescence ---\n
-current_file_path = os.path.abspath(os.path.dirname(__file__))\n
-parent_dir = os.path.abspath(os.path.join(current_file_path, '..'))\n
-if parent_dir not in sys.path:\n
-    sys.path.insert(0, parent_dir)\n
-\n
-# --- Importations personnalisées ---\n
-from dataset import DataSet, PersonnalInput\n
-from utils.utilities import filter_args # Assurez-vous que ce chemin est correct\n
-\n
-# --- Constantes spécifiques à cette donnée ---\n
-DIRECTION = 'emitted'\n
-FILE_PATTERN = f'velov_{DIRECTION}_by_station' # Sera complété par args.freq\n
-DATA_SUBFOLDER = 'velov' # Sous-dossier dans FOLDER_PATH\n
-\n
-# Fréquence native la plus fine disponible (pour info)\n
-NATIVE_FREQ = '2min' # A vérifier, basé sur votre commentaire\n
-# Couverture théorique\n
-START = '2019-10-01' # Exemple basé sur head()\n
-END = '2020-04-01'\n
-# Liste des périodes invalides\n
-list_of_invalid_period = []\n
-\n
-C = 1 # Nombre de canaux/features par unité spatiale\n
-\n
-# Colonnes attendues dans le CSV\n
-DATE_COL = 'date_sortie' # Ou 'date_entree' pour 'attracted'?\n
-LOCATION_COL = 'id_sortie' # Ou 'id_entree' pour 'attracted'?\n
-VALUE_COL = 'volume'\n
-\n
-def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_period, args, normalize=True):\n
-    \"\"\"\n
-    Charge, pivote, filtre et pré-traite les données velov (emitted).\n
-    \"\"\"\n
-    target_freq = args.freq\n
-    # Construction spécifique du nom de fichier pour velov\n
-    file_name = f\"{FILE_PATTERN}{target_freq}\"\n
-    data_file_path = os.path.join(ROOT, FOLDER_PATH, DATA_SUBFOLDER, f\"{file_name}.csv\")\n
-\n
-    print(f\"Chargement des données depuis : {data_file_path}\")\n
-    try:\n
-        df = pd.read_csv(data_file_path)\n
-    except FileNotFoundError:\n
-        print(f\"ERREUR : Le fichier {data_file_path} n'a pas été trouvé.\")\n
-        print(f\"Vérifiez que la fréquence '{target_freq}' existe pour velov_{DIRECTION} et que les chemins sont corrects.\")\n
-        return None\n
-    except Exception as e:\n
-        print(f\"ERREUR lors du chargement du fichier {file_name}.csv: {e}\")\n
-        return None\n
-\n
-    # --- Prétraitement ---\n
-    try:\n
-        # Renommer pour utiliser les noms génériques (si différents)\n
-        # df = df.rename(columns={'date_sortie': DATE_COL, 'id_sortie': LOCATION_COL, 'volume': VALUE_COL})\n
-\n
-        # Convertir en datetime\n
-        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n
-\n
-        # Pivoter le DataFrame\n
-        print(\"Pivotage du DataFrame...\")\n
-        df_pivoted = df.pivot_table(index=DATE_COL, columns=LOCATION_COL, values=VALUE_COL, aggfunc='sum')\n
-\n
-        # Remplacer les NaN par 0\n
-        df_pivoted = df_pivoted.fillna(0)\n
-\n
-        # S'assurer que l'index est un DatetimeIndex\n
-        df_pivoted.index = pd.to_datetime(df_pivoted.index)\n
-\n
-        # Filtrage temporel basé sur l'intersection\n
-        print(f\"Filtrage temporel sur {len(intersect_coverage_period)} dates...\")\n
-        df_filtered = df_pivoted[df_pivoted.index.isin(intersect_coverage_period)].copy()\n
-        local_df_dates = pd.DataFrame(df_filtered.index, columns=['date'])\n
-\n
-        if df_filtered.empty:\n
-             print(f\"ERREUR : Aucune donnée restante après filtrage temporel pour {file_name}.csv\")\n
-             # ... (messages d'erreur comme ci-dessus) ...\n
-             return None\n
-\n
-        print(f\"Données filtrées. Dimensions: {df_filtered.shape}\")\n
-\n
-        # Conversion en Tensor\n
-        data_T = torch.tensor(df_filtered.values).float()\n
-\n
-    except KeyError as e:\n
-        print(f\"ERREUR: Colonne manquante dans {file_name}.csv : {e}. Vérifiez les noms de colonnes.\")\n
-        return None\n
-    except Exception as e:\n
-        print(f\"ERREUR pendant le prétraitement des données {file_name}.csv: {e}\")\n
-        return None\n
-\n
-    # --- Création et Prétraitement avec PersonnalInput ---\n
-    print(\"Création et prétraitement de l'objet PersonnalInput...\")\n
-    processed_input = load_input_and_preprocess(\n
-        dims=[0],\n
-        normalize=normalize,\n
-        invalid_dates=invalid_dates,\n
-        args=args,\n
-        data_T=data_T,\n
-        dataset=dataset,\n
-        df_dates=local_df_dates\n
-    )\n
-\n
-    if processed_input is None: return None\n
-\n
-    # --- Finalisation Métadonnées ---\n
-    processed_input.spatial_unit = df_filtered.columns.tolist()\n
-    processed_input.C = C\n
-    processed_input.periods = None\n
-\n
-    print(f\"Chargement et prétraitement de velov_{DIRECTION} terminés.\")\n
-    return processed_input\n
-\n
-# Definition de load_input_and_preprocess (identique à subway_indiv.py)\n
-def load_input_and_preprocess(dims, normalize, invalid_dates, args, data_T, dataset, df_dates):\n
-    \"\"\"\n
-    Fonction helper pour instancier PersonnalInput à partir d'un Tensor\n
-    et appeler preprocess.\n
-    \"\"\"\n
-    args_DataSet = filter_args(DataSet, args)\n
-    try:\n
-        personal_instance = PersonnalInput(\n
-            invalid_dates, args, tensor=data_T, dates=df_dates,\n
-            time_step_per_hour=getattr(dataset, 'time_step_per_hour', None),\n
-            dims=dims, **args_DataSet\n
-        )\n
-        print(\"Appel de la méthode preprocess...\")\n
-        personal_instance.preprocess(\n
-            args.train_prop, args.valid_prop, args.test_prop,\n
-            args.train_valid_test_split_method, normalize=normalize\n
-        )\n
-        print(\"Méthode preprocess terminée.\")\n
-        return personal_instance\n
-    except Exception as e:\n
-        print(f\"ERREUR lors de l'instanciation ou preprocess de PersonnalInput : {e}\")\n
-        import traceback\n
-        traceback.print_exc()\n
-        return None\n
-\n
-# --- Point d'entrée pour exécution directe (optionnel, pour tests) ---\n
-# ... (Similaire à subway_indiv.py, adapter les mocks) ...\n
\\ No newline at end of file\n
diff --git a/load_inputs/velov.py b/load_inputs/velov.py\n
index 8f8f10e..2717c8a 100644\n
--- a/load_inputs/velov.py\n
+++ b/load_inputs/velov.py\n
@@ -17 +17 @@ from utils.utilities import filter_args # Assurez-vous que ce chemin est correct\n
-\n
+from build_inputs.load_preprocessed_dataset import load_input_and_preprocess\n
@@ -18,0 +19 @@ from utils.utilities import filter_args # Assurez-vous que ce chemin est correct\n
+FILE_BASE_NAME = 'velov'\n
@@ -21 +22 @@ FILE_PATTERN = f'velov_{DIRECTION}_by_station' # Sera complété par args.freq\n
-DATA_SUBFOLDER = 'velov' # Sous-dossier dans FOLDER_PATH\n
+DATA_SUBFOLDER = 'agg_data/velov' # Sous-dossier dans FOLDER_PATH\n
@@ -38 +39 @@ VALUE_COL = 'volume'\n
-def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_period, args, normalize=True):\n
+def load_data(ROOT, FOLDER_PATH, invalid_dates, coverage_period, args, normalize=True):\n
@@ -77,2 +78,2 @@ def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_peri\n
-        print(f\"Filtrage temporel sur {len(intersect_coverage_period)} dates...\")\n
-        df_filtered = df_pivoted[df_pivoted.index.isin(intersect_coverage_period)].copy()\n
+        print(f\"Filtrage temporel sur {len(coverage_period)} dates...\")\n
+        df_filtered = df_pivoted[df_pivoted.index.isin(coverage_period)].copy()\n
@@ -100,11 +101,3 @@ def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_peri\n
-    processed_input = load_input_and_preprocess(\n
-        dims=[0],\n
-        normalize=normalize,\n
-        invalid_dates=invalid_dates,\n
-        args=args,\n
-        data_T=data_T,\n
-        dataset=dataset,\n
-        df_dates=local_df_dates\n
-    )\n
-\n
-    if processed_input is None: return None\n
+    dims = [0] # if [0] then Normalisation on temporal dim\n
+\n
+    processed_input = load_input_and_preprocess(dims = dims,normalize=normalize,invalid_dates=invalid_dates,args=args,data_T=data_T,coverage_period=coverage_period)\n
@@ -115 +108 @@ def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_peri\n
-    processed_input.periods = None\n
+    processed_input.periods = None # Pas de périodicité spécifique définie ici\n
@@ -117 +110 @@ def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_peri\n
-    print(f\"Chargement et prétraitement de velov_{DIRECTION} terminés.\")\n
+    print(f\"Chargement et prétraitement de {FILE_BASE_NAME} terminés.\")\n
@@ -120,26 +112,0 @@ def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_peri\n
-# Definition de load_input_and_preprocess (identique à subway_indiv.py)\n
-def load_input_and_preprocess(dims, normalize, invalid_dates, args, data_T, dataset, df_dates):\n
-    \"\"\"\n
-    Fonction helper pour instancier PersonnalInput à partir d'un Tensor\n
-    et appeler preprocess.\n
-    \"\"\"\n
-    args_DataSet = filter_args(DataSet, args)\n
-    try:\n
-        personal_instance = PersonnalInput(\n
-            invalid_dates, args, tensor=data_T, dates=df_dates,\n
-            time_step_per_hour=getattr(dataset, 'time_step_per_hour', None),\n
-            dims=dims, **args_DataSet\n
-        )\n
-        print(\"Appel de la méthode preprocess...\")\n
-        personal_instance.preprocess(\n
-            args.train_prop, args.valid_prop, args.test_prop,\n
-            args.train_valid_test_split_method, normalize=normalize\n
-        )\n
-        print(\"Méthode preprocess terminée.\")\n
-        return personal_instance\n
-    except Exception as e:\n
-        print(f\"ERREUR lors de l'instanciation ou preprocess de PersonnalInput : {e}\")\n
-        import traceback\n
-        traceback.print_exc()\n
-        return None\n
-\n"
{"commit":"d69d37cbb6322cb411ad163b6d577b8500035121","author":"Fr-ocasting","date":"2025-04-22T18:54:47+02:00","message":"Gross modif  flexibilité du code et debut integration multimodale"},"diff":"diff --git a/K_fold_validation/K_fold_validation.py b/K_fold_validation/K_fold_validation.py\n
index 4c9f579..53cee98 100644\n
--- a/K_fold_validation/K_fold_validation.py\n
+++ b/K_fold_validation/K_fold_validation.py\n
@@ -17 +16,0 @@ from constants.paths import FOLDER_PATH\n
-from build_inputs.load_datasets_to_predict import get_intersect_of_coverage_periods\n
@@ -72 +71 @@ class KFoldSplitter(object):\n
-        subway_ds,NetMob_ds,args = load_complete_ds(self.args,normalize = normalize)  #,dic_class2rpz\n
+        target_ds,contextual_ds,args = load_complete_ds(self.args,normalize = normalize)  #,dic_class2rpz\n
@@ -74 +73 @@ class KFoldSplitter(object):\n
-        return(subway_ds,NetMob_ds,args) #,dic_class2rpz)\n
+        return(target_ds,contextual_ds,args) #,dic_class2rpz)\n
@@ -114,2 +113,2 @@ class KFoldSplitter(object):\n
-        K_subway_ds = []\n
-        subway_ds_init,_,args = self.load_init_ds(normalize = True)  # Load 'U' and 'U_target'. # Define already feature vect for the K-th fold with proportion train/valid/test.\n
+        K_ds = []\n
+        target_ds_init,_,args = self.load_init_ds(normalize = True)  # Load 'U' and 'U_target'. # Define already feature vect for the K-th fold with proportion train/valid/test.\n
@@ -118 +117 @@ class KFoldSplitter(object):\n
-        df_verif_init = subway_ds_init.tensor_limits_keeper.df_verif \n
+        df_verif_init = target_ds_init.tensor_limits_keeper.df_verif \n
@@ -124,3 +123,3 @@ class KFoldSplitter(object):\n
-        N1 = len(subway_ds_init.tensor_limits_keeper.df_verif_train)//self.args.K_fold\n
-        N_valid = len(subway_ds_init.tensor_limits_keeper.df_verif_valid)\n
-        N_test = len(subway_ds_init.tensor_limits_keeper.df_verif_test)\n
+        N1 = len(target_ds_init.tensor_limits_keeper.df_verif_train)//self.args.K_fold\n
+        N_valid = len(target_ds_init.tensor_limits_keeper.df_verif_valid)\n
+        N_test = len(target_ds_init.tensor_limits_keeper.df_verif_test)\n
@@ -130 +129 @@ class KFoldSplitter(object):\n
-                K_subway_ds.append(subway_ds_init)\n
+                K_ds.append(target_ds_init)\n
@@ -134 +133 @@ class KFoldSplitter(object):\n
-                args_copy.set_spatial_units = subway_ds_init.spatial_unit\n
+                args_copy.set_spatial_units = target_ds_init.spatial_unit\n
@@ -136 +135 @@ class KFoldSplitter(object):\n
-                coverage_period_tmps =pd.date_range(df_verif_init.min().min(), df_verif_init.iloc[:new_nb_samples].max().max(), freq=f'{60 // subway_ds_init.time_step_per_hour}min')# coverage_period_init.iloc[:new_nb_samples]\n
+                coverage_period_tmps =pd.date_range(df_verif_init.min().min(), df_verif_init.iloc[:new_nb_samples].max().max(), freq=f'{60 // target_ds_init.time_step_per_hour}min')# coverage_period_init.iloc[:new_nb_samples]\n
@@ -142,2 +141,2 @@ class KFoldSplitter(object):\n
-                subway_ds_tmps,_,_= load_complete_ds(args_copy,coverage_period=coverage_period_tmps,normalize = True)  # Normalize\n
-                subway_ds_tmps.init_invalid_dates = subway_ds_init.invalid_dates\n
+                target_ds_tmps,_,_= load_complete_ds(args_copy,coverage_period=coverage_period_tmps,normalize = True)  # Normalize\n
+                target_ds_tmps.init_invalid_dates = target_ds_init.invalid_dates\n
@@ -145,2 +144,2 @@ class KFoldSplitter(object):\n
-                K_subway_ds.append(subway_ds_tmps)\n
-        return K_subway_ds,args\n
+                K_ds.append(target_ds_tmps)\n
+        return K_ds,args\n
diff --git a/build_inputs/load_adj.py b/build_inputs/load_adj.py\n
index 0113b1e..ab17ffc 100644\n
--- a/build_inputs/load_adj.py\n
+++ b/build_inputs/load_adj.py\n
@@ -8 +8 @@ if ROOT not in sys.path:\n
-from constants.paths import ABS_PATH_PACKAGE,FOLDER_PATH,DATA_TO_PREDICT\n
+from constants.paths import ABS_PATH_PACKAGE,FOLDER_PATH\n
@@ -28 +28 @@ def load_adj(dataset,folder = 'adj',adj_type = 'adj',threshold = None):\n
-        gso = pd.read_csv(f'{ABS_PATH_PACKAGE}/{FOLDER_PATH}/{DATA_TO_PREDICT}/{folder}/{adj_type}.csv',index_col = 0)\n
+        gso = pd.read_csv(f'{ABS_PATH_PACKAGE}/{FOLDER_PATH}/{dataset.target_data}/{folder}/{adj_type}.csv',index_col = 0)\n
diff --git a/build_inputs/load_netmob_data.py b/build_inputs/load_contextual_data.py\n
similarity index 72%\n
rename from build_inputs/load_netmob_data.py\n
rename to build_inputs/load_contextual_data.py\n
index 0a08c8c..42150a2 100644\n
--- a/build_inputs/load_netmob_data.py\n
+++ b/build_inputs/load_contextual_data.py\n
@@ -18 +18 @@ from dataset import PersonnalInput\n
-from constants.paths import FOLDER_PATH,DATA_TO_PREDICT\n
+from constants.paths import FOLDER_PATH\n
@@ -19,0 +20,2 @@ from build_inputs.load_datasets_to_predict import load_datasets_to_predict\n
+from utils.utilities import get_time_step_per_hour\n
+\n
@@ -47 +49 @@ def replace_heure_d_ete(tensor,start = 572, end = 576):\n
-def load_input_and_preprocess(dims,normalize,invalid_dates,args,netmob_T,dataset):\n
+def load_input_and_preprocess(dims,normalize,invalid_dates,args,netmob_T,dataset=None,df_dates=None):\n
@@ -53,2 +55,5 @@ def load_input_and_preprocess(dims,normalize,invalid_dates,args,netmob_T,dataset\n
-    NetMob_ds = PersonnalInput(invalid_dates,args, tensor = netmob_T, dates = dataset.df_dates,\n
-                           time_step_per_hour = dataset.time_step_per_hour,\n
+    if df_dates is None:\n
+        df_dates = dataset.df_dates\n
+\n
+    NetMob_ds = PersonnalInput(invalid_dates,args, tensor = netmob_T, dates = df_dates,\n
+                           time_step_per_hour = get_time_step_per_hour(args.time_step_per_hour),\n
@@ -68 +73 @@ def load_input_and_preprocess(dims,normalize,invalid_dates,args,netmob_T,dataset\n
-def tackle_input_data(dataset,invalid_dates,intersect_coverage_period,args,normalize):\n
+def tackle_input_data(invalid_dates,intersect_coverage_period,args,normalize):\n
@@ -81 +86 @@ def tackle_input_data(dataset,invalid_dates,intersect_coverage_period,args,norma\n
-        #NetMob_ds = load_netmob_lyon_map(dataset,invalid_dates,args,columns = columns,normalize = normalize)\n
+        #contextual_ds = load_netmob_lyon_map(dataset,invalid_dates,args,columns = columns,normalize = normalize)\n
@@ -83 +88 @@ def tackle_input_data(dataset,invalid_dates,intersect_coverage_period,args,norma\n
-        NetMob_ds = load_data(dataset,parent_dir,invalid_dates,intersect_coverage_period,args,restricted,normalize= True)\n
+        contextual_ds = load_data(parent_dir,invalid_dates,intersect_coverage_period,args,restricted,normalize= True)\n
@@ -90 +95 @@ def tackle_input_data(dataset,invalid_dates,intersect_coverage_period,args,norma\n
-        NetMob_ds = load_data(dataset,parent_dir,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize = normalize) \n
+        contextual_ds = load_data(parent_dir,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize = normalize) \n
@@ -96 +101 @@ def tackle_input_data(dataset,invalid_dates,intersect_coverage_period,args,norma\n
-        NetMob_ds = load_data(dataset,parent_dir,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize= normalize)\n
+        contextual_ds = load_data(parent_dir,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize= normalize)\n
@@ -102 +107 @@ def tackle_input_data(dataset,invalid_dates,intersect_coverage_period,args,norma\n
-        NetMob_ds = load_data(dataset,parent_dir,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize= normalize)\n
+        contextual_ds = load_data(parent_dir,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize= normalize)\n
@@ -108 +113 @@ def tackle_input_data(dataset,invalid_dates,intersect_coverage_period,args,norma\n
-        NetMob_ds = load_data(dataset,args,parent_dir,FOLDER_PATH,intersect_coverage_period,normalize,invalid_dates)  \n
+        contextual_ds = load_data(parent_dir,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize)  \n
@@ -114,5 +119 @@ def tackle_input_data(dataset,invalid_dates,intersect_coverage_period,args,norma\n
-        if 'subway_in'  == DATA_TO_PREDICT:\n
-            NetMob_ds,_,_,_ = load_datasets_to_predict(args,coverage_period=intersect_coverage_period,normalize=True)\n
-        else:\n
-            raise NotImplementedError\n
-            NetMob_ds = load_data(args,ROOT,FOLDER_PATH,coverage_period = None,filename=None) \n
+        contextual_ds = load_data(args,coverage_period=intersect_coverage_period,normalize=True)\n
@@ -121,0 +123,6 @@ def tackle_input_data(dataset,invalid_dates,intersect_coverage_period,args,norma\n
+    elif 'subway_indiv' in args.dataset_names:\n
+        from load_inputs.subway_in import load_data\n
+        contextual_ds = load_data(args,coverage_period=intersect_coverage_period,normalize=True)\n
+        args.vision_input_type = 'POIs'\n
+        netmob_dataset_name = 'subway_indiv'  \n
+\n
@@ -124 +131 @@ def tackle_input_data(dataset,invalid_dates,intersect_coverage_period,args,norma\n
-        NetMob_ds = load_data(dataset,args,parent_dir,FOLDER_PATH,intersect_coverage_period,normalize,invalid_dates)  \n
+        contextual_ds = load_data(args,parent_dir,FOLDER_PATH,intersect_coverage_period,normalize,invalid_dates)  \n
@@ -129,2 +136,23 @@ def tackle_input_data(dataset,invalid_dates,intersect_coverage_period,args,norma\n
-    \n
-    return(NetMob_ds,args,netmob_dataset_name)\n
+    \"\"\"\n
+    elif 'subway_in' in args.dataset_names:\n
+        from load_inputs.subway_in import load_data\n
+        if 'subway_in'  == dataset.target_data:\n
+            contextual_ds,_,_,_ = load_datasets_to_predict(args,coverage_period=intersect_coverage_period,normalize=True)\n
+        else:\n
+            raise NotImplementedError\n
+            contextual_ds = load_data(args,ROOT,FOLDER_PATH,coverage_period = None,filename=None) \n
+        args.vision_input_type = 'POIs'\n
+        netmob_dataset_name = 'subway_in'\n
+\n
+    elif 'subway_indiv' in args.dataset_names:\n
+        from load_inputs.subway_in import load_data\n
+        if 'subway_indiv'  == dataset.target_data:\n
+            contextual_ds,_,_,_ = load_datasets_to_predict(args,coverage_period=intersect_coverage_period,normalize=True)\n
+        else:\n
+            raise NotImplementedError\n
+            contextual_ds = load_data(args,ROOT,FOLDER_PATH,coverage_period = None,filename=None) \n
+        args.vision_input_type = 'POIs'\n
+        netmob_dataset_name = 'subway_indiv'\n
+    \"\"\"\n
+\n
+    return(contextual_ds,args,netmob_dataset_name)\n
@@ -161 +189 @@ def tackle_config_of_feature_extractor_module(contextual_ds,args_vision):\n
-def tackle_netmob(dataset,invalid_dates,intersect_coverage_period,args,normalize = True):\n
+def tackle_contextual(target_ds,invalid_dates,intersect_coverage_period,args,normalize = True):\n
@@ -163,6 +191,6 @@ def tackle_netmob(dataset,invalid_dates,intersect_coverage_period,args,normalize\n
-    # BOOLEAN VALUE : True IF NETMOB or SUBWAY_OUT IS USED as contextual data\n
-    bool_netmob = (sum([True for d in args.dataset_names if (('netmob' in d) or ('subway_out' in d)) ])) > 0\n
-    if (DATA_TO_PREDICT == 'subway_in') and (len([d for d in args.dataset_names if d == 'subway_in'])>1):  # case where subway-in is a contextual data\n
-        bool_netmob  = True\n
-        \n
-    if bool_netmob: \n
+    # USE CONTEXTUAL DATA\n
+    if len(args.dataset_names) > 1: \n
+        more_than_2_contextual_data = (target_ds.target_data in args.dataset_names) and len(args.dataset_names)>2\n
+        more_than_2_contextual_data = more_than_2_contextual_data or ((target_ds.target_data not in args.dataset_names)  and len(args.dataset_names)>1)\n
+        if more_than_2_contextual_data:\n
+            raise NotImplementedError(f\"More than 2 contextual data have been defined. Please check the code.\")\n
@@ -170 +198 @@ def tackle_netmob(dataset,invalid_dates,intersect_coverage_period,args,normalize\n
-        NetMob_ds,args,netmob_dataset_name = tackle_input_data(dataset,invalid_dates,intersect_coverage_period,args,normalize)\n
+        NetMob_ds,args,netmob_dataset_name = tackle_input_data(invalid_dates,intersect_coverage_period,args,normalize)\n
diff --git a/build_inputs/load_datasets_to_predict.py b/build_inputs/load_datasets_to_predict.py\n
index 3529999..fa08ffb 100644\n
--- a/build_inputs/load_datasets_to_predict.py\n
+++ b/build_inputs/load_datasets_to_predict.py\n
@@ -8,3 +8,3 @@ current_file_path = os.path.abspath(os.path.dirname(__file__))\n
-parent_dir = os.path.abspath(os.path.join(current_file_path,'..'))\n
-if parent_dir not in sys.path:\n
-    sys.path.insert(0,parent_dir)\n
+ROOT = os.path.abspath(os.path.join(current_file_path,'..'))\n
+if ROOT not in sys.path:\n
+    sys.path.insert(0,ROOT)\n
@@ -15 +15 @@ from dataset import PersonnalInput,DataSet\n
-from constants.paths import DATA_TO_PREDICT,FOLDER_PATH,USELESS_DATES\n
+from constants.paths import FOLDER_PATH,USELESS_DATES\n
@@ -19,25 +19 @@ from utils.seasonal_decomposition import fill_and_decompose_df\n
-def preprocess_dataset(dataset,args,invalid_dates,normalize = True): \n
-    print(f\"\\nInit Dataset: '{dataset.raw_values.size()} with {dataset.raw_values.numel()} Total nb of elements and {torch.isnan(dataset.raw_values).sum()} Nan values\")\n
-\n
-    args_DataSet = filter_args(DataSet, args)\n
-    print(f'nb {DATA_TO_PREDICT} invalid dates: ',len(invalid_dates))\n
-    preprocesed_ds = PersonnalInput(invalid_dates,args,\n
-                                     tensor = dataset.raw_values, \n
-                                     dates = dataset.df_dates, \n
-                                     spatial_unit = dataset.spatial_unit,\n
-                                     indices_spatial_unit = dataset.indices_spatial_unit,\n
-                                     time_step_per_hour = dataset.time_step_per_hour,\n
-                                     #minmaxnorm = dataset.minmaxnorm,\n
-                                     #standardize = dataset.standardize,\n
-                                     city = dataset.city,\n
-                                     dims=dataset.dims,\n
-                                     periods = dataset.periods,\n
-                                     **args_DataSet\n
-                                     )\n
-\n
-    preprocesed_ds.preprocess(args.train_prop,args.valid_prop,args.test_prop,args.train_valid_test_split_method,normalize)\n
-    preprocesed_ds = add_noise(preprocesed_ds,args)\n
-\n
-    return(preprocesed_ds)\n
-\n
-\n
+\"\"\"\n
@@ -63 +39 @@ def add_noise(preprocesed_ds,args):\n
-        preprocesed_ds.noises = {DATA_TO_PREDICT:df_noises}\n
+        preprocesed_ds.noises = {preprocesed_ds.target_data:df_noises}\n
@@ -67,2 +43 @@ def add_noise(preprocesed_ds,args):\n
-\n
-\n
+\"\"\"\n
@@ -108 +83 @@ def load_datasets_to_predict(args,coverage_period,normalize=True):\n
-    module_data = importlib.import_module(f\"load_inputs.{DATA_TO_PREDICT}\")\n
+    module_data = importlib.import_module(f\"load_inputs.{args.target_data}\")\n
@@ -109,0 +85 @@ def load_datasets_to_predict(args,coverage_period,normalize=True):\n
+    \"\"\"\n
@@ -111,2 +87,4 @@ def load_datasets_to_predict(args,coverage_period,normalize=True):\n
-    args.n_vertex = dataset.n_vertex\n
-    args.C = dataset.C\n
+    preprocesed_ds = preprocess_dataset(dataset,args,union_invalid_dates,normalize)\n
+    \"\"\"\n
+    preprocessed_ds = module_data.load_data(ROOT,FOLDER_PATH,union_invalid_dates,intersect_coverage_period,args,normalize= True)\n
+    print(f\"\\nInit Dataset: '{preprocessed_ds.raw_values.size()} with {preprocessed_ds.raw_values.numel()} Total nb of elements and {torch.isnan(preprocessed_ds.raw_values).sum()} Nan values\")\n
@@ -113,0 +92,5 @@ def load_datasets_to_predict(args,coverage_period,normalize=True):\n
+    if args.data_augmentation and args.DA_method == 'noise':\n
+        if args.DA_noise_from == 'MSTL':\n
+            raise NotImplementedError(\"Has to decompose seasonal component first but not implemented yet. Please refer to the commented code in the top of this file.\")\n
+    args.n_vertex = preprocessed_ds.n_vertex\n
+    args.C = preprocessed_ds.C\n
@@ -115,2 +98,2 @@ def load_datasets_to_predict(args,coverage_period,normalize=True):\n
-    preprocesed_ds = preprocess_dataset(dataset,args,union_invalid_dates,normalize)\n
-    return(preprocesed_ds,dataset,union_invalid_dates,intersect_coverage_period)\n
+  \n
+    return(preprocessed_ds,union_invalid_dates,intersect_coverage_period)\n
diff --git a/build_inputs/load_preprocessed_dataset.py b/build_inputs/load_preprocessed_dataset.py\n
index 3104258..783f2c0 100644\n
--- a/build_inputs/load_preprocessed_dataset.py\n
+++ b/build_inputs/load_preprocessed_dataset.py\n
@@ -11 +11 @@ if parent_dir not in sys.path:\n
-from build_inputs.load_netmob_data import tackle_netmob\n
+from build_inputs.load_contextual_data import tackle_contextual\n
@@ -15 +14,0 @@ from build_inputs.load_calendar import load_calendar,get_args_embedding\n
-from constants.paths import DATA_TO_PREDICT\n
@@ -60 +59 @@ def update_contextual_tensor(dataset_name,args,need_local_spatial_attn,ds_to_pre\n
-def add_contextual_data(args,subway_ds,contextual_ds,dict_calendar_U_train,dict_calendar_U_valid,dict_calendar_U_test):\n
+def add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_calendar_U_valid,dict_calendar_U_test):\n
@@ -75 +74 @@ def add_contextual_data(args,subway_ds,contextual_ds,dict_calendar_U_train,dict_\n
-    subway_ds.normalizers = {DATA_TO_PREDICT:subway_ds.normalizer}\n
+    target_ds.normalizers = {target_ds.target_data:target_ds.normalizer}\n
@@ -90 +89 @@ def add_contextual_data(args,subway_ds,contextual_ds,dict_calendar_U_train,dict_\n
-    contextual_dataset_names = [dataset_name for dataset_name in args.dataset_names if dataset_name != DATA_TO_PREDICT]\n
+    contextual_dataset_names = [dataset_name for dataset_name in args.dataset_names if dataset_name != target_ds.target_data]\n
@@ -92,3 +91,3 @@ def add_contextual_data(args,subway_ds,contextual_ds,dict_calendar_U_train,dict_\n
-    # Particular case if we use a dataset as contextual data AND DATA_TO_PREDICT:\n
-    if len([ds_name for ds_name in args.dataset_names if DATA_TO_PREDICT == ds_name])>1: \n
-        contextual_dataset_names.append(DATA_TO_PREDICT)\n
+    # Particular case if we use a dataset as contextual data AND target_ds.target_data:\n
+    if len([ds_name for ds_name in args.dataset_names if target_ds.target_data == ds_name])>1: \n
+        contextual_dataset_names.append(target_ds.target_data)\n
@@ -110,2 +109,2 @@ def add_contextual_data(args,subway_ds,contextual_ds,dict_calendar_U_train,dict_\n
-             contextual_tensors,subway_ds,contextual_tensors,ds_which_need_spatial_attn,positions,pos_node_attributes,dict_node_attr2dataset,node_attr_which_need_attn = update_contextual_tensor(dataset_name,args,need_local_spatial_attn,\n
-                                                                                                                             subway_ds,contextual_tensors,contextual_ds,\n
+             contextual_tensors,target_ds,contextual_tensors,ds_which_need_spatial_attn,positions,pos_node_attributes,dict_node_attr2dataset,node_attr_which_need_attn = update_contextual_tensor(dataset_name,args,need_local_spatial_attn,\n
+                                                                                                                             target_ds,contextual_tensors,contextual_ds,\n
@@ -125 +124 @@ def add_contextual_data(args,subway_ds,contextual_ds,dict_calendar_U_train,dict_\n
-             subway_ds.normalizers.update({dataset_name:contextual_ds.normalizer})\n
+             target_ds.normalizers.update({dataset_name:contextual_ds.normalizer})\n
@@ -128 +127 @@ def add_contextual_data(args,subway_ds,contextual_ds,dict_calendar_U_train,dict_\n
-        elif (dataset_name == 'subway_out') or (dataset_name == 'subway_in'):\n
+        elif (dataset_name == 'subway_out') or (dataset_name == 'subway_in') or (dataset_name == 'subway_indiv'):\n
@@ -130,2 +129,2 @@ def add_contextual_data(args,subway_ds,contextual_ds,dict_calendar_U_train,dict_\n
-            contextual_tensors,subway_ds,contextual_tensors,ds_which_need_spatial_attn,positions,pos_node_attributes,dict_node_attr2dataset,node_attr_which_need_attn = update_contextual_tensor(dataset_name,args,need_local_spatial_attn,\n
-                                                                                                                             subway_ds,contextual_tensors,contextual_ds,\n
+            contextual_tensors,target_ds,contextual_tensors,ds_which_need_spatial_attn,positions,pos_node_attributes,dict_node_attr2dataset,node_attr_which_need_attn = update_contextual_tensor(dataset_name,args,need_local_spatial_attn,\n
+                                                                                                                             target_ds,contextual_tensors,contextual_ds,\n
@@ -153 +152 @@ def add_contextual_data(args,subway_ds,contextual_ds,dict_calendar_U_train,dict_\n
-                subway_ds.noises[dataset_name] = df_noises         \n
+                target_ds.noises[dataset_name] = df_noises         \n
@@ -156,2 +155,2 @@ def add_contextual_data(args,subway_ds,contextual_ds,dict_calendar_U_train,dict_\n
-            contextual_tensors,subway_ds,contextual_tensors,ds_which_need_spatial_attn,positions,pos_node_attributes,dict_node_attr2dataset,node_attr_which_need_attn = update_contextual_tensor(dataset_name,args,need_local_spatial_attn,\n
-                                                                                                                             subway_ds,contextual_tensors,contextual_ds,\n
+            contextual_tensors,target_ds,contextual_tensors,ds_which_need_spatial_attn,positions,pos_node_attributes,dict_node_attr2dataset,node_attr_which_need_attn = update_contextual_tensor(dataset_name,args,need_local_spatial_attn,\n
+                                                                                                                             target_ds,contextual_tensors,contextual_ds,\n
@@ -166,2 +165,2 @@ def add_contextual_data(args,subway_ds,contextual_ds,dict_calendar_U_train,dict_\n
-            contextual_tensors,subway_ds,contextual_tensors,ds_which_need_spatial_attn,positions,pos_node_attributes,dict_node_attr2dataset,node_attr_which_need_attn = update_contextual_tensor(dataset_name,args,need_local_spatial_attn,\n
-                                                                                                                             subway_ds,contextual_tensors,contextual_ds,\n
+            contextual_tensors,target_ds,contextual_tensors,ds_which_need_spatial_attn,positions,pos_node_attributes,dict_node_attr2dataset,node_attr_which_need_attn = update_contextual_tensor(dataset_name,args,need_local_spatial_attn,\n
+                                                                                                                             target_ds,contextual_tensors,contextual_ds,\n
@@ -176,2 +175,2 @@ def add_contextual_data(args,subway_ds,contextual_ds,dict_calendar_U_train,dict_\n
-            contextual_tensors,subway_ds,contextual_tensors,ds_which_need_spatial_attn,positions,pos_node_attributes,dict_node_attr2dataset,node_attr_which_need_attn = update_contextual_tensor(dataset_name,args,need_local_spatial_attn,\n
-                                                                                                                             subway_ds,contextual_tensors,contextual_ds,\n
+            contextual_tensors,target_ds,contextual_tensors,ds_which_need_spatial_attn,positions,pos_node_attributes,dict_node_attr2dataset,node_attr_which_need_attn = update_contextual_tensor(dataset_name,args,need_local_spatial_attn,\n
+                                                                                                                             target_ds,contextual_tensors,contextual_ds,\n
@@ -203 +202 @@ def add_contextual_data(args,subway_ds,contextual_ds,dict_calendar_U_train,dict_\n
-                subway_ds.noises[dataset_name] = df_noises\n
+                target_ds.noises[dataset_name] = df_noises\n
@@ -208,2 +207,2 @@ def add_contextual_data(args,subway_ds,contextual_ds,dict_calendar_U_train,dict_\n
-    subway_ds.contextual_tensors = contextual_tensors\n
-    subway_ds.get_dataloader()\n
+    target_ds.contextual_tensors = contextual_tensors\n
+    target_ds.get_dataloader()\n
@@ -212 +211 @@ def add_contextual_data(args,subway_ds,contextual_ds,dict_calendar_U_train,dict_\n
-    subway_ds.contextual_positions = positions\n
+    target_ds.contextual_positions = positions\n
@@ -219 +218 @@ def add_contextual_data(args,subway_ds,contextual_ds,dict_calendar_U_train,dict_\n
-    return(subway_ds,args)\n
+    return(target_ds,args)\n
@@ -224 +223,2 @@ def load_complete_ds(args,coverage_period = None,normalize = True):\n
-    subway_ds,dataset,invalid_dates,intersect_coverage_period = load_datasets_to_predict(args,coverage_period,normalize)\n
+    target_ds,invalid_dates,intersect_coverage_period = load_datasets_to_predict(args,coverage_period,normalize)\n
+    \n
@@ -226,4 +226 @@ def load_complete_ds(args,coverage_period = None,normalize = True):\n
-    '''\n
-    dict_calendar_U_train,dict_calendar_U_valid,dict_calendar_U_test,dic_class2rpz,dic_rpz2class,nb_words_embedding = load_calendar(subway_ds)\n
-    '''\n
-    dict_calendar_U_train,dict_calendar_U_valid,dict_calendar_U_test = load_calendar(subway_ds)\n
+    dict_calendar_U_train,dict_calendar_U_valid,dict_calendar_U_test = load_calendar(target_ds)\n
@@ -231,11 +227,0 @@ def load_complete_ds(args,coverage_period = None,normalize = True):\n
-    # Calendar data for training (with Time-Embedding):\n
-    '''\n
-    args = tackle_calendar(args,dic_class2rpz,dic_rpz2class,nb_words_embedding)\n
-    '''\n
-    # Netmob: \n
-    args,NetMob_ds = tackle_netmob(dataset,invalid_dates,intersect_coverage_period,args,normalize = normalize)\n
-    print('subway_ds.U_valid',subway_ds.U_valid.size())\n
-    if NetMob_ds is not None:\n
-        print('NetMob_ds.U_valid',NetMob_ds.U_valid.size())\n
-    # Add Contextual Tensors and their positions: \n
-    subway_ds,args = add_contextual_data(args,subway_ds,NetMob_ds,dict_calendar_U_train,dict_calendar_U_valid,dict_calendar_U_test)\n
@@ -242,0 +229,8 @@ def load_complete_ds(args,coverage_period = None,normalize = True):\n
+    # Contextual: \n
+    args,contextual_ds = tackle_contextual(target_ds,invalid_dates,intersect_coverage_period,args,normalize = normalize)\n
+    print('target_ds.U_valid',target_ds.U_valid.size())\n
+    if contextual_ds is not None:\n
+        print('contextual_ds.U_valid',contextual_ds.U_valid.size())\n
+\n
+    # Add Contextual Tensors and their positions: \n
+    target_ds,args = add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_calendar_U_valid,dict_calendar_U_test)\n
@@ -244,2 +238,2 @@ def load_complete_ds(args,coverage_period = None,normalize = True):\n
-    assert subway_ds.U_train.dim() == 3, f'Feature Vector does not have the good dimension. Expected shape dimension [B,N,L], got {subway_ds.U_train.dim()} dim: {subway_ds.U_train.size()}'\n
-    return(subway_ds,NetMob_ds,args) #,args.dic_class2rpz)\n
\\ No newline at end of file\n
+    assert target_ds.U_train.dim() == 3, f'Feature Vector does not have the good dimension. Expected shape dimension [B,N,L], got {target_ds.U_train.dim()} dim: {target_ds.U_train.size()}'\n
+    return(target_ds,contextual_ds,args) #,args.dic_class2rpz)\n
\\ No newline at end of file\n
diff --git a/build_inputs/preprocess_subway_15.py b/build_inputs/preprocess_subway_15.py\n
index 6b7a274..11cc3f7 100644\n
--- a/build_inputs/preprocess_subway_15.py\n
+++ b/build_inputs/preprocess_subway_15.py\n
@@ -186 +186 @@ if __name__ == '__main__':\n
-    from constants.paths import FOLDER_PATH, DATA_TO_PREDICT\n
+    from constants.paths import FOLDER_PATH\n
@@ -211 +211 @@ if __name__ == '__main__':\n
-    if not os.path.exists(f'{ROOT}/{FOLDER_PATH}/{DATA_TO_PREDICT}'):\n
+    if not os.path.exists(f'{ROOT}/{FOLDER_PATH}/{args.target_data}'):\n
@@ -214,5 +214,5 @@ if __name__ == '__main__':\n
-        if not os.path.exists(f'{ROOT}/{FOLDER_PATH}/{DATA_TO_PREDICT}/adj/'):\n
-            os.mkdir(f'{ROOT}/{FOLDER_PATH}/{DATA_TO_PREDICT}/adj/')\n
-        adj.to_csv(f'{ROOT}/{FOLDER_PATH}/{DATA_TO_PREDICT}/adj/adj.csv')\n
-        corr.to_csv(f'{ROOT}/{FOLDER_PATH}/{DATA_TO_PREDICT}/adj/corr.csv')\n
-        dist.to_csv(f'{ROOT}/{FOLDER_PATH}/{DATA_TO_PREDICT}/adj/dist.csv')\n
+        if not os.path.exists(f'{ROOT}/{FOLDER_PATH}/{args.target_data}/adj/'):\n
+            os.mkdir(f'{ROOT}/{FOLDER_PATH}/{args.target_data}/adj/')\n
+        adj.to_csv(f'{ROOT}/{FOLDER_PATH}/{args.target_data}/adj/adj.csv')\n
+        corr.to_csv(f'{ROOT}/{FOLDER_PATH}/{args.target_data}/adj/corr.csv')\n
+        dist.to_csv(f'{ROOT}/{FOLDER_PATH}/{args.target_data}/adj/dist.csv')\n
diff --git a/constants/config.py b/constants/config.py\n
index 4c8c906..3b9c532 100644\n
--- a/constants/config.py\n
+++ b/constants/config.py\n
@@ -15 +14,0 @@ if parent_dir not in sys.path:\n
-from constants.paths import DATA_TO_PREDICT\n
@@ -22 +20,0 @@ def get_config(model_name,dataset_names,dataset_for_coverage,config = {}):\n
-    #data_module = importlib.import_module(f\"load_inputs.{DATA_TO_PREDICT}\")\n
diff --git a/constants/paths.py b/constants/paths.py\n
index 9e940b0..49393ee 100644\n
--- a/constants/paths.py\n
+++ b/constants/paths.py\n
@@ -15 +15 @@ if torch.cuda.is_available():\n
-    DATA_TO_PREDICT = 'subway_in' # 'data_bidon'  # 'subway_in' # 'METR_LA' # 'PEMS_BAY' #'CRITER_3lanes'\n
+    #DATA_TO_PREDICT = 'subway_indiv' # 'subway_indiv', 'data_bidon'  # 'subway_in' # 'METR_LA' # 'PEMS_BAY' #'CRITER_3lanes'\n
@@ -25 +25 @@ else:\n
-    DATA_TO_PREDICT = 'subway_in' # 'data_bidon'  # 'subway_in' # 'METR_LA' # 'PEMS_BAY' # 'CRITER_3lanes'\n
+    #DATA_TO_PREDICT = 'subway_in' # 'data_bidon'  # 'subway_in' # 'METR_LA' # 'PEMS_BAY' # 'CRITER_3lanes'\n
diff --git a/data_augmentation/data_augmentation.py b/data_augmentation/data_augmentation.py\n
index 31ae31a..2144822 100644\n
--- a/data_augmentation/data_augmentation.py\n
+++ b/data_augmentation/data_augmentation.py\n
@@ -11 +10,0 @@ if parent_dir not in sys.path:\n
-from constants.paths import USELESS_DATES,DATA_TO_PREDICT\n
@@ -32,0 +32 @@ class DataAugmenter(object):\n
+        self.target_data = ds.target_data\n
@@ -123,2 +123 @@ class DataAugmenter(object):\n
-        U_train_copy, Utarget_train_copy = jiterringobject.compute_noise_injection(U_train_copy, Utarget_train_copy, ds, DATA_TO_PREDICT, mask_inject, out_dim, alpha)\n
-        #U_train_copy,Utarget_train_copy = self.compute_noise_injection(U_train_copy,Utarget_train_copy,ds,DATA_TO_PREDICT,mask_inject,out_dim,alpha)\n
+        U_train_copy, Utarget_train_copy = jiterringobject.compute_noise_injection(U_train_copy, Utarget_train_copy, ds, self.target_data, mask_inject, out_dim, alpha)\n
diff --git a/dataset.py b/dataset.py\n
index cbf6e0c..e416326 100644\n
--- a/dataset.py\n
+++ b/dataset.py\n
@@ -592,0 +593 @@ class PersonnalInput(DataSet):\n
+        self.target_data = arg_parser.target_data\n
diff --git a/dl_models/STGCN/get_gso.py b/dl_models/STGCN/get_gso.py\n
index 1003c4a..7213473 100644\n
--- a/dl_models/STGCN/get_gso.py\n
+++ b/dl_models/STGCN/get_gso.py\n
@@ -14 +13,0 @@ from dl_models.STGCN.STGCN_utilities import calc_chebynet_gso,calc_gso\n
-from constants.paths import DATA_TO_PREDICT\n
@@ -31 +30 @@ def get_output_kernel_size(args):\n
-    if  not (DATA_TO_PREDICT in args.dataset_names):\n
+    if  not (args.target_data in args.dataset_names):\n
@@ -58 +57 @@ def get_block_dims(args,Ko):\n
-        if  not(DATA_TO_PREDICT in args.dataset_names) and is_condition(args):\n
+        if  not(args.target_data in args.dataset_names) and is_condition(args):\n
diff --git a/dl_models/full_model.py b/dl_models/full_model.py\n
index f02ac1a..1172072 100644\n
--- a/dl_models/full_model.py\n
+++ b/dl_models/full_model.py\n
@@ -30 +29,0 @@ from build_inputs.load_adj import load_adj\n
-from constants.paths import DATA_TO_PREDICT\n
@@ -67 +66 @@ class full_model(nn.Module):\n
-        if DATA_TO_PREDICT in args.dataset_names :\n
+        if dataset.target_data in args.dataset_names :\n
diff --git a/dl_models/vision_models/ImageAvgPooling/load_config.py b/dl_models/vision_models/ImageAvgPooling/load_config.py\n
index 4fbf866..9505912 100644\n
--- a/dl_models/vision_models/ImageAvgPooling/load_config.py\n
+++ b/dl_models/vision_models/ImageAvgPooling/load_config.py\n
@@ -6 +6 @@ Have to define :\n
->> get_config() which return 'args', a NameSpace containing the hyperparameter of feature extractor model updated thank to parameters from the backbone model and DATA_TO_PREDICT\n
+>> get_config() which return 'args', a NameSpace containing the hyperparameter of feature extractor model updated thank to parameters from the backbone model and args.target_data\n
diff --git a/examples/.ipynb_checkpoints/benchmark-checkpoint.py b/examples/.ipynb_checkpoints/benchmark-checkpoint.py\n
index 22dfde7..2fe2ec8 100644\n
--- a/examples/.ipynb_checkpoints/benchmark-checkpoint.py\n
+++ b/examples/.ipynb_checkpoints/benchmark-checkpoint.py\n
@@ -90 +90 @@ if __name__ == '__main__':\n
-\n
+    target_data = 'subway_in'\n
@@ -97,2 +97 @@ if __name__ == '__main__':\n
-        from constants.paths import DATA_TO_PREDICT\n
-        assert DATA_TO_PREDICT in dataset_names, f'You are trying to predict {DATA_TO_PREDICT} with only these data: {dataset_names}'\n
+        assert target_data in dataset_names, f'You are trying to predict {target_data} with only these data: {dataset_names}'\n
diff --git a/examples/benchmark.py b/examples/benchmark.py\n
index 78dbb34..2a5a00c 100644\n
--- a/examples/benchmark.py\n
+++ b/examples/benchmark.py\n
@@ -117 +116,0 @@ if __name__ == '__main__':\n
-        from constants.paths import DATA_TO_PREDICT\n
diff --git a/examples/load_best_config.py b/examples/load_best_config.py\n
index 4bf8ec3..e2da85d 100644\n
--- a/examples/load_best_config.py\n
+++ b/examples/load_best_config.py\n
@@ -15,3 +14,0 @@ from constants.paths import SAVE_DIRECTORY\n
-from utils.utilities_DL import get_loss,load_model_and_optimizer\n
-from build_inputs.load_datasets_to_predict import load_datasets_to_predict\n
-from calendar_class import get_time_slots_labels\n
@@ -111,11 +107,0 @@ def get_trainer_and_ds_from_saved_trial(trial_id,add_name_id,save_folder,modific\n
-\n
-\n
-if __name__ == '__main__':\n
-    args = load_best_config(trial_id = 'subway_in_STGCN_MSELoss_2024_08_21_14_50_2810',folder = 'save/HyperparameterTuning',metric = '_metric/Loss_model')\n
-    # Load model with the best config:\n
-    dataset,_,_,_ = load_datasets_to_predict(args,coverage_period)\n
-    _,dic_class2rpz,_,_ = get_time_slots_labels(dataset,nb_class = [0,1,2,3])\n
-    loss_function = get_loss(args)\n
-    model,optimizer,scheduler = load_model_and_optimizer(args,dic_class2rpz)\n
-\n
-    print(model)\n
\\ No newline at end of file\n
diff --git a/examples/test_tmps.py b/examples/test_tmps.py\n
index ee1f250..588199e 100644\n
--- a/examples/test_tmps.py\n
+++ b/examples/test_tmps.py\n
@@ -19 +19 @@ if True:\n
-    save_folder = 'K_fold_validation/training_with_HP_tuning/subway_in_only'\n
+    save_folder = 'K_fold_validation/training_with_HP_tuning/ASTGCN_2025_04_21_20_06_76371'\n
@@ -22,2 +22,2 @@ if True:\n
-    trial_id = 'subway_in_STGCN_MSELoss_2025_01_20_14_27_20569'\n
-    epochs_validation = 100\n
+    trial_id = 'subway_in_subway_out_ASTGCN_MSELoss_2025_04_21_20_06_76371'\n
+    epochs_validation = 160\n
@@ -28,10 +28,7 @@ if True:\n
-                    'standardize': False,\n
-                    'minmaxnorm':True,\n
-\n
-                    'learnable_adj_matrix' : False,\n
-                    \n
-                    'stacked_contextual': True,\n
-                    'temporal_graph_transformer_encoder': False,\n
-                    'compute_node_attr_with_attn': False,\n
-\n
-                    'freq': '15min',\n
+                    #'standardize': False,\n
+                    #'minmaxnorm':True,\n
+                    #'learnable_adj_matrix' : False,\n
+                    #'stacked_contextual': True,\n
+                    #'temporal_graph_transformer_encoder': False,\n
+                    #'compute_node_attr_with_attn': False,\n
+                    #'freq': '15min',\n
@@ -40,11 +37,27 @@ if True:\n
-    config_diffs = {'_1':{},  \n
-                    '_2':{},  \n
-                    '_3':{},  \n
-                    '_4':{},  \n
-                    '_5':{},   \n
-                    '_6':{},  \n
-                    '_7':{},  \n
-                    '_8':{},  \n
-                    '_9':{},  \n
-                    '_10':{},\n
-                    }\n
+    config_diffs = {'subway_in_subway_out':{}, \n
+                    'subway_in_subway_in':{'dataset_names':['subway_in','subway_in']},  \n
+                    'station_epsilon100_Google_Maps':{'dataset_names':['subway_in','netmob_POIs'],\n
+                                                    'NetMob_only_epsilon': True,   \n
+                                                    'NetMob_selected_apps':  ['Google_Maps'],\n
+                                                    'NetMob_transfer_mode' :  ['DL'],\n
+                                                    'NetMob_selected_tags': ['station_epsilon100'],\n
+                                                    'NetMob_expanded' : ''},\n
+                    'station_epsilon100_Web_Weather':{'dataset_names':['subway_in','netmob_POIs'],\n
+                                                    'NetMob_only_epsilon': True,   \n
+                                                    'NetMob_selected_apps':  ['Web_Weather'],\n
+                                                    'NetMob_transfer_mode' :  ['DL'],\n
+                                                    'NetMob_selected_tags': ['station_epsilon100'],\n
+                                                    'NetMob_expanded' : ''},\n
+                    'station_epsilon100_Web_Downloads':{'dataset_names':['subway_in','netmob_POIs'],\n
+                                                    'NetMob_only_epsilon': True,   \n
+                                                    'NetMob_selected_apps':  ['Web_Downloads'],\n
+                                                    'NetMob_transfer_mode' :  ['DL'],\n
+                                                    'NetMob_selected_tags': ['station_epsilon100'],\n
+                                                    'NetMob_expanded' : ''},\n
+                    'station_epsilon100_Web_Deezer':{'dataset_names':['subway_in','netmob_POIs'],\n
+                                                    'NetMob_only_epsilon': True,   \n
+                                                    'NetMob_selected_apps':  ['Deezer'],\n
+                                                    'NetMob_transfer_mode' :  ['DL'],\n
+                                                    'NetMob_selected_tags': ['station_epsilon100'],\n
+                                                    'NetMob_expanded' : ''}\n
+                                                      }\n
diff --git a/load_inputs/criter.py b/load_inputs/criter.py\n
new file mode 100644\n
index 0000000..7fddf11\n
--- /dev/null\n
+++ b/load_inputs/criter.py\n
@@ -0,0 +1,204 @@\n
+import sys\n
+import os\n
+import pandas as pd\n
+import torch\n
+import numpy as np\n
+from datetime import datetime\n
+import glob # Pour lister les fichiers\n
+\n
+# --- Gestion de l'arborescence ---\n
+current_file_path = os.path.abspath(os.path.dirname(__file__))\n
+parent_dir = os.path.abspath(os.path.join(current_file_path, '..'))\n
+if parent_dir not in sys.path:\n
+    sys.path.insert(0, parent_dir)\n
+\n
+# --- Importations personnalisées ---\n
+from dataset import DataSet, PersonnalInput\n
+from utils.utilities import filter_args # Assurez-vous que ce chemin est correct\n
+# Import de votre fonction spécifique pour CRITER\n
+try:\n
+    # Ajustez le chemin si nécessaire\n
+    from build_inputs.load_raw_data import load_CRITER\n
+except ImportError:\n
+    print(\"ERREUR: Impossible d'importer 'load_CRITER'. Vérifiez le chemin dans 'load_inputs/criter.py'\")\n
+    # Définir une fonction factice pour éviter les erreurs si l'import échoue\n
+    def load_CRITER(file_path):\n
+        print(f\"AVERTISSEMENT: load_CRITER non trouvé, chargement de {file_path} échouera.\")\n
+        # Retourne un DataFrame vide avec les colonnes attendues pour éviter des erreurs aval\n
+        return pd.DataFrame(columns=['HORODATE', 'ID_POINT_MESURE', 'DEBIT_HEURE'])\n
+\n
+\n
+# --- Constantes spécifiques à cette donnée ---\n
+FILE_BASE_NAME = 'CRITER'\n
+DATA_SUBFOLDER_PATTERN = 'Comptages_Velo_Routier/CRITER/6 min {year}' # Sera formaté avec l'année\n
+\n
+NATIVE_FREQ = '6min' # La fréquence des fichiers bruts\n
+# Couverture théorique\n
+START = '2019-01-01' # Exemple basé sur head()\n
+END = '2020-01-01'\n
+# Liste des périodes invalides\n
+list_of_invalid_period = []\n
+\n
+C = 1 # Nombre de canaux/features par unité spatiale\n
+\n
+# Colonnes attendues DANS LE RETOUR de load_CRITER (à adapter si load_CRITER retourne autre chose)\n
+DATE_COL = 'HORODATE'\n
+LOCATION_COL = 'ID_POINT_MESURE'\n
+VALUE_COL = 'DEBIT_HEURE' # Ou une autre colonne pertinente retournée par load_CRITER\n
+\n
+def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_period, args, normalize=True):\n
+    \"\"\"\n
+    Charge, concatène, pivote, filtre et pré-traite les données CRITER.\n
+    \"\"\"\n
+    target_freq = args.freq\n
+    # CRITER a une fréquence native de 6min. On chargera ces fichiers puis on resamplera.\n
+    native_freq_criter = NATIVE_FREQ\n
+\n
+    # Déterminer les années couvertes par intersect_coverage_period\n
+    min_date = intersect_coverage_period.min()\n
+    max_date = intersect_coverage_period.max()\n
+    years_to_load = range(min_date.year, max_date.year + 1)\n
+\n
+    all_files_to_load = []\n
+    for year in years_to_load:\n
+        criter_path_year = os.path.join(ROOT, FOLDER_PATH, DATA_SUBFOLDER_PATTERN.format(year=year))\n
+        # Lister tous les fichiers .txt dans le dossier de l'année\n
+        # Attention: ceci charge tous les fichiers de l'année, même ceux hors période.\n
+        # Un filtrage plus fin sur les noms de fichiers pourrait être nécessaire si beaucoup de fichiers.\n
+        files_in_year = sorted(glob.glob(os.path.join(criter_path_year, \"*.txt\")))\n
+        if not files_in_year:\n
+             print(f\"AVERTISSEMENT: Aucun fichier CRITER trouvé pour l'année {year} dans {criter_path_year}\")\n
+        all_files_to_load.extend(files_in_year)\n
+\n
+    if not all_files_to_load:\n
+        print(f\"ERREUR: Aucun fichier CRITER trouvé pour les années {list(years_to_load)} dans {os.path.join(ROOT, FOLDER_PATH)}\")\n
+        return None\n
+\n
+    print(f\"Chargement de {len(all_files_to_load)} fichiers CRITER...\")\n
+    list_df_criter = []\n
+    for f_path in all_files_to_load:\n
+        try:\n
+            df_temp = load_CRITER(f_path)\n
+            list_df_criter.append(df_temp)\n
+        except Exception as e:\n
+            print(f\"ERREUR lors du chargement du fichier CRITER {f_path} avec load_CRITER : {e}\")\n
+            # Optionnel : décider si on continue sans ce fichier ou si on arrête tout\n
+\n
+    if not list_df_criter:\n
+        print(\"ERREUR: Aucun DataFrame CRITER n'a pu être chargé.\")\n
+        return None\n
+\n
+    # Concaténer tous les DataFrames chargés\n
+    print(\"Concaténation des DataFrames CRITER...\")\n
+    df = pd.concat(list_df_criter, ignore_index=True)\n
+\n
+    # --- Prétraitement ---\n
+    try:\n
+        # S'assurer que la colonne date est bien en datetime\n
+        # (load_CRITER devrait idéalement déjà le faire)\n
+        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n
+\n
+        # Garder uniquement les colonnes nécessaires pour le pivot\n
+        df = df[[DATE_COL, LOCATION_COL, VALUE_COL]].copy()\n
+        \n
+        # Supprimer les doublons potentiels (même point, même horodate) avant pivot\n
+        df = df.drop_duplicates(subset=[DATE_COL, LOCATION_COL], keep='first')\n
+\n
+\n
+        # Pivoter le DataFrame\n
+        print(\"Pivotage du DataFrame CRITER...\")\n
+        df_pivoted = df.pivot_table(index=DATE_COL, columns=LOCATION_COL, values=VALUE_COL, aggfunc='sum') # aggfunc='sum' ou 'mean'?\n
+\n
+        # Remplacer les NaN par 0\n
+        df_pivoted = df_pivoted.fillna(0)\n
+\n
+        # S'assurer que l'index est un DatetimeIndex\n
+        df_pivoted.index = pd.to_datetime(df_pivoted.index)\n
+\n
+        # Ré-échantillonage si target_freq est différent de 6min\n
+        if target_freq != native_freq_criter:\n
+            print(f\"Ré-échantillonage de {native_freq_criter} vers {target_freq}...\")\n
+            try:\n
+                 # Vérifier si la fréquence cible est plus grossière\n
+                 if pd.to_timedelta(target_freq) >= pd.to_timedelta(native_freq_criter):\n
+                     df_pivoted = df_pivoted.resample(target_freq).sum() # 'sum' car ce sont des débits/comptes?\n
+                 else:\n
+                     print(f\"AVERTISSEMENT: La fréquence cible {target_freq} est plus fine que la fréquence native {native_freq_criter}. Aucun ré-échantillonage effectué.\")\n
+            except ValueError as e:\n
+                 print(f\"ERREUR lors de la tentative de ré-échantillonage vers {target_freq}: {e}\")\n
+                 # Continuer avec la fréquence native ? Ou retourner une erreur ?\n
+                 # return None # Option plus sûre\n
+\n
+        # Filtrage temporel basé sur l'intersection\n
+        print(f\"Filtrage temporel CRITER sur {len(intersect_coverage_period)} dates...\")\n
+        df_filtered = df_pivoted[df_pivoted.index.isin(intersect_coverage_period)].copy()\n
+        local_df_dates = pd.DataFrame(df_filtered.index, columns=['date'])\n
+\n
+        if df_filtered.empty:\n
+             print(f\"ERREUR : Aucune donnée CRITER restante après filtrage temporel.\")\n
+             # ... (messages d'erreur comme ci-dessus) ...\n
+             return None\n
+\n
+        print(f\"Données CRITER filtrées. Dimensions: {df_filtered.shape}\")\n
+\n
+        # Conversion en Tensor\n
+        data_T = torch.tensor(df_filtered.values).float()\n
+\n
+    except KeyError as e:\n
+        print(f\"ERREUR: Colonne manquante dans les données CRITER retournées par load_CRITER : {e}.\")\n
+        return None\n
+    except Exception as e:\n
+        print(f\"ERREUR pendant le prétraitement des données CRITER : {e}\")\n
+        return None\n
+\n
+    # --- Création et Prétraitement avec PersonnalInput ---\n
+    print(\"Création et prétraitement de l'objet PersonnalInput pour CRITER...\")\n
+    processed_input = load_input_and_preprocess(\n
+        dims=[0],\n
+        normalize=normalize,\n
+        invalid_dates=invalid_dates,\n
+        args=args,\n
+        data_T=data_T,\n
+        dataset=dataset,\n
+        df_dates=local_df_dates\n
+    )\n
+\n
+    if processed_input is None: return None\n
+\n
+    # --- Finalisation Métadonnées ---\n
+    processed_input.spatial_unit = df_filtered.columns.tolist()\n
+    processed_input.C = C\n
+    processed_input.periods = None\n
+\n
+    print(f\"Chargement et prétraitement de {FILE_BASE_NAME} terminés.\")\n
+    return processed_input\n
+\n
+\n
+# Definition de load_input_and_preprocess (identique à subway_indiv.py)\n
+def load_input_and_preprocess(dims, normalize, invalid_dates, args, data_T, dataset, df_dates):\n
+    \"\"\"\n
+    Fonction helper pour instancier PersonnalInput à partir d'un Tensor\n
+    et appeler preprocess.\n
+    \"\"\"\n
+    args_DataSet = filter_args(DataSet, args)\n
+    try:\n
+        personal_instance = PersonnalInput(\n
+            invalid_dates, args, tensor=data_T, dates=df_dates,\n
+            time_step_per_hour=getattr(dataset, 'time_step_per_hour', None),\n
+            dims=dims, **args_DataSet\n
+        )\n
+        print(\"Appel de la méthode preprocess...\")\n
+        personal_instance.preprocess(\n
+            args.train_prop, args.valid_prop, args.test_prop,\n
+            args.train_valid_test_split_method, normalize=normalize\n
+        )\n
+        print(\"Méthode preprocess terminée.\")\n
+        return personal_instance\n
+    except Exception as e:\n
+        print(f\"ERREUR lors de l'instanciation ou preprocess de PersonnalInput : {e}\")\n
+        import traceback\n
+        traceback.print_exc()\n
+        return None\n
+\n
+# --- Point d'entrée pour exécution directe (optionnel, pour tests) ---\n
+# ... (Similaire à subway_indiv.py, adapter les mocks, notamment pour ROOT et FOLDER_PATH pointant vers les données CRITER) ...\n
\\ No newline at end of file\n
diff --git a/load_inputs/netmob_POIs.py b/load_inputs/netmob_POIs.py\n
index d0a8bc1..be90e08 100644\n
--- a/load_inputs/netmob_POIs.py\n
+++ b/load_inputs/netmob_POIs.py\n
@@ -17 +17,3 @@ from utils.utilities import filter_args\n
-from build_inputs.load_netmob_data import find_positions,replace_heure_d_ete\n
+from build_inputs.load_contextual_data import find_positions,replace_heure_d_ete\n
+from utils.utilities import get_time_step_per_hour\n
+\n
@@ -39 +41 @@ list_of_invalid_period.append([datetime(2019,5,23,0,0),datetime(2019,5,25,6,0)])\n
-def load_data(dataset,ROOT,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize= True): # args,ROOT,FOLDER_PATH,coverage_period = None\n
+def load_data(ROOT,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize= True): # args,ROOT,FOLDER_PATH,coverage_period = None\n
@@ -68,0 +71 @@ def load_data(dataset,ROOT,FOLDER_PATH,invalid_dates,intersect_coverage_period,a\n
+    \"\"\"\n
@@ -71 +74 @@ def load_data(dataset,ROOT,FOLDER_PATH,invalid_dates,intersect_coverage_period,a\n
-    # Reduce dimensionality : [T',R] -> [T',R']\n
+    \"\"\"\n
@@ -73 +76 @@ def load_data(dataset,ROOT,FOLDER_PATH,invalid_dates,intersect_coverage_period,a\n
-    \n
+    # Reduce dimensionality : [T',R] -> [T',R']\n
@@ -80 +83 @@ def load_data(dataset,ROOT,FOLDER_PATH,invalid_dates,intersect_coverage_period,a\n
-    NetMob_POI = load_input_and_preprocess(dims = dims,normalize=normalize,invalid_dates=invalid_dates,args=args,netmob_T=netmob_T,dataset=dataset,df_dates = local_df_dates)\n
+    NetMob_POI = load_input_and_preprocess(dims = dims,normalize=normalize,invalid_dates=invalid_dates,args=args,netmob_T=netmob_T,intersect_coverage_period = intersect_coverage_period) \n
@@ -84 +86,0 @@ def load_data(dataset,ROOT,FOLDER_PATH,invalid_dates,intersect_coverage_period,a\n
-\n
@@ -104,3 +106,3 @@ def load_data_npy(ROOT,FOLDER_PATH,args):\n
-def load_input_and_preprocess(dims,normalize,invalid_dates,args,netmob_T,dataset,df_dates=None):\n
-    if df_dates is None:\n
-        df_dates = dataset.df_dates\n
+def load_input_and_preprocess(dims,normalize,invalid_dates,args,netmob_T,intersect_coverage_period):\n
+    df_dates = pd.DataFrame(intersect_coverage_period)\n
+    df_dates.columns = ['date']\n
@@ -113 +115 @@ def load_input_and_preprocess(dims,normalize,invalid_dates,args,netmob_T,dataset\n
-                           time_step_per_hour = dataset.time_step_per_hour,\n
+                            time_step_per_hour = get_time_step_per_hour(args.freq),\n
diff --git a/load_inputs/netmob_POIs_per_station.py b/load_inputs/netmob_POIs_per_station.py\n
index c457a11..9a2b52b 100644\n
--- a/load_inputs/netmob_POIs_per_station.py\n
+++ b/load_inputs/netmob_POIs_per_station.py\n
@@ -17 +17 @@ from utils.utilities import filter_args\n
-from build_inputs.load_netmob_data import find_positions,replace_heure_d_ete\n
+from build_inputs.load_contextual_data import find_positions,replace_heure_d_ete\n
diff --git a/load_inputs/netmob_image_per_station.py b/load_inputs/netmob_image_per_station.py\n
index 32c98ab..e222611 100644\n
--- a/load_inputs/netmob_image_per_station.py\n
+++ b/load_inputs/netmob_image_per_station.py\n
@@ -15 +15 @@ import pickle\n
-from build_inputs.load_netmob_data import find_positions,replace_heure_d_ete\n
+from build_inputs.load_contextual_data import find_positions,replace_heure_d_ete\n
diff --git a/load_inputs/netmob_video_lyon.py b/load_inputs/netmob_video_lyon.py\n
index f114184..8f295fd 100644\n
--- a/load_inputs/netmob_video_lyon.py\n
+++ b/load_inputs/netmob_video_lyon.py\n
@@ -14 +14 @@ import pickle\n
-from build_inputs.load_netmob_data import find_positions,replace_heure_d_ete\n
+from build_inputs.load_contextual_data import find_positions,replace_heure_d_ete\n
diff --git a/load_inputs/subway_in.py b/load_inputs/subway_in.py\n
index d7fe478..9993517 100644\n
--- a/load_inputs/subway_in.py\n
+++ b/load_inputs/subway_in.py\n
@@ -10 +10 @@ if parent_dir not in sys.path:\n
-from dataset import DataSet\n
+from dataset import DataSet,PersonnalInput\n
@@ -13 +12,0 @@ from utils.utilities import filter_args,get_time_step_per_hour\n
-\n
@@ -39 +38,20 @@ n_vertex = 40\n
-def load_data(args,ROOT,FOLDER_PATH,coverage_period = None,filename=None):\n
+def load_data(ROOT,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize= True,filename=None):\n
+    dataset = load_DataSet(args,ROOT,FOLDER_PATH,coverage_period = intersect_coverage_period,filename=filename)\n
+    args_DataSet = filter_args(DataSet, args)\n
+\n
+    preprocesed_ds = PersonnalInput(invalid_dates, args,tensor = dataset.raw_values, dates = dataset.df_dates, \n
+                                    spatial_unit = dataset.spatial_unit,\n
+                                    indices_spatial_unit = dataset.indices_spatial_unit,\n
+                                    time_step_per_hour = dataset.time_step_per_hour,\n
+                                    city = dataset.city,\n
+                                    dims=dataset.dims,\n
+                                    periods = dataset.periods,\n
+                                     **args_DataSet\n
+                                     )\n
+\n
+    preprocesed_ds.preprocess(args.train_prop,args.valid_prop,args.test_prop,args.train_valid_test_split_method,normalize)\n
+\n
+    return preprocesed_ds\n
+\n
+\n
+def load_DataSet(args,ROOT,FOLDER_PATH,coverage_period = None,filename=None):\n
diff --git a/load_inputs/subway_indiv.py b/load_inputs/subway_indiv.py\n
new file mode 100644\n
index 0000000..bda9224\n
--- /dev/null\n
+++ b/load_inputs/subway_indiv.py\n
@@ -0,0 +1,186 @@\n
+import sys\n
+import os\n
+import pandas as pd\n
+import torch\n
+import numpy as np\n
+from datetime import datetime\n
+\n
+# --- Gestion de l'arborescence ---\n
+current_file_path = os.path.abspath(os.path.dirname(__file__))\n
+parent_dir = os.path.abspath(os.path.join(current_file_path, '..'))\n
+if parent_dir not in sys.path:\n
+    sys.path.insert(0, parent_dir)\n
+\n
+# --- Importations personnalisées ---\n
+from dataset import DataSet, PersonnalInput\n
+from utils.utilities import filter_args # Assurez-vous que ce chemin est correct\n
+\n
+# --- Constantes spécifiques à cette donnée ---\n
+# Le nom de fichier sera construit dynamiquement basé sur args.freq\n
+FILE_BASE_NAME = 'subway_indiv'\n
+DATA_SUBFOLDER = 'validation_individuelle' # Sous-dossier dans FOLDER_PATH\n
+\n
+# Fréquence native la plus fine disponible (pour info, le chargement dépendra de args.freq)\n
+NATIVE_FREQ = '3min'\n
+# Couverture théorique (à remplacer par les vraies dates si connues)\n
+START = '2019-10-01' # Exemple basé sur head()\n
+END = '2020-04-01'\n
+# Liste des périodes invalides (à compléter si nécessaire)\n
+list_of_invalid_period = []\n
+\n
+C = 1 # Nombre de canaux/features par unité spatiale\n
+\n
+# Colonnes attendues dans le CSV\n
+DATE_COL = 'VAL_DATE'\n
+LOCATION_COL = 'COD_TRG'\n
+VALUE_COL = 'Flow'\n
+\n
+def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_period, args, normalize=True):\n
+    \"\"\"\n
+    Charge, pivote, filtre et pré-traite les données subway_indiv.\n
+\n
+    Args:\n
+        dataset (DataSet): L'objet DataSet principal (pour contexte).\n
+        ROOT (str): Chemin racine du projet ou des données.\n
+        FOLDER_PATH (str): Chemin vers le dossier contenant DATA_SUBFOLDER.\n
+        invalid_dates (list): Liste des périodes invalides fournie globalement.\n
+        intersect_coverage_period (pd.DatetimeIndex): Période temporelle à conserver.\n
+        args (Namespace): Arguments globaux (contient args.freq).\n
+        normalize (bool): Faut-il normaliser les données.\n
+\n
+    Returns:\n
+        PersonnalInput: Objet contenant les données traitées.\n
+    \"\"\"\n
+    target_freq = args.freq\n
+    file_name = f\"{FILE_BASE_NAME}_{target_freq}\"\n
+    data_file_path = os.path.join(ROOT, FOLDER_PATH, DATA_SUBFOLDER, file_name, f\"{file_name}.csv\")\n
+\n
+    print(f\"Chargement des données depuis : {data_file_path}\")\n
+    try:\n
+        df = pd.read_csv(data_file_path)\n
+    except FileNotFoundError:\n
+        print(f\"ERREUR : Le fichier {data_file_path} n'a pas été trouvé.\")\n
+        print(f\"Vérifiez que la fréquence '{target_freq}' existe pour {FILE_BASE_NAME} et que les chemins sont corrects.\")\n
+        return None\n
+    except Exception as e:\n
+        print(f\"ERREUR lors du chargement du fichier {file_name}.csv: {e}\")\n
+        return None\n
+\n
+    # --- Prétraitement ---\n
+    try:\n
+        # Convertir en datetime\n
+        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n
+\n
+        # Pivoter le DataFrame\n
+        print(\"Pivotage du DataFrame...\")\n
+        df_pivoted = df.pivot_table(index=DATE_COL, columns=LOCATION_COL, values=VALUE_COL, aggfunc='sum')\n
+\n
+        # Remplacer les NaN (dus au pivot) par 0\n
+        df_pivoted = df_pivoted.fillna(0)\n
+\n
+        # S'assurer que l'index est un DatetimeIndex\n
+        df_pivoted.index = pd.to_datetime(df_pivoted.index)\n
+\n
+        # Ré-échantillonage (si args.freq est différent de la freq native du fichier ET plus grossier - rare ici car on charge le fichier exact)\n
+        # Note: Normalement, on charge le fichier correspondant à args.freq, donc pas besoin de resample ici.\n
+        # Laissez ce bloc commenté sauf si vous avez besoin de charger une fréquence plus fine et de la ré-échantillonner.\n
+        # current_freq = pd.infer_freq(df_pivoted.index) # Ou extraire de NATIVE_FREQ/target_freq\n
+        # if target_freq != current_freq and pd.to_timedelta(target_freq) > pd.to_timedelta(current_freq):\n
+        #    print(f\"Ré-échantillonage de {current_freq} vers {target_freq}...\")\n
+        #    df_pivoted = df_pivoted.resample(target_freq).sum() # ou .mean() selon la donnée\n
+\n
+        # Filtrage temporel basé sur l'intersection\n
+        print(f\"Filtrage temporel sur {len(intersect_coverage_period)} dates...\")\n
+        # Assurez-vous que les deux index sont bien des DatetimeIndex\n
+        df_filtered = df_pivoted[df_pivoted.index.isin(intersect_coverage_period)].copy()\n
+        local_df_dates = pd.DataFrame(df_filtered.index, columns=['date'])\n
+\n
+        if df_filtered.empty:\n
+             print(f\"ERREUR : Aucune donnée restante après filtrage temporel pour {file_name}.csv\")\n
+             print(f\"Vérifiez la couverture de intersect_coverage_period ({intersect_coverage_period.min()} - {intersect_coverage_period.max()})\")\n
+             print(f\"et la couverture du fichier chargé ({df_pivoted.index.min()} - {df_pivoted.index.max()})\")\n
+             return None\n
+\n
+        print(f\"Données filtrées. Dimensions: {df_filtered.shape}\")\n
+\n
+        # Conversion en Tensor\n
+        data_T = torch.tensor(df_filtered.values).float()\n
+\n
+    except KeyError as e:\n
+        print(f\"ERREUR: Colonne manquante dans {file_name}.csv : {e}. Vérifiez DATE_COL, LOCATION_COL, VALUE_COL.\")\n
+        return None\n
+    except Exception as e:\n
+        print(f\"ERREUR pendant le prétraitement des données {file_name}.csv: {e}\")\n
+        return None\n
+\n
+\n
+    # --- Création et Prétraitement avec PersonnalInput ---\n
+    # Utilisation de la fonction helper locale\n
+    print(\"Création et prétraitement de l'objet PersonnalInput...\")\n
+    processed_input = load_input_and_preprocess(\n
+        dims=[0], # Normalisation sur la dimension temporelle par défaut\n
+        normalize=normalize,\n
+        invalid_dates=invalid_dates, # Utilise les invalid_dates globales passées\n
+        args=args,\n
+        data_T=data_T,\n
+        dataset=dataset, # Passe le dataset principal pour contexte\n
+        df_dates=local_df_dates\n
+    )\n
+\n
+    if processed_input is None:\n
+        return None\n
+\n
+    # --- Finalisation Métadonnées ---\n
+    processed_input.spatial_unit = df_filtered.columns.tolist()\n
+    processed_input.C = C\n
+    processed_input.periods = None # Pas de périodicité spécifique définie ici\n
+\n
+    print(f\"Chargement et prétraitement de {FILE_BASE_NAME} terminés.\")\n
+    return processed_input\n
+\n
+def load_input_and_preprocess(dims, normalize, invalid_dates, args, data_T, dataset, df_dates):\n
+    \"\"\"\n
+    Fonction helper pour instancier PersonnalInput à partir d'un Tensor\n
+    et appeler preprocess.\n
+    \"\"\"\n
+    # Filtrer les arguments de args qui sont pertinents pour DataSet/PersonnalInput\n
+    args_DataSet = filter_args(DataSet, args)\n
+\n
+    try:\n
+        # Instancier PersonnalInput avec le Tensor\n
+        # Note: on passe le time_step_per_hour et potentiellement d'autres\n
+        # attributs de contexte depuis le 'dataset' principal.\n
+        personal_instance = PersonnalInput(\n
+            invalid_dates, # Les dates invalides spécifiques à cette donnée (ou globales)\n
+            args,          # L'objet args complet\n
+            tensor=data_T,\n
+            dates=df_dates,\n
+            time_step_per_hour=getattr(dataset, 'time_step_per_hour', None), # Hérite du dataset principal\n
+            dims=dims,     # Dimensions pour la normalisation éventuelle\n
+            # minmaxnorm=getattr(dataset, 'minmaxnorm', None),   # Hérite si besoin\n
+            # standardize=getattr(dataset, 'standardize', None), # Hérite si besoin\n
+            **args_DataSet # Arguments filtrés de args pour DataSet\n
+        )\n
+\n
+        # Appeler la méthode preprocess de l'instance\n
+        print(\"Appel de la méthode preprocess...\")\n
+        personal_instance.preprocess(\n
+            args.train_prop,\n
+            args.valid_prop,\n
+            args.test_prop,\n
+            args.train_valid_test_split_method,\n
+            normalize=normalize\n
+        )\n
+        print(\"Méthode preprocess terminée.\")\n
+        return personal_instance\n
+\n
+    except Exception as e:\n
+        print(f\"ERREUR lors de l'instanciation ou preprocess de PersonnalInput : {e}\")\n
+        import traceback\n
+        traceback.print_exc()\n
+        return None\n
+\n
+\n
+if __name__ == \"__main__\":\n
+    # blabla\n
+    blabla\n
\\ No newline at end of file\n
diff --git a/load_inputs/subway_out.py b/load_inputs/subway_out.py\n
index 55a7a66..91f58c2 100644\n
--- a/load_inputs/subway_out.py\n
+++ b/load_inputs/subway_out.py\n
@@ -38,2 +38 @@ C = 1\n
-def load_data(dataset,args,ROOT,FOLDER_PATH,intersect_coverage_period,normalize,invalid_dates):\n
-    id_stations = dataset.spatial_unit\n
+def load_data(ROOT,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize):\n
@@ -41 +40 @@ def load_data(dataset,args,ROOT,FOLDER_PATH,intersect_coverage_period,normalize,\n
-    subway_out = load_data_from_subway_in_py(args,ROOT,FOLDER_PATH,intersect_coverage_period,filename = FILE_NAME)\n
+    subway_out = load_data_from_subway_in_py(ROOT,FOLDER_PATH,invalid_dates,intersect_coverage_period,args,normalize= normalize,filename = FILE_NAME)\n
@@ -49,2 +48 @@ def load_data(dataset,args,ROOT,FOLDER_PATH,intersect_coverage_period,normalize,\n
-\n
-    preprocessed_personal_input = load_input_and_preprocess(dims = dims,normalize=normalize,invalid_dates=invalid_dates,args=args,netmob_T=T_subway_out,dataset=dataset)\n
+    preprocessed_personal_input = load_input_and_preprocess(dims = dims,normalize=normalize,invalid_dates=invalid_dates,args=args,netmob_T=T_subway_out,intersect_coverage_period=intersect_coverage_period)\n
diff --git a/load_inputs/tramway_indiv.py b/load_inputs/tramway_indiv.py\n
new file mode 100644\n
index 0000000..7a2b4ee\n
--- /dev/null\n
+++ b/load_inputs/tramway_indiv.py\n
@@ -0,0 +1,147 @@\n
+import sys\n
+import os\n
+import pandas as pd\n
+import torch\n
+import numpy as np\n
+from datetime import datetime\n
+\n
+# --- Gestion de l'arborescence ---\n
+current_file_path = os.path.abspath(os.path.dirname(__file__))\n
+parent_dir = os.path.abspath(os.path.join(current_file_path, '..'))\n
+if parent_dir not in sys.path:\n
+    sys.path.insert(0, parent_dir)\n
+\n
+# --- Importations personnalisées ---\n
+from dataset import DataSet, PersonnalInput\n
+from utils.utilities import filter_args # Assurez-vous que ce chemin est correct\n
+\n
+# --- Constantes spécifiques à cette donnée ---\n
+DIRECTION = 'emitted'\n
+FILE_PATTERN = f'velov_{DIRECTION}_by_station' # Sera complété par args.freq\n
+DATA_SUBFOLDER = 'velov' # Sous-dossier dans FOLDER_PATH\n
+\n
+# Fréquence native la plus fine disponible (pour info)\n
+NATIVE_FREQ = '2min' # A vérifier, basé sur votre commentaire\n
+# Couverture théorique\n
+START = '2019-10-01' # Exemple basé sur head()\n
+END = '2020-04-01'\n
+# Liste des périodes invalides\n
+list_of_invalid_period = []\n
+\n
+C = 1 # Nombre de canaux/features par unité spatiale\n
+\n
+# Colonnes attendues dans le CSV\n
+DATE_COL = 'date_sortie' # Ou 'date_entree' pour 'attracted'?\n
+LOCATION_COL = 'id_sortie' # Ou 'id_entree' pour 'attracted'?\n
+VALUE_COL = 'volume'\n
+\n
+def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_period, args, normalize=True):\n
+    \"\"\"\n
+    Charge, pivote, filtre et pré-traite les données velov (emitted).\n
+    \"\"\"\n
+    target_freq = args.freq\n
+    # Construction spécifique du nom de fichier pour velov\n
+    file_name = f\"{FILE_PATTERN}{target_freq}\"\n
+    data_file_path = os.path.join(ROOT, FOLDER_PATH, DATA_SUBFOLDER, f\"{file_name}.csv\")\n
+\n
+    print(f\"Chargement des données depuis : {data_file_path}\")\n
+    try:\n
+        df = pd.read_csv(data_file_path)\n
+    except FileNotFoundError:\n
+        print(f\"ERREUR : Le fichier {data_file_path} n'a pas été trouvé.\")\n
+        print(f\"Vérifiez que la fréquence '{target_freq}' existe pour velov_{DIRECTION} et que les chemins sont corrects.\")\n
+        return None\n
+    except Exception as e:\n
+        print(f\"ERREUR lors du chargement du fichier {file_name}.csv: {e}\")\n
+        return None\n
+\n
+    # --- Prétraitement ---\n
+    try:\n
+        # Renommer pour utiliser les noms génériques (si différents)\n
+        # df = df.rename(columns={'date_sortie': DATE_COL, 'id_sortie': LOCATION_COL, 'volume': VALUE_COL})\n
+\n
+        # Convertir en datetime\n
+        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n
+\n
+        # Pivoter le DataFrame\n
+        print(\"Pivotage du DataFrame...\")\n
+        df_pivoted = df.pivot_table(index=DATE_COL, columns=LOCATION_COL, values=VALUE_COL, aggfunc='sum')\n
+\n
+        # Remplacer les NaN par 0\n
+        df_pivoted = df_pivoted.fillna(0)\n
+\n
+        # S'assurer que l'index est un DatetimeIndex\n
+        df_pivoted.index = pd.to_datetime(df_pivoted.index)\n
+\n
+        # Filtrage temporel basé sur l'intersection\n
+        print(f\"Filtrage temporel sur {len(intersect_coverage_period)} dates...\")\n
+        df_filtered = df_pivoted[df_pivoted.index.isin(intersect_coverage_period)].copy()\n
+        local_df_dates = pd.DataFrame(df_filtered.index, columns=['date'])\n
+\n
+        if df_filtered.empty:\n
+             print(f\"ERREUR : Aucune donnée restante après filtrage temporel pour {file_name}.csv\")\n
+             # ... (messages d'erreur comme ci-dessus) ...\n
+             return None\n
+\n
+        print(f\"Données filtrées. Dimensions: {df_filtered.shape}\")\n
+\n
+        # Conversion en Tensor\n
+        data_T = torch.tensor(df_filtered.values).float()\n
+\n
+    except KeyError as e:\n
+        print(f\"ERREUR: Colonne manquante dans {file_name}.csv : {e}. Vérifiez les noms de colonnes.\")\n
+        return None\n
+    except Exception as e:\n
+        print(f\"ERREUR pendant le prétraitement des données {file_name}.csv: {e}\")\n
+        return None\n
+\n
+    # --- Création et Prétraitement avec PersonnalInput ---\n
+    print(\"Création et prétraitement de l'objet PersonnalInput...\")\n
+    processed_input = load_input_and_preprocess(\n
+        dims=[0],\n
+        normalize=normalize,\n
+        invalid_dates=invalid_dates,\n
+        args=args,\n
+        data_T=data_T,\n
+        dataset=dataset,\n
+        df_dates=local_df_dates\n
+    )\n
+\n
+    if processed_input is None: return None\n
+\n
+    # --- Finalisation Métadonnées ---\n
+    processed_input.spatial_unit = df_filtered.columns.tolist()\n
+    processed_input.C = C\n
+    processed_input.periods = None\n
+\n
+    print(f\"Chargement et prétraitement de velov_{DIRECTION} terminés.\")\n
+    return processed_input\n
+\n
+# Definition de load_input_and_preprocess (identique à subway_indiv.py)\n
+def load_input_and_preprocess(dims, normalize, invalid_dates, args, data_T, dataset, df_dates):\n
+    \"\"\"\n
+    Fonction helper pour instancier PersonnalInput à partir d'un Tensor\n
+    et appeler preprocess.\n
+    \"\"\"\n
+    args_DataSet = filter_args(DataSet, args)\n
+    try:\n
+        personal_instance = PersonnalInput(\n
+            invalid_dates, args, tensor=data_T, dates=df_dates,\n
+            time_step_per_hour=getattr(dataset, 'time_step_per_hour', None),\n
+            dims=dims, **args_DataSet\n
+        )\n
+        print(\"Appel de la méthode preprocess...\")\n
+        personal_instance.preprocess(\n
+            args.train_prop, args.valid_prop, args.test_prop,\n
+            args.train_valid_test_split_method, normalize=normalize\n
+        )\n
+        print(\"Méthode preprocess terminée.\")\n
+        return personal_instance\n
+    except Exception as e:\n
+        print(f\"ERREUR lors de l'instanciation ou preprocess de PersonnalInput : {e}\")\n
+        import traceback\n
+        traceback.print_exc()\n
+        return None\n
+\n
+# --- Point d'entrée pour exécution directe (optionnel, pour tests) ---\n
+# ... (Similaire à subway_indiv.py, adapter les mocks) ...\n
\\ No newline at end of file\n
diff --git a/load_inputs/velov.py b/load_inputs/velov.py\n
new file mode 100644\n
index 0000000..8f8f10e\n
--- /dev/null\n
+++ b/load_inputs/velov.py\n
@@ -0,0 +1,147 @@\n
+import sys\n
+import os\n
+import pandas as pd\n
+import torch\n
+import numpy as np\n
+from datetime import datetime\n
+\n
+# --- Gestion de l'arborescence ---\n
+current_file_path = os.path.abspath(os.path.dirname(__file__))\n
+parent_dir = os.path.abspath(os.path.join(current_file_path, '..'))\n
+if parent_dir not in sys.path:\n
+    sys.path.insert(0, parent_dir)\n
+\n
+# --- Importations personnalisées ---\n
+from dataset import DataSet, PersonnalInput\n
+from utils.utilities import filter_args # Assurez-vous que ce chemin est correct\n
+\n
+# --- Constantes spécifiques à cette donnée ---\n
+DIRECTION = 'emitted'\n
+FILE_PATTERN = f'velov_{DIRECTION}_by_station' # Sera complété par args.freq\n
+DATA_SUBFOLDER = 'velov' # Sous-dossier dans FOLDER_PATH\n
+\n
+# Fréquence native la plus fine disponible (pour info)\n
+NATIVE_FREQ = '2min' # A vérifier, basé sur votre commentaire\n
+# Couverture théorique\n
+START = '2019-01-01' # Exemple basé sur head()\n
+END = '2020-01-01'\n
+# Liste des périodes invalides\n
+list_of_invalid_period = []\n
+\n
+C = 1 # Nombre de canaux/features par unité spatiale\n
+\n
+# Colonnes attendues dans le CSV\n
+DATE_COL = 'date_sortie' # Ou 'date_entree' pour 'attracted'?\n
+LOCATION_COL = 'id_sortie' # Ou 'id_entree' pour 'attracted'?\n
+VALUE_COL = 'volume'\n
+\n
+def load_data(dataset, ROOT, FOLDER_PATH, invalid_dates, intersect_coverage_period, args, normalize=True):\n
+    \"\"\"\n
+    Charge, pivote, filtre et pré-traite les données velov (emitted).\n
+    \"\"\"\n
+    target_freq = args.freq\n
+    # Construction spécifique du nom de fichier pour velov\n
+    file_name = f\"{FILE_PATTERN}{target_freq}\"\n
+    data_file_path = os.path.join(ROOT, FOLDER_PATH, DATA_SUBFOLDER, f\"{file_name}.csv\")\n
+\n
+    print(f\"Chargement des données depuis : {data_file_path}\")\n
+    try:\n
+        df = pd.read_csv(data_file_path)\n
+    except FileNotFoundError:\n
+        print(f\"ERREUR : Le fichier {data_file_path} n'a pas été trouvé.\")\n
+        print(f\"Vérifiez que la fréquence '{target_freq}' existe pour velov_{DIRECTION} et que les chemins sont corrects.\")\n
+        return None\n
+    except Exception as e:\n
+        print(f\"ERREUR lors du chargement du fichier {file_name}.csv: {e}\")\n
+        return None\n
+\n
+    # --- Prétraitement ---\n
+    try:\n
+        # Renommer pour utiliser les noms génériques (si différents)\n
+        # df = df.rename(columns={'date_sortie': DATE_COL, 'id_sortie': LOCATION_COL, 'volume': VALUE_COL})\n
+\n
+        # Convertir en datetime\n
+        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n
+\n
+        # Pivoter le DataFrame\n
+        print(\"Pivotage du DataFrame...\")\n
+        df_pivoted = df.pivot_table(index=DATE_COL, columns=LOCATION_COL, values=VALUE_COL, aggfunc='sum')\n
+\n
+        # Remplacer les NaN par 0\n
+        df_pivoted = df_pivoted.fillna(0)\n
+\n
+        # S'assurer que l'index est un DatetimeIndex\n
+        df_pivoted.index = pd.to_datetime(df_pivoted.index)\n
+\n
+        # Filtrage temporel basé sur l'intersection\n
+        print(f\"Filtrage temporel sur {len(intersect_coverage_period)} dates...\")\n
+        df_filtered = df_pivoted[df_pivoted.index.isin(intersect_coverage_period)].copy()\n
+        local_df_dates = pd.DataFrame(df_filtered.index, columns=['date'])\n
+\n
+        if df_filtered.empty:\n
+             print(f\"ERREUR : Aucune donnée restante après filtrage temporel pour {file_name}.csv\")\n
+             # ... (messages d'erreur comme ci-dessus) ...\n
+             return None\n
+\n
+        print(f\"Données filtrées. Dimensions: {df_filtered.shape}\")\n
+\n
+        # Conversion en Tensor\n
+        data_T = torch.tensor(df_filtered.values).float()\n
+\n
+    except KeyError as e:\n
+        print(f\"ERREUR: Colonne manquante dans {file_name}.csv : {e}. Vérifiez les noms de colonnes.\")\n
+        return None\n
+    except Exception as e:\n
+        print(f\"ERREUR pendant le prétraitement des données {file_name}.csv: {e}\")\n
+        return None\n
+\n
+    # --- Création et Prétraitement avec PersonnalInput ---\n
+    print(\"Création et prétraitement de l'objet PersonnalInput...\")\n
+    processed_input = load_input_and_preprocess(\n
+        dims=[0],\n
+        normalize=normalize,\n
+        invalid_dates=invalid_dates,\n
+        args=args,\n
+        data_T=data_T,\n
+        dataset=dataset,\n
+        df_dates=local_df_dates\n
+    )\n
+\n
+    if processed_input is None: return None\n
+\n
+    # --- Finalisation Métadonnées ---\n
+    processed_input.spatial_unit = df_filtered.columns.tolist()\n
+    processed_input.C = C\n
+    processed_input.periods = None\n
+\n
+    print(f\"Chargement et prétraitement de velov_{DIRECTION} terminés.\")\n
+    return processed_input\n
+\n
+# Definition de load_input_and_preprocess (identique à subway_indiv.py)\n
+def load_input_and_preprocess(dims, normalize, invalid_dates, args, data_T, dataset, df_dates):\n
+    \"\"\"\n
+    Fonction helper pour instancier PersonnalInput à partir d'un Tensor\n
+    et appeler preprocess.\n
+    \"\"\"\n
+    args_DataSet = filter_args(DataSet, args)\n
+    try:\n
+        personal_instance = PersonnalInput(\n
+            invalid_dates, args, tensor=data_T, dates=df_dates,\n
+            time_step_per_hour=getattr(dataset, 'time_step_per_hour', None),\n
+            dims=dims, **args_DataSet\n
+        )\n
+        print(\"Appel de la méthode preprocess...\")\n
+        personal_instance.preprocess(\n
+            args.train_prop, args.valid_prop, args.test_prop,\n
+            args.train_valid_test_split_method, normalize=normalize\n
+        )\n
+        print(\"Méthode preprocess terminée.\")\n
+        return personal_instance\n
+    except Exception as e:\n
+        print(f\"ERREUR lors de l'instanciation ou preprocess de PersonnalInput : {e}\")\n
+        import traceback\n
+        traceback.print_exc()\n
+        return None\n
+\n
+# --- Point d'entrée pour exécution directe (optionnel, pour tests) ---\n
+# ... (Similaire à subway_indiv.py, adapter les mocks) ...\n
\\ No newline at end of file\n
diff --git a/utils/utilities.py b/utils/utilities.py\n
index c4325ed..e87810f 100644\n
--- a/utils/utilities.py\n
+++ b/utils/utilities.py\n
@@ -110 +110 @@ def get_mode_date2path(df_list,df_names):\n
-            month = csv_path.split('/')[2].split('_')[0]\n
+            month = csv_path.split('/')[-1].split('_')[0]\n"
{"commit":"f522f81090ff4d743791a73e148283a0105cb804","author":"Fr-ocasting","date":"2025-04-22T11:22:53+02:00","message":"commit"},"diff":"diff --git a/examples/Total_evaluation_of_model.py b/examples/Total_evaluation_of_model.py\n
index b5f0280..f535db1 100644\n
--- a/examples/Total_evaluation_of_model.py\n
+++ b/examples/Total_evaluation_of_model.py\n
@@ -162 +162 @@ if __name__ == '__main__':\n
-        model_name = 'ASTGCN' #'CNN' # 'STGCN'\n
+        model_name = 'STGCN' #'CNN' # 'STGCN'\n"
{"commit":"9ebef206531adbcc3f1ada78cb3514b093123ea7","author":"Fr-ocasting","date":"2025-04-21T15:45:45+02:00","message":"add opening netmob save"},"diff":"diff --git a/HP_tuning/hyperparameter_tuning_ray.py b/HP_tuning/hyperparameter_tuning_ray.py\n
index 5466b50..1377354 100644\n
--- a/HP_tuning/hyperparameter_tuning_ray.py\n
+++ b/HP_tuning/hyperparameter_tuning_ray.py\n
@@ -33,4 +33 @@ def HP_modification(config,args):\n
-            if key == 'HP_max_epochs':\n
-                setattr(args, 'epochs',config['HP_max_epochs'])\n
-                \n
-            elif key == 'scheduler':\n
+            if key == 'scheduler':\n
diff --git a/examples/benchmark.py b/examples/benchmark.py\n
index 89c374d..78dbb34 100644\n
--- a/examples/benchmark.py\n
+++ b/examples/benchmark.py\n
@@ -37,0 +38,4 @@ def local_get_args(model_name,args_init,dataset_names,dataset_for_coverage,modif\n
+\n
+        if key == 'HP_max_epochs':\n
+            if args.epochs < value:\n
+                args.epochs = value\n
diff --git a/load_inputs/netmob_POIs_per_station.py b/load_inputs/netmob_POIs_per_station.py\n
index ff80799..c457a11 100644\n
--- a/load_inputs/netmob_POIs_per_station.py\n
+++ b/load_inputs/netmob_POIs_per_station.py\n
@@ -104 +103,0 @@ def load_data(dataset,ROOT,FOLDER_PATH,invalid_dates,intersect_coverage_period,a\n
-\n
@@ -111,0 +111,2 @@ def load_data_npy(id_station,ROOT,FOLDER_PATH,args):\n
+    data_app = extract_apps_tags_modes(data_app,metadata,args.NetMob_selected_apps,args.NetMob_selected_tags,args.NetMob_transfer_mode)\n
+    return(data_app)\n
@@ -113,3 +114,4 @@ def load_data_npy(id_station,ROOT,FOLDER_PATH,args):\n
-    pos_selected_apps = [k for k,app in enumerate(metadata['apps']) if app in args.NetMob_selected_apps]\n
-    pos_selected_modes = [k for k,mode in enumerate(metadata['transfer_modes']) if mode in args.NetMob_transfer_mode]\n
-    pos_selected_tags = [k for k,tag in enumerate(metadata['tags']) if tag in args.NetMob_selected_tags]\n
+def extract_apps_tags_modes(data_app,metadata,NetMob_selected_apps,NetMob_selected_tags,NetMob_transfer_mode):\n
+    pos_selected_apps = [k for k,app in enumerate(metadata['apps']) if app in NetMob_selected_apps]\n
+    pos_selected_modes = [k for k,mode in enumerate(metadata['transfer_modes']) if mode in NetMob_transfer_mode]\n
+    pos_selected_tags = [k for k,tag in enumerate(metadata['tags']) if tag in NetMob_selected_tags]\n
@@ -125,2 +127 @@ def load_data_npy(id_station,ROOT,FOLDER_PATH,args):\n
-\n
-    return(data_app)\n
+    return data_app\n"
{"commit":"4bcf026891bbd5d111bb1b332397037e86b60b6b","author":"Fr-ocasting","date":"2025-04-21T10:59:43+02:00","message":"correction threshold pour correlation adj_mx + abs(corr)"},"diff":"diff --git a/HP_tuning/hyperparameter_tuning_ray.py b/HP_tuning/hyperparameter_tuning_ray.py\n
index 1377354..5466b50 100644\n
--- a/HP_tuning/hyperparameter_tuning_ray.py\n
+++ b/HP_tuning/hyperparameter_tuning_ray.py\n
@@ -33 +33,4 @@ def HP_modification(config,args):\n
-            if key == 'scheduler':\n
+            if key == 'HP_max_epochs':\n
+                setattr(args, 'epochs',config['HP_max_epochs'])\n
+                \n
+            elif key == 'scheduler':\n
diff --git a/HP_tuning/ray_config.py b/HP_tuning/ray_config.py\n
index c32e0e9..b26abbf 100644\n
--- a/HP_tuning/ray_config.py\n
+++ b/HP_tuning/ray_config.py\n
@@ -70,2 +70,2 @@ def get_ray_config(args):\n
-    num_cpus = 36 if torch.cuda.is_available() else 6\n
-    max_concurrent_trials = 18 if torch.cuda.is_available() else 6\n
+    num_cpus = 24 if torch.cuda.is_available() else 6\n
+    max_concurrent_trials = 12 if torch.cuda.is_available() else 6\n
diff --git a/build_inputs/load_adj.py b/build_inputs/load_adj.py\n
index 530742c..0113b1e 100644\n
--- a/build_inputs/load_adj.py\n
+++ b/build_inputs/load_adj.py\n
@@ -15,2 +15,5 @@ def load_adj(dataset,folder = 'adj',adj_type = 'adj',threshold = None):\n
-        - 'corr' is based on Pearson Correlation Coefficient (PCC)\n
-        - 'dist' is based on gaussian kernel exp(-dist(u,v)^2 / sigma^2)\n
+    If i and j distant, then value -> 0, else, value -> 1. \n
+        - 'corr' is based on Pearson Correlation Coefficient (PCC). \n
+            Then, gso = abs(Correlation-Matrix([v1,...,vn]))\n
+        - 'dist' is based on gaussian kernel \n
+           Then, gso = exp(-dist(u,v)^2 / sigma^2)\n
@@ -20,2 +23,3 @@ def load_adj(dataset,folder = 'adj',adj_type = 'adj',threshold = None):\n
-        gso =  pd.DataFrame(dataset.train_input).corr()\n
-\n
+        gso =  abs(pd.DataFrame(dataset.train_input).corr())\n
+        assert threshold is not None, f\"You defined a distance-based (adj_type: {adj_type}) matrix but you did not define any threshold distance\"\n
+        gso[gso < threshold] = 0 \n
@@ -26,2 +30,2 @@ def load_adj(dataset,folder = 'adj',adj_type = 'adj',threshold = None):\n
-        if adj_type == 'dist':\n
-            assert threshold is not None, \"You defined a distance matrix but you did not define any threshold distance\"\n
+        if (adj_type == 'dist') :\n
+            assert threshold is not None, f\"You defined a distance-based (adj_type: {adj_type}) matrix but you did not define any threshold distance\"\n
diff --git a/dl_models/ASTGCN/load_config.py b/dl_models/ASTGCN/load_config.py\n
index 4de92ff..6ad8d5d 100644\n
--- a/dl_models/ASTGCN/load_config.py\n
+++ b/dl_models/ASTGCN/load_config.py\n
@@ -5 +5 @@ parser = argparse.ArgumentParser(description='ASTGCN')\n
-parser.add_argument('--nb_block', type=int, default=2, \n
+parser.add_argument('--nb_block', type=int, default=2, # 2 \n
@@ -7 +7 @@ parser.add_argument('--nb_block', type=int, default=2,\n
-parser.add_argument('--K', type=int, default=3, \n
+parser.add_argument('--K', type=int, default=3,   #3\n
@@ -9 +9 @@ parser.add_argument('--K', type=int, default=3,\n
-parser.add_argument('--nb_chev_filter', type=int, default=64, \n
+parser.add_argument('--nb_chev_filter', type=int, default=32, #64 \n
@@ -11 +11 @@ parser.add_argument('--nb_chev_filter', type=int, default=64,\n
-parser.add_argument('--nb_time_filter', type=int, default=64, \n
+parser.add_argument('--nb_time_filter', type=int, default=64, # 64\n
@@ -34,7 +33,0 @@ def transfer_from_orig_args(args_init,args):\n
-\n
-\n
-\n
-\n
-\n
-\n
-\n
diff --git a/dl_models/ASTGCN/search_space.py b/dl_models/ASTGCN/search_space.py\n
index 813ddeb..d36eb6d 100644\n
--- a/dl_models/ASTGCN/search_space.py\n
+++ b/dl_models/ASTGCN/search_space.py\n
@@ -4,5 +4,7 @@ from itertools import product\n
-config = {'nb_block' : tune.choice([1,2,3,4]),\n
-            'K' : tune.choice([1,2,3]),\n
-            'nb_chev_filter' : tune.choice([16,32,64,128,256]),\n
-            'nb_time_filter' : tune.choice([16,32,64,128,256]),\n
-            'threshold' : tune.choice([[0.1,0.3,0.7]])\n
+config = {#'nb_block' : tune.choice([1,2,3,4]),\n
+            #'K' : tune.choice([1,2,3]),\n
+            #'nb_chev_filter' : tune.choice([16,32,64,128,256]),\n
+            #'nb_time_filter' : tune.choice([16,32,64,128,256]),\n
+            #'threshold' : tune.choice([0.1,0.3,0.7]),\n
+            'threshold' : tune.choice([0,0.65,0.7,0.75,0.8]),\n
+            #'adj_type': tune.choice(['adj','dist','corr'])\n
diff --git a/examples/Total_evaluation_of_model.py b/examples/Total_evaluation_of_model.py\n
index beb0221..b5f0280 100644\n
--- a/examples/Total_evaluation_of_model.py\n
+++ b/examples/Total_evaluation_of_model.py\n
@@ -172 +172 @@ if __name__ == '__main__':\n
-                                            'HP_max_epochs':200,#100,\n
+                                            'HP_max_epochs':500,#100,\n
@@ -192,2 +192,2 @@ if __name__ == '__main__':\n
-        epochs_validation = 200#100\n
-        num_samples = 1000 # 500\n
+        epochs_validation = 500#100\n
+        num_samples = 200 # 500\n"
{"commit":"21ba9bba620e63450a44b47b6d89aba21f07e9c2","author":"Fr-ocasting","date":"2025-04-20T00:03:16+02:00","message":"lancement hp tuning ASTGCN2019"},"diff":"diff --git a/HP_tuning/ray_search_space.py b/HP_tuning/ray_search_space.py\n
index 2bf3ad4..83d86d7 100644\n
--- a/HP_tuning/ray_search_space.py\n
+++ b/HP_tuning/ray_search_space.py\n
@@ -14 +14 @@ def get_search_space_ray(args):\n
-    config = {\"lr\": tune.qloguniform(1e-5, 5e-3, 1e-5), # tune.qloguniform(5e-5, 5e-3, 5e-5)\n
+    config = {\"lr\": tune.qloguniform(1e-5, 1e-2, 1e-5), # tune.qloguniform(5e-5, 5e-3, 5e-5)\n
@@ -19 +19 @@ def get_search_space_ray(args):\n
-                                            \"torch_scheduler_milestone\": tune.randint(1, 30),\n
+                                            \"torch_scheduler_milestone\": tune.randint(1, 50),\n
diff --git a/dl_models/ASTGCN/load_config.py b/dl_models/ASTGCN/load_config.py\n
index b2588c9..4de92ff 100644\n
--- a/dl_models/ASTGCN/load_config.py\n
+++ b/dl_models/ASTGCN/load_config.py\n
@@ -13 +13 @@ parser.add_argument('--nb_time_filter', type=int, default=64,\n
-parser.add_argument('--adj_type', type=str, default='adj', choices=['adj', 'dist','corr'], \n
+parser.add_argument('--adj_type', type=str, default='corr', choices=['adj', 'dist','corr'], \n
diff --git a/dl_models/ASTGCN/search_space.py b/dl_models/ASTGCN/search_space.py\n
index 4ffa8af..813ddeb 100644\n
--- a/dl_models/ASTGCN/search_space.py\n
+++ b/dl_models/ASTGCN/search_space.py\n
@@ -4,2 +4,6 @@ from itertools import product\n
-config = {\n
-          }\n
\\ No newline at end of file\n
+config = {'nb_block' : tune.choice([1,2,3,4]),\n
+            'K' : tune.choice([1,2,3]),\n
+            'nb_chev_filter' : tune.choice([16,32,64,128,256]),\n
+            'nb_time_filter' : tune.choice([16,32,64,128,256]),\n
+            'threshold' : tune.choice([[0.1,0.3,0.7]])\n
+          }\n
diff --git a/examples/Total_evaluation_of_model.py b/examples/Total_evaluation_of_model.py\n
index c65159a..beb0221 100644\n
--- a/examples/Total_evaluation_of_model.py\n
+++ b/examples/Total_evaluation_of_model.py\n
@@ -162 +162 @@ if __name__ == '__main__':\n
-        model_name = 'STGCN' #'CNN'\n
+        model_name = 'ASTGCN' #'CNN' # 'STGCN'\n
@@ -172 +172 @@ if __name__ == '__main__':\n
-                                            'HP_max_epochs':500,#100,\n
+                                            'HP_max_epochs':200,#100,\n
@@ -192,2 +192,2 @@ if __name__ == '__main__':\n
-        epochs_validation = 500#100\n
-        num_samples = 400 # 500\n
+        epochs_validation = 200#100\n
+        num_samples = 1000 # 500\n"
{"commit":"7fdf224c9eec255bcc7c6208045a4deb943f468b","author":"Fr-ocasting","date":"2025-04-11T17:22:40+02:00","message":"commit"},"diff":"diff --git a/dl_models/ASTGCN/ASTGCN.py b/dl_models/ASTGCN/ASTGCN.py\n
index e6b5348..a4383d8 100644\n
--- a/dl_models/ASTGCN/ASTGCN.py\n
+++ b/dl_models/ASTGCN/ASTGCN.py\n
@@ -32,0 +33,10 @@ class Spatial_Attention_layer(nn.Module):\n
+        self.init_parameters()\n
+        \n
+    def init_parameters(self):\n
+        # Uniform distribution on 1D and Xavier uniform distribution on tensors >= 2D\n
+        nn.init.uniform_(self.W1)   # 1D \n
+        nn.init.xavier_uniform_(self.W2)\n
+        nn.init.uniform_(self.W3)\n
+        nn.init.xavier_uniform_(self.bs)\n
+        nn.init.xavier_uniform_(self.Vs)\n
+\n
@@ -39 +49,2 @@ class Spatial_Attention_layer(nn.Module):\n
-\n
+        # print('\\nStart Spatial Attention Layer: ')\n
+        # print('nan in x, W1, W2: ',torch.isnan(x).any().item(),torch.isnan(self.W1).any().item(),torch.isnan(self.W2).any().item())\n
@@ -48,0 +60,5 @@ class Spatial_Attention_layer(nn.Module):\n
+        # print('nan in lhs: ',torch.isnan(lhs).any().item())\n
+        # print('nan in rhs: ',torch.isnan(rhs).any().item())\n
+        # print('nan in product: ',torch.isnan(product).any().item())\n
+        # print('nan in S: ',torch.isnan(S).any().item())\n
+        # print('nan in S_normalized: ',torch.isnan(S_normalized).any().item())\n
@@ -71,0 +88,7 @@ class cheb_conv_withSAt(nn.Module):\n
+        self.init_parameters()\n
+\n
+    def init_parameters(self):\n
+        # Uniform distribution on 1D and Xavier uniform distribution on tensors >= 2D\n
+        for theta in self.Theta:\n
+            nn.init.xavier_uniform_(theta)\n
+\n
@@ -113,0 +137,9 @@ class Temporal_Attention_layer(nn.Module):\n
+        self.init_parameters()\n
+        \n
+    def init_parameters(self):\n
+        # Uniform distribution on 1D and Xavier uniform distribution on tensors >= 2D\n
+        nn.init.uniform_(self.U1)   # 1D \n
+        nn.init.xavier_uniform_(self.U2)\n
+        nn.init.uniform_(self.U3)\n
+        nn.init.xavier_uniform_(self.be)\n
+        nn.init.xavier_uniform_(self.Ve)\n
@@ -121,0 +154,2 @@ class Temporal_Attention_layer(nn.Module):\n
+        #print('\\nStart Temporal Attention Layer: ')\n
+        #print('nan in x, U1, U2: ',torch.isnan(x).any().item(),torch.isnan(self.U1).any().item(),torch.isnan(self.U2).any().item())\n
@@ -134,0 +169,6 @@ class Temporal_Attention_layer(nn.Module):\n
+        #print('\\nnan in lhs: ',torch.isnan(lhs).any().item())\n
+        #print('nan in rhs: ',torch.isnan(rhs).any().item())\n
+        #print('nan in product: ',torch.isnan(product).any().item())\n
+        #print('nan in E: ',torch.isnan(E).any().item())\n
+        #print('nan in E_normalized: ',torch.isnan(E_normalized).any().item())\n
+\n
@@ -155,0 +196,6 @@ class cheb_conv(nn.Module):\n
+        self.init_parameters()\n
+\n
+    def init_parameters(self):\n
+        # Uniform distribution on 1D and Xavier uniform distribution on tensors >= 2D\n
+        for theta in self.Theta:\n
+            nn.init.xavier_uniform_(theta)\n
@@ -204,0 +251,2 @@ class ASTGCN_block(nn.Module):\n
+        #print('\\nx start block_i:')\n
+        #print('nan in x: ',torch.isnan(x).any().item())\n
@@ -208,0 +257 @@ class ASTGCN_block(nn.Module):\n
+        #print('nan in x after TAt: ',torch.isnan(temporal_At).any().item())\n
@@ -211 +260 @@ class ASTGCN_block(nn.Module):\n
-\n
+        #print('nan in x after matmul reshape: ',torch.isnan(x_TAt).any().item())\n
@@ -213,0 +263 @@ class ASTGCN_block(nn.Module):\n
+        #print('nan in x after SAt : ',torch.isnan(spatial_At).any().item())\n
@@ -216,0 +267 @@ class ASTGCN_block(nn.Module):\n
+        #print('nan in x after cheb_conv_SAt : ',torch.isnan(spatial_gcn).any().item())\n
@@ -220,0 +272 @@ class ASTGCN_block(nn.Module):\n
+        #print('nan in x after time_conv : ',torch.isnan(time_conv_output).any().item())\n
@@ -223,0 +276 @@ class ASTGCN_block(nn.Module):\n
+        #print('nan in x after residual_conv : ',torch.isnan(x_residual).any().item())\n
@@ -225,0 +279 @@ class ASTGCN_block(nn.Module):\n
+        #print('nan in x after ln(F(ReLU)) : ',torch.isnan(x_residual).any().item())\n
@@ -227,0 +282,2 @@ class ASTGCN_block(nn.Module):\n
+\n
+\n
@@ -261,0 +318,2 @@ class ASTGCN(nn.Module):\n
+        #print('\\nx entry: ')\n
+        #print('nan in x: ',torch.isnan(x).any())\n
@@ -264,0 +323,2 @@ class ASTGCN(nn.Module):\n
+        #print('\\nx after blocks: ')\n
+        #print('nan in x: ',torch.isnan(x).any())\n
@@ -266,0 +327,2 @@ class ASTGCN(nn.Module):\n
+        #print('\\nx after final_conv: ')\n
+        #print('nan in x: ',torch.isnan(x).any())\n
diff --git a/trainer.py b/trainer.py\n
index f354e1b..7004f72 100644\n
--- a/trainer.py\n
+++ b/trainer.py\n
@@ -353,0 +354,2 @@ class Trainer(object):\n
+            #print('\\npred: ', pred.size(), 'y_b: ', y_b.size())\n
+            #print('loss: ',loss)\n"
{"commit":"88a8767e6e36dfd9561c6f7f8750a6a528ee0a7f","author":"Fr-ocasting","date":"2025-04-10T18:30:43+02:00","message":"commit"},"diff":"diff --git a/dl_models/ASTGCN/ASTGCN.py b/dl_models/ASTGCN/ASTGCN.py\n
index e688a1a..e6b5348 100644\n
--- a/dl_models/ASTGCN/ASTGCN.py\n
+++ b/dl_models/ASTGCN/ASTGCN.py\n
@@ -5 +4,0 @@ Copy from https://github.com/guoshnBJTU/ASTGCN-2019-pytorch/blob/master/train_AS\n
-from torch import nn\n
@@ -7 +6,2 @@ import torch\n
-\n
+import torch.nn as nn\n
+import torch.nn.functional as F\n
@@ -18,5 +18 @@ if parent_dir not in sys.path:\n
-from dl_models.TimeEmbedding.time_embedding import TimeEmbedding\n
-import torch\n
-import torch.nn as nn\n
-import torch.nn.functional as F\n
-from lib.utils import scaled_Laplacian, cheb_polynomial\n
+from dl_models.ASTGCN.lib.utils import scaled_Laplacian, cheb_polynomial\n
@@ -235 +231 @@ class ASTGCN_block(nn.Module):\n
-class ASTGCN_submodule(nn.Module):\n
+class ASTGCN(nn.Module):\n
@@ -249 +245 @@ class ASTGCN_submodule(nn.Module):\n
-        super(ASTGCN_submodule, self).__init__()\n
+        super(ASTGCN, self).__init__()\n
@@ -261 +257 @@ class ASTGCN_submodule(nn.Module):\n
-    def forward(self, x):\n
+    def forward(self, x,extracted_feature=None,time_elt=None):\n
@@ -263 +259 @@ class ASTGCN_submodule(nn.Module):\n
-        :param x: (B, N_nodes, F_in, T_in)\n
+        :param x: (B, F_in, N_nodes, T_in)  but PERMUTE to be (B, N_nodes, F_in, T_in)\n
@@ -265,0 +262 @@ class ASTGCN_submodule(nn.Module):\n
+        x = x.permute(0,2,1,3)  # (B, F_in, N_nodes, T_in) -> (B, N_nodes, F_in, T_in) \n
@@ -271 +267,0 @@ class ASTGCN_submodule(nn.Module):\n
-\n
@@ -292 +288 @@ def make_model(DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter,\n
-    model = ASTGCN_submodule(DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter, time_strides, cheb_polynomials, num_for_predict, len_input, num_of_vertices)\n
+    model = ASTGCN(DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter, time_strides, cheb_polynomials, num_for_predict, len_input, num_of_vertices)\n
diff --git a/dl_models/ASTGCN/lib/utils.py b/dl_models/ASTGCN/lib/utils.py\n
index 3975dda..6261cb4 100644\n
--- a/dl_models/ASTGCN/lib/utils.py\n
+++ b/dl_models/ASTGCN/lib/utils.py\n
@@ -8 +8 @@ from scipy.sparse.linalg import eigs\n
-\n
+import pandas as pd \n
@@ -105,0 +106,3 @@ def scaled_Laplacian(W):\n
+    if type(L) == pd.DataFrame:\n
+        L = L.values.astype(float)\n
+\n
diff --git a/dl_models/ASTGCN/load_config.py b/dl_models/ASTGCN/load_config.py\n
index 34acff5..b2588c9 100644\n
--- a/dl_models/ASTGCN/load_config.py\n
+++ b/dl_models/ASTGCN/load_config.py\n
@@ -5,2 +5,15 @@ parser = argparse.ArgumentParser(description='ASTGCN')\n
-parser.add_argument('--c_in', type=int, default=1, \n
-                    help='Channel dimension of the input. Usually = 1. As we consider trafic forecasting, we could set c_in = 2 if two time series are considered : Speed and Flow')\n
+parser.add_argument('--nb_block', type=int, default=2, \n
+                    help='Number of blocks in the model')\n
+parser.add_argument('--K', type=int, default=3, \n
+                    help='Order of the Chebyshev polynomial')\n
+parser.add_argument('--nb_chev_filter', type=int, default=64, \n
+                    help='Number of Chebyshev filters')\n
+parser.add_argument('--nb_time_filter', type=int, default=64, \n
+                    help='Number of time filters')\n
+parser.add_argument('--adj_type', type=str, default='adj', choices=['adj', 'dist','corr'], \n
+                    help='type of adjacency matrix')\n
+parser.add_argument('--threshold', type=float, default=0.3, \n
+                    help='threshold to build sparse weighted adjacency matrix. Replace each value below threshold with 0. Is not used if adj_type == \"adj\"')\n
+parser.add_argument('--time_strides', type=int, default=1, \n
+                    help='stride of the conv2D on the temporal dim')\n
+\n
@@ -8,0 +22,19 @@ args = parser.parse_args(args=[])\n
+''' Config that comes from the original 'args_init' : '''\n
+def transfer_from_orig_args(args_init,args):\n
+    args.num_of_vertices = args_init.n_vertex\n
+    args.num_for_predict = args_init.step_ahead\n
+    args.len_input = args_init.L\n
+    args.in_channels = args_init.C\n
+    args.DEVICE = args_init.device\n
+    return args\n
+\n
+\n
+\n
+\n
+\n
+\n
+\n
+\n
+\n
+\n
+\n
@@ -12 +44 @@ parser_HP.add_argument('--batch_size', type=int, default=32, help=\"Batch size\")\n
-parser_HP.add_argument('--lr', type=float, default=5e-3, help=\"Lr\")\n
+parser_HP.add_argument('--lr', type=float, default=1e-3, help=\"Lr\")\n
@@ -16,0 +49,27 @@ args_HP = parser_HP.parse_args(args=[])\n
+\n
+# Init config to remove in this framework: \n
+if False:\n
+    parser.add_argument('--adj_filename', type=str, default='./data/PEMS04/distance.csv', \n
+                        help='Path to the adjacency matrix file')\n
+    parser.add_argument('--graph_signal_matrix_filename', type=str, default='./data/PEMS04/PEMS04.npz', \n
+                        help='Path to the graph signal matrix file')\n
+    parser.add_argument('--ctx', type=int, default=0, \n
+                    help='Context or device ID (e.g., GPU ID)')\n
+    parser.add_argument('--start_epoch', type=int, default=0, \n
+                    help='Starting epoch for training')\n
+    parser.add_argument('--metric_method', type=str, default='unmask', \n
+                        help='Metric method for evaluation')\n
+    parser.add_argument('--missing_value', type=float, default=0.0, \n
+                        help='Value to use for missing data')\n
+    parser.add_argument('--model_name', type=str, default='astgcn_r', \n
+                        help='Name of the model')\n
+    parser.add_argument('--points_per_hour', type=int, default=12, \n
+                        help='Number of points per hour in the dataset')\n
+    parser.add_argument('--num_of_weeks', type=int, default=0, \n
+                        help='Number of weeks of data to use')\n
+    parser.add_argument('--num_of_days', type=int, default=0, \n
+                        help='Number of days of data to use')\n
+    parser.add_argument('--num_of_hours', type=int, default=1, \n
+                        help='Number of hours of data to use')\n
+\n
+\n
@@ -19 +77,0 @@ if False:\n
-\n
diff --git a/dl_models/full_model.py b/dl_models/full_model.py\n
index f543cd5..f02ac1a 100644\n
--- a/dl_models/full_model.py\n
+++ b/dl_models/full_model.py\n
@@ -23,0 +24,2 @@ from dl_models.TFT.TFT import TFT\n
+from dl_models.ASTGCN.ASTGCN import ASTGCN\n
+from dl_models.ASTGCN.lib.utils import cheb_polynomial,scaled_Laplacian\n
@@ -399 +401,10 @@ def load_model(dataset, args):\n
-\n
+    if args.model_name == 'ASTGCN':\n
+        from dl_models.ASTGCN.load_config import args as ASTGCN_args\n
+        from dl_models.ASTGCN.load_config import transfer_from_orig_args \n
+        ASTGCN_args = transfer_from_orig_args(args,ASTGCN_args)\n
+        filtered_args = {k: v for k, v in vars(ASTGCN_args).items() if k in inspect.signature(ASTGCN.__init__).parameters.keys()}\n
+        adj_mx,_ = load_adj(dataset,adj_type = args.adj_type, threshold= args.threshold)\n
+        L_tilde = scaled_Laplacian(adj_mx)\n
+        cheb_polynomials = [torch.from_numpy(i).type(torch.FloatTensor).to(args.device) for i in cheb_polynomial(L_tilde, ASTGCN_args.K)]\n
+        #model = ASTGCN(DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter, time_strides, cheb_polynomials, num_for_predict, len_input, num_of_vertices)\n
+        model = ASTGCN(**filtered_args,cheb_polynomials=cheb_polynomials).to(args.device)\n"
{"commit":"a56f8f7ed14316e8ed29090fedfc59ad72d2196e","author":"Fr-ocasting","date":"2025-04-10T11:17:13+02:00","message":"commit"},"diff":"diff --git a/dl_models/ASTGCN/ASTGCN.py b/dl_models/ASTGCN/ASTGCN.py\n
new file mode 100644\n
index 0000000..e688a1a\n
--- /dev/null\n
+++ b/dl_models/ASTGCN/ASTGCN.py\n
@@ -0,0 +1,300 @@\n
+\"\"\"\n
+Copy from https://github.com/guoshnBJTU/ASTGCN-2019-pytorch/blob/master/train_ASTGCN_r.py\n
+\"\"\"\n
+\n
+from torch import nn\n
+import torch\n
+\n
+# Relative path:\n
+import sys \n
+import os \n
+current_file_path = os.path.abspath(os.path.dirname(__file__))\n
+parent_dir = os.path.abspath(os.path.join(current_file_path,'..'))\n
+if parent_dir not in sys.path:\n
+    sys.path.insert(0,parent_dir)\n
+# ...\n
+\n
+# Personnal import:\n
+from dl_models.TimeEmbedding.time_embedding import TimeEmbedding\n
+import torch\n
+import torch.nn as nn\n
+import torch.nn.functional as F\n
+from lib.utils import scaled_Laplacian, cheb_polynomial\n
+\n
+\n
+class Spatial_Attention_layer(nn.Module):\n
+    '''\n
+    compute spatial attention scores\n
+    '''\n
+    def __init__(self, DEVICE, in_channels, num_of_vertices, num_of_timesteps):\n
+        super(Spatial_Attention_layer, self).__init__()\n
+        self.W1 = nn.Parameter(torch.FloatTensor(num_of_timesteps).to(DEVICE))\n
+        self.W2 = nn.Parameter(torch.FloatTensor(in_channels, num_of_timesteps).to(DEVICE))\n
+        self.W3 = nn.Parameter(torch.FloatTensor(in_channels).to(DEVICE))\n
+        self.bs = nn.Parameter(torch.FloatTensor(1, num_of_vertices, num_of_vertices).to(DEVICE))\n
+        self.Vs = nn.Parameter(torch.FloatTensor(num_of_vertices, num_of_vertices).to(DEVICE))\n
+\n
+\n
+    def forward(self, x):\n
+        '''\n
+        :param x: (batch_size, N, F_in, T)\n
+        :return: (B,N,N)\n
+        '''\n
+\n
+        lhs = torch.matmul(torch.matmul(x, self.W1), self.W2)  # (b,N,F,T)(T)->(b,N,F)(F,T)->(b,N,T)\n
+\n
+        rhs = torch.matmul(self.W3, x).transpose(-1, -2)  # (F)(b,N,F,T)->(b,N,T)->(b,T,N)\n
+\n
+        product = torch.matmul(lhs, rhs)  # (b,N,T)(b,T,N) -> (B, N, N)\n
+\n
+        S = torch.matmul(self.Vs, torch.sigmoid(product + self.bs))  # (N,N)(B, N, N)->(B,N,N)\n
+\n
+        S_normalized = F.softmax(S, dim=1)\n
+\n
+        return S_normalized\n
+\n
+\n
+class cheb_conv_withSAt(nn.Module):\n
+    '''\n
+    K-order chebyshev graph convolution\n
+    '''\n
+\n
+    def __init__(self, K, cheb_polynomials, in_channels, out_channels):\n
+        '''\n
+        :param K: int\n
+        :param in_channles: int, num of channels in the input sequence\n
+        :param out_channels: int, num of channels in the output sequence\n
+        '''\n
+        super(cheb_conv_withSAt, self).__init__()\n
+        self.K = K\n
+        self.cheb_polynomials = cheb_polynomials\n
+        self.in_channels = in_channels\n
+        self.out_channels = out_channels\n
+        self.DEVICE = cheb_polynomials[0].device\n
+        self.Theta = nn.ParameterList([nn.Parameter(torch.FloatTensor(in_channels, out_channels).to(self.DEVICE)) for _ in range(K)])\n
+\n
+    def forward(self, x, spatial_attention):\n
+        '''\n
+        Chebyshev graph convolution operation\n
+        :param x: (batch_size, N, F_in, T)\n
+        :return: (batch_size, N, F_out, T)\n
+        '''\n
+\n
+        batch_size, num_of_vertices, in_channels, num_of_timesteps = x.shape\n
+\n
+        outputs = []\n
+\n
+        for time_step in range(num_of_timesteps):\n
+\n
+            graph_signal = x[:, :, :, time_step]  # (b, N, F_in)\n
+\n
+            output = torch.zeros(batch_size, num_of_vertices, self.out_channels).to(self.DEVICE)  # (b, N, F_out)\n
+\n
+            for k in range(self.K):\n
+\n
+                T_k = self.cheb_polynomials[k]  # (N,N)\n
+\n
+                T_k_with_at = T_k.mul(spatial_attention)   # (N,N)*(N,N) = (N,N) 多行和为1, 按着列进行归一化\n
+\n
+                theta_k = self.Theta[k]  # (in_channel, out_channel)\n
+\n
+                rhs = T_k_with_at.permute(0, 2, 1).matmul(graph_signal)  # (N, N)(b, N, F_in) = (b, N, F_in) 因为是左乘，所以多行和为1变为多列和为1，即一行之和为1，进行左乘\n
+\n
+                output = output + rhs.matmul(theta_k)  # (b, N, F_in)(F_in, F_out) = (b, N, F_out)\n
+\n
+            outputs.append(output.unsqueeze(-1))  # (b, N, F_out, 1)\n
+\n
+        return F.relu(torch.cat(outputs, dim=-1))  # (b, N, F_out, T)\n
+\n
+\n
+class Temporal_Attention_layer(nn.Module):\n
+    def __init__(self, DEVICE, in_channels, num_of_vertices, num_of_timesteps):\n
+        super(Temporal_Attention_layer, self).__init__()\n
+        self.U1 = nn.Parameter(torch.FloatTensor(num_of_vertices).to(DEVICE))\n
+        self.U2 = nn.Parameter(torch.FloatTensor(in_channels, num_of_vertices).to(DEVICE))\n
+        self.U3 = nn.Parameter(torch.FloatTensor(in_channels).to(DEVICE))\n
+        self.be = nn.Parameter(torch.FloatTensor(1, num_of_timesteps, num_of_timesteps).to(DEVICE))\n
+        self.Ve = nn.Parameter(torch.FloatTensor(num_of_timesteps, num_of_timesteps).to(DEVICE))\n
+\n
+    def forward(self, x):\n
+        '''\n
+        :param x: (batch_size, N, F_in, T)\n
+        :return: (B, T, T)\n
+        '''\n
+        _, num_of_vertices, num_of_features, num_of_timesteps = x.shape\n
+\n
+        lhs = torch.matmul(torch.matmul(x.permute(0, 3, 2, 1), self.U1), self.U2)\n
+        # x:(B, N, F_in, T) -> (B, T, F_in, N)\n
+        # (B, T, F_in, N)(N) -> (B,T,F_in)\n
+        # (B,T,F_in)(F_in,N)->(B,T,N)\n
+\n
+        rhs = torch.matmul(self.U3, x)  # (F)(B,N,F,T)->(B, N, T)\n
+\n
+        product = torch.matmul(lhs, rhs)  # (B,T,N)(B,N,T)->(B,T,T)\n
+\n
+        E = torch.matmul(self.Ve, torch.sigmoid(product + self.be))  # (B, T, T)\n
+\n
+        E_normalized = F.softmax(E, dim=1)\n
+\n
+        return E_normalized\n
+\n
+\n
+class cheb_conv(nn.Module):\n
+    '''\n
+    K-order chebyshev graph convolution\n
+    '''\n
+\n
+    def __init__(self, K, cheb_polynomials, in_channels, out_channels):\n
+        '''\n
+        :param K: int\n
+        :param in_channles: int, num of channels in the input sequence\n
+        :param out_channels: int, num of channels in the output sequence\n
+        '''\n
+        super(cheb_conv, self).__init__()\n
+        self.K = K\n
+        self.cheb_polynomials = cheb_polynomials\n
+        self.in_channels = in_channels\n
+        self.out_channels = out_channels\n
+        self.DEVICE = cheb_polynomials[0].device\n
+        self.Theta = nn.ParameterList([nn.Parameter(torch.FloatTensor(in_channels, out_channels).to(self.DEVICE)) for _ in range(K)])\n
+\n
+    def forward(self, x):\n
+        '''\n
+        Chebyshev graph convolution operation\n
+        :param x: (batch_size, N, F_in, T)\n
+        :return: (batch_size, N, F_out, T)\n
+        '''\n
+\n
+        batch_size, num_of_vertices, in_channels, num_of_timesteps = x.shape\n
+\n
+        outputs = []\n
+\n
+        for time_step in range(num_of_timesteps):\n
+\n
+            graph_signal = x[:, :, :, time_step]  # (b, N, F_in)\n
+\n
+            output = torch.zeros(batch_size, num_of_vertices, self.out_channels).to(self.DEVICE)  # (b, N, F_out)\n
+\n
+            for k in range(self.K):\n
+\n
+                T_k = self.cheb_polynomials[k]  # (N,N)\n
+\n
+                theta_k = self.Theta[k]  # (in_channel, out_channel)\n
+\n
+                rhs = graph_signal.permute(0, 2, 1).matmul(T_k).permute(0, 2, 1)\n
+\n
+                output = output + rhs.matmul(theta_k)\n
+\n
+            outputs.append(output.unsqueeze(-1))\n
+\n
+        return F.relu(torch.cat(outputs, dim=-1))\n
+\n
+\n
+class ASTGCN_block(nn.Module):\n
+\n
+    def __init__(self, DEVICE, in_channels, K, nb_chev_filter, nb_time_filter, time_strides, cheb_polynomials, num_of_vertices, num_of_timesteps):\n
+        super(ASTGCN_block, self).__init__()\n
+        self.TAt = Temporal_Attention_layer(DEVICE, in_channels, num_of_vertices, num_of_timesteps)\n
+        self.SAt = Spatial_Attention_layer(DEVICE, in_channels, num_of_vertices, num_of_timesteps)\n
+        self.cheb_conv_SAt = cheb_conv_withSAt(K, cheb_polynomials, in_channels, nb_chev_filter)\n
+        self.time_conv = nn.Conv2d(nb_chev_filter, nb_time_filter, kernel_size=(1, 3), stride=(1, time_strides), padding=(0, 1))\n
+        self.residual_conv = nn.Conv2d(in_channels, nb_time_filter, kernel_size=(1, 1), stride=(1, time_strides))\n
+        self.ln = nn.LayerNorm(nb_time_filter)  #需要将channel放到最后一个维度上\n
+\n
+    def forward(self, x):\n
+        '''\n
+        :param x: (batch_size, N, F_in, T)\n
+        :return: (batch_size, N, nb_time_filter, T)\n
+        '''\n
+        batch_size, num_of_vertices, num_of_features, num_of_timesteps = x.shape\n
+\n
+        # TAt\n
+        temporal_At = self.TAt(x)  # (b, T, T)\n
+\n
+        x_TAt = torch.matmul(x.reshape(batch_size, -1, num_of_timesteps), temporal_At).reshape(batch_size, num_of_vertices, num_of_features, num_of_timesteps)\n
+\n
+        # SAt\n
+        spatial_At = self.SAt(x_TAt)\n
+\n
+        # cheb gcn\n
+        spatial_gcn = self.cheb_conv_SAt(x, spatial_At)  # (b,N,F,T)\n
+        # spatial_gcn = self.cheb_conv(x)\n
+\n
+        # convolution along the time axis\n
+        time_conv_output = self.time_conv(spatial_gcn.permute(0, 2, 1, 3))  # (b,N,F,T)->(b,F,N,T) 用(1,3)的卷积核去做->(b,F,N,T)\n
+\n
+        # residual shortcut\n
+        x_residual = self.residual_conv(x.permute(0, 2, 1, 3))  # (b,N,F,T)->(b,F,N,T) 用(1,1)的卷积核去做->(b,F,N,T)\n
+\n
+        x_residual = self.ln(F.relu(x_residual + time_conv_output).permute(0, 3, 2, 1)).permute(0, 2, 3, 1)\n
+        # (b,F,N,T)->(b,T,N,F) -ln-> (b,T,N,F)->(b,N,F,T)\n
+\n
+        return x_residual\n
+\n
+\n
+class ASTGCN_submodule(nn.Module):\n
+\n
+    def __init__(self, DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter, time_strides, cheb_polynomials, num_for_predict, len_input, num_of_vertices):\n
+        '''\n
+        :param nb_block:\n
+        :param in_channels:\n
+        :param K:\n
+        :param nb_chev_filter:\n
+        :param nb_time_filter:\n
+        :param time_strides:\n
+        :param cheb_polynomials:\n
+        :param nb_predict_step:\n
+        '''\n
+\n
+        super(ASTGCN_submodule, self).__init__()\n
+\n
+        self.BlockList = nn.ModuleList([ASTGCN_block(DEVICE, in_channels, K, nb_chev_filter, nb_time_filter, time_strides, cheb_polynomials, num_of_vertices, len_input)])\n
+\n
+        self.BlockList.extend([ASTGCN_block(DEVICE, nb_time_filter, K, nb_chev_filter, nb_time_filter, 1, cheb_polynomials, num_of_vertices, len_input//time_strides) for _ in range(nb_block-1)])\n
+\n
+        self.final_conv = nn.Conv2d(int(len_input/time_strides), num_for_predict, kernel_size=(1, nb_time_filter))\n
+\n
+        self.DEVICE = DEVICE\n
+\n
+        self.to(DEVICE)\n
+\n
+    def forward(self, x):\n
+        '''\n
+        :param x: (B, N_nodes, F_in, T_in)\n
+        :return: (B, N_nodes, T_out)\n
+        '''\n
+        for block in self.BlockList:\n
+            x = block(x)\n
+\n
+        output = self.final_conv(x.permute(0, 3, 1, 2))[:, :, :, -1].permute(0, 2, 1)\n
+        # (b,N,F,T)->(b,T,N,F)-conv<1,F>->(b,c_out*T,N,1)->(b,c_out*T,N)->(b,N,T)\n
+\n
+        return output\n
+\n
+\n
+def make_model(DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter, time_strides, adj_mx, num_for_predict, len_input, num_of_vertices):\n
+    '''\n
+\n
+    :param DEVICE:\n
+    :param nb_block:\n
+    :param in_channels:\n
+    :param K:\n
+    :param nb_chev_filter:\n
+    :param nb_time_filter:\n
+    :param time_strides:\n
+    :param cheb_polynomials:\n
+    :param nb_predict_step:\n
+    :param len_input\n
+    :return:\n
+    '''\n
+    L_tilde = scaled_Laplacian(adj_mx)\n
+    cheb_polynomials = [torch.from_numpy(i).type(torch.FloatTensor).to(DEVICE) for i in cheb_polynomial(L_tilde, K)]\n
+    model = ASTGCN_submodule(DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter, time_strides, cheb_polynomials, num_for_predict, len_input, num_of_vertices)\n
+\n
+    for p in model.parameters():\n
+        if p.dim() > 1:\n
+            nn.init.xavier_uniform_(p)\n
+        else:\n
+            nn.init.uniform_(p)\n
+\n
+    return model\n
\\ No newline at end of file\n
diff --git a/dl_models/ASTGCN/lib/utils.py b/dl_models/ASTGCN/lib/utils.py\n
new file mode 100644\n
index 0000000..3975dda\n
--- /dev/null\n
+++ b/dl_models/ASTGCN/lib/utils.py\n
@@ -0,0 +1,252 @@\n
+import os\n
+import numpy as np\n
+import torch\n
+import torch.utils.data\n
+from sklearn.metrics import mean_absolute_error\n
+from sklearn.metrics import mean_squared_error\n
+from scipy.sparse.linalg import eigs\n
+\n
+\n
+def re_normalization(x, mean, std):\n
+    x = x * std + mean\n
+    return x\n
+\n
+\n
+def max_min_normalization(x, _max, _min):\n
+    x = 1. * (x - _min)/(_max - _min)\n
+    x = x * 2. - 1.\n
+    return x\n
+\n
+\n
+def re_max_min_normalization(x, _max, _min):\n
+    x = (x + 1.) / 2.\n
+    x = 1. * x * (_max - _min) + _min\n
+    return x\n
+\n
+\n
+def get_adjacency_matrix(distance_df_filename, num_of_vertices, id_filename=None):\n
+    '''\n
+    Parameters\n
+    ----------\n
+    distance_df_filename: str, path of the csv file contains edges information\n
+\n
+    num_of_vertices: int, the number of vertices\n
+\n
+    Returns\n
+    ----------\n
+    A: np.ndarray, adjacency matrix\n
+\n
+    '''\n
+    if 'npy' in distance_df_filename:\n
+\n
+        adj_mx = np.load(distance_df_filename)\n
+\n
+        return adj_mx, None\n
+\n
+    else:\n
+\n
+        import csv\n
+\n
+        A = np.zeros((int(num_of_vertices), int(num_of_vertices)),\n
+                     dtype=np.float32)\n
+\n
+        distaneA = np.zeros((int(num_of_vertices), int(num_of_vertices)),\n
+                            dtype=np.float32)\n
+\n
+        if id_filename:\n
+\n
+            with open(id_filename, 'r') as f:\n
+                id_dict = {int(i): idx for idx, i in enumerate(f.read().strip().split('\\n'))}  # 把节点id（idx）映射成从0开始的索引\n
+\n
+            with open(distance_df_filename, 'r') as f:\n
+                f.readline()\n
+                reader = csv.reader(f)\n
+                for row in reader:\n
+                    if len(row) != 3:\n
+                        continue\n
+                    i, j, distance = int(row[0]), int(row[1]), float(row[2])\n
+                    A[id_dict[i], id_dict[j]] = 1\n
+                    distaneA[id_dict[i], id_dict[j]] = distance\n
+            return A, distaneA\n
+\n
+        else:\n
+\n
+            with open(distance_df_filename, 'r') as f:\n
+                f.readline()\n
+                reader = csv.reader(f)\n
+                for row in reader:\n
+                    if len(row) != 3:\n
+                        continue\n
+                    i, j, distance = int(row[0]), int(row[1]), float(row[2])\n
+                    A[i, j] = 1\n
+                    distaneA[i, j] = distance\n
+            return A, distaneA\n
+\n
+\n
+def scaled_Laplacian(W):\n
+    '''\n
+    compute \\tilde{L}\n
+\n
+    Parameters\n
+    ----------\n
+    W: np.ndarray, shape is (N, N), N is the num of vertices\n
+\n
+    Returns\n
+    ----------\n
+    scaled_Laplacian: np.ndarray, shape (N, N)\n
+\n
+    '''\n
+\n
+    assert W.shape[0] == W.shape[1]\n
+\n
+    D = np.diag(np.sum(W, axis=1))\n
+\n
+    L = D - W\n
+\n
+    lambda_max = eigs(L, k=1, which='LR')[0].real\n
+\n
+    return (2 * L) / lambda_max - np.identity(W.shape[0])\n
+\n
+\n
+def cheb_polynomial(L_tilde, K):\n
+    '''\n
+    compute a list of chebyshev polynomials from T_0 to T_{K-1}\n
+\n
+    Parameters\n
+    ----------\n
+    L_tilde: scaled Laplacian, np.ndarray, shape (N, N)\n
+\n
+    K: the maximum order of chebyshev polynomials\n
+\n
+    Returns\n
+    ----------\n
+    cheb_polynomials: list(np.ndarray), length: K, from T_0 to T_{K-1}\n
+\n
+    '''\n
+\n
+    N = L_tilde.shape[0]\n
+\n
+    cheb_polynomials = [np.identity(N), L_tilde.copy()]\n
+\n
+    for i in range(2, K):\n
+        cheb_polynomials.append(2 * L_tilde * cheb_polynomials[i - 1] - cheb_polynomials[i - 2])\n
+\n
+    return cheb_polynomials\n
+\n
+\n
+def load_graphdata_channel1(graph_signal_matrix_filename, num_of_hours, num_of_days, num_of_weeks, DEVICE, batch_size, shuffle=True):\n
+    '''\n
+    这个是为PEMS的数据准备的函数\n
+    将x,y都处理成归一化到[-1,1]之前的数据;\n
+    每个样本同时包含所有监测点的数据，所以本函数构造的数据输入时空序列预测模型；\n
+    该函数会把hour, day, week的时间串起来；\n
+    注： 从文件读入的数据，x是最大最小归一化的，但是y是真实值\n
+    这个函数转为mstgcn，astgcn设计，返回的数据x都是通过减均值除方差进行归一化的，y都是真实值\n
+    :param graph_signal_matrix_filename: str\n
+    :param num_of_hours: int\n
+    :param num_of_days: int\n
+    :param num_of_weeks: int\n
+    :param DEVICE:\n
+    :param batch_size: int\n
+    :return:\n
+    three DataLoaders, each dataloader contains:\n
+    test_x_tensor: (B, N_nodes, in_feature, T_input)\n
+    test_decoder_input_tensor: (B, N_nodes, T_output)\n
+    test_target_tensor: (B, N_nodes, T_output)\n
+\n
+    '''\n
+\n
+    file = os.path.basename(graph_signal_matrix_filename).split('.')[0]\n
+\n
+    dirpath = os.path.dirname(graph_signal_matrix_filename)\n
+\n
+    filename = os.path.join(dirpath,\n
+                            file + '_r' + str(num_of_hours) + '_d' + str(num_of_days) + '_w' + str(num_of_weeks)) +'_astcgn'\n
+\n
+    print('load file:', filename)\n
+\n
+    file_data = np.load(filename + '.npz')\n
+    train_x = file_data['train_x']  # (10181, 307, 3, 12)\n
+    train_x = train_x[:, :, 0:1, :]\n
+    train_target = file_data['train_target']  # (10181, 307, 12)\n
+\n
+    val_x = file_data['val_x']\n
+    val_x = val_x[:, :, 0:1, :]\n
+    val_target = file_data['val_target']\n
+\n
+    test_x = file_data['test_x']\n
+    test_x = test_x[:, :, 0:1, :]\n
+    test_target = file_data['test_target']\n
+\n
+    mean = file_data['mean'][:, :, 0:1, :]  # (1, 1, 3, 1)\n
+    std = file_data['std'][:, :, 0:1, :]  # (1, 1, 3, 1)\n
+\n
+    # ------- train_loader -------\n
+    train_x_tensor = torch.from_numpy(train_x).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)\n
+    train_target_tensor = torch.from_numpy(train_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\n
+\n
+    train_dataset = torch.utils.data.TensorDataset(train_x_tensor, train_target_tensor)\n
+\n
+    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n
+\n
+    # ------- val_loader -------\n
+    val_x_tensor = torch.from_numpy(val_x).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)\n
+    val_target_tensor = torch.from_numpy(val_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\n
+\n
+    val_dataset = torch.utils.data.TensorDataset(val_x_tensor, val_target_tensor)\n
+\n
+    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n
+\n
+    # ------- test_loader -------\n
+    test_x_tensor = torch.from_numpy(test_x).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)\n
+    test_target_tensor = torch.from_numpy(test_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\n
+\n
+    test_dataset = torch.utils.data.TensorDataset(test_x_tensor, test_target_tensor)\n
+\n
+    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n
+\n
+    # print\n
+    print('train:', train_x_tensor.size(), train_target_tensor.size())\n
+    print('val:', val_x_tensor.size(), val_target_tensor.size())\n
+    print('test:', test_x_tensor.size(), test_target_tensor.size())\n
+\n
+    return train_loader, train_target_tensor, val_loader, val_target_tensor, test_loader, test_target_tensor, mean, std\n
+\n
+\n
+def compute_val_loss_mstgcn(net, val_loader, criterion,  masked_flag,missing_value,sw, epoch, limit=None):\n
+    '''\n
+    for rnn, compute mean loss on validation set\n
+    :param net: model\n
+    :param val_loader: torch.utils.data.utils.DataLoader\n
+    :param criterion: torch.nn.MSELoss\n
+    :param sw: tensorboardX.SummaryWriter\n
+    :param global_step: int, current global_step\n
+    :param limit: int,\n
+    :return: val_loss\n
+    '''\n
+\n
+    net.train(False)  # ensure dropout layers are in evaluation mode\n
+\n
+    with torch.no_grad():\n
+\n
+        val_loader_length = len(val_loader)  # nb of batch\n
+\n
+        tmp = []  # 记录了所有batch的loss\n
+\n
+        for batch_index, batch_data in enumerate(val_loader):\n
+            encoder_inputs, labels = batch_data\n
+            outputs = net(encoder_inputs)\n
+            if masked_flag:\n
+                loss = criterion(outputs, labels, missing_value)\n
+            else:\n
+                loss = criterion(outputs, labels)\n
+\n
+            tmp.append(loss.item())\n
+            if batch_index % 100 == 0:\n
+                print('validation batch %s / %s, loss: %.2f' % (batch_index + 1, val_loader_length, loss.item()))\n
+            if (limit is not None) and batch_index >= limit:\n
+                break\n
+\n
+        validation_loss = sum(tmp) / len(tmp)\n
+        sw.add_scalar('validation_loss', validation_loss, epoch)\n
+    return validation_loss\n
\\ No newline at end of file\n
diff --git a/dl_models/ASTGCN/load_config.py b/dl_models/ASTGCN/load_config.py\n
new file mode 100644\n
index 0000000..34acff5\n
--- /dev/null\n
+++ b/dl_models/ASTGCN/load_config.py\n
@@ -0,0 +1,24 @@\n
+import argparse\n
+\n
+parser = argparse.ArgumentParser(description='ASTGCN')\n
+\n
+parser.add_argument('--c_in', type=int, default=1, \n
+                    help='Channel dimension of the input. Usually = 1. As we consider trafic forecasting, we could set c_in = 2 if two time series are considered : Speed and Flow')\n
+args = parser.parse_args(args=[])\n
+\n
+parser_HP = argparse.ArgumentParser(description='HP')\n
+parser_HP.add_argument('--weight_decay', type=float, default=0.0005, help=\"weight decay for AdamW\")\n
+parser_HP.add_argument('--batch_size', type=int, default=32, help=\"Batch size\")\n
+parser_HP.add_argument('--lr', type=float, default=5e-3, help=\"Lr\")\n
+parser_HP.add_argument('--dropout', type=float, default=0.2, help=\"Dropout\")\n
+parser_HP.add_argument('--epochs', type=int, default=100, help=\"Epochs\")\n
+parser_HP.add_argument('--scheduler', type=bool, default=None, choices = [True, None], help=\"If True then acitvate a Lr scheduler with a warmup before reducing following an exponential function\")\n
+args_HP = parser_HP.parse_args(args=[])\n
+# Other possible parameters: \n
+if False: \n
+\n
+    parser_HP.add_argument(\"--momentum\", type=float, default=0.95, help=\"momentum for SGD\")\n
+    parser_HP.add_argument('--scheduler', type=bool, default=True, help=\"If True then acitvate a Lr scheduler with a warmup before reducing following an exponential function\")\n
+    parser_HP.add_argument('--torch_scheduler_milestone', type=int, default=5, help=\"Number of epochs while we have a Lr warming up\")\n
+    parser_HP.add_argument('--torch_scheduler_gamma', type=float, default=0.99, help=\"Exponential coefficient associated to the lr decrease\")\n
+    parser_HP.add_argument('--torch_scheduler_lr_start_factor', type=float, default=0.2, help=\"Multiplicator coefficient of the lr for the first epoch, until reaching the value 'lr' at the epoch 'torch_scheduler_milestone\")\n
\\ No newline at end of file\n
diff --git a/dl_models/ASTGCN/search_space.py b/dl_models/ASTGCN/search_space.py\n
new file mode 100644\n
index 0000000..4ffa8af\n
--- /dev/null\n
+++ b/dl_models/ASTGCN/search_space.py\n
@@ -0,0 +1,5 @@\n
+from ray import tune\n
+from itertools import product\n
+\n
+config = {\n
+          }\n
\\ No newline at end of file\n"
{"commit":"ee1503b9c84899be3ee9a89d38c28cbf24a86541","author":"Fr-ocasting","date":"2025-04-10T10:37:54+02:00","message":"commit"},"diff":"diff --git a/build_inputs/load_netmob_data.py b/build_inputs/load_netmob_data.py\n
index 229a042..0a08c8c 100644\n
--- a/build_inputs/load_netmob_data.py\n
+++ b/build_inputs/load_netmob_data.py\n
@@ -167 +167 @@ def tackle_netmob(dataset,invalid_dates,intersect_coverage_period,args,normalize\n
-\n
+        \n
diff --git a/build_inputs/load_preprocessed_dataset.py b/build_inputs/load_preprocessed_dataset.py\n
index 2541c40..3104258 100644\n
--- a/build_inputs/load_preprocessed_dataset.py\n
+++ b/build_inputs/load_preprocessed_dataset.py\n
@@ -238 +238,2 @@ def load_complete_ds(args,coverage_period = None,normalize = True):\n
-    print('NetMob_ds.U_valid',NetMob_ds.U_valid.size())\n
+    if NetMob_ds is not None:\n
+        print('NetMob_ds.U_valid',NetMob_ds.U_valid.size())\n
diff --git a/constants/config.py b/constants/config.py\n
index 67de0e8..4c8c906 100644\n
--- a/constants/config.py\n
+++ b/constants/config.py\n
@@ -148 +148 @@ def get_config(model_name,dataset_names,dataset_for_coverage,config = {}):\n
-    config['stacked_contextual'] = False # If True then stack contextual information to the channel dim. Does not consider anymore contextual tensors but an input tensor.\n
+    config['stacked_contextual'] = True # If True then stack contextual information to the channel dim. Does not consider anymore contextual tensors but an input tensor.\n
diff --git a/dataset.py b/dataset.py\n
index 1c11f06..cbf6e0c 100644\n
--- a/dataset.py\n
+++ b/dataset.py\n
@@ -14,0 +15 @@ from utils.utilities import load_inputs_from_dataloader\n
+from packaging.version import Version\n
@@ -205 +206,8 @@ class Normalizer(object):\n
-        output = torch.nan_to_num(output_with_nan_and_inf,0,0,0)  # Set 0 when devided by maxi - mini = 0 (0 when Nan, 0 when +inf, 0 when -inf\n
+        if Version(torch.__version__) >= Version(\"2.0.0\"):\n
+            output = torch.nan_to_num(output_with_nan_and_inf,0,0,0)  # Set 0 when devided by maxi - mini = 0 (0 when Nan, 0 when +inf, 0 when -inf\n
+        else:\n
+            output = output_with_nan_and_inf.clone()\n
+            output[torch.isnan(output)] = 0\n
+            output[torch.isinf(output)] = 0\n
+            output[output == float('inf')] = 0\n
+            output[output == float('-inf')] = 0\n
diff --git a/load_inputs/subway_out.py b/load_inputs/subway_out.py\n
index b8943a3..55a7a66 100644\n
--- a/load_inputs/subway_out.py\n
+++ b/load_inputs/subway_out.py\n
@@ -42 +42 @@ def load_data(dataset,args,ROOT,FOLDER_PATH,intersect_coverage_period,normalize,\n
-    T_subway_out = torch.Tensor(subway_out.raw_values)\n
+    T_subway_out = torch.Tensor(subway_out.raw_values.float())\n"
{"commit":"1c4439a77531b18880570418414c5c229c588caa","author":"Fr-ocasting","date":"2025-04-09T19:17:09+02:00","message":"commit"},"diff":"diff --git a/dataset.py b/dataset.py\n
index d2a81e8..1c11f06 100644\n
--- a/dataset.py\n
+++ b/dataset.py\n
@@ -13,0 +14 @@ from data_augmentation.data_augmentation import DataAugmenter\n
+from utils.utilities import load_inputs_from_dataloader\n
@@ -489,0 +491,4 @@ class DataSet(object):\n
+    def load_all_inputs_from_training_mode(self,training_mode):\n
+        X,Y,X_c,nb_contextual = load_inputs_from_dataloader(self.dataloader[training_mode],self.args.device)\n
+        return X,Y,X_c,nb_contextual\n
+\n
diff --git a/jupyter_ipynb/Entropy_and_information/generate_variables.py b/jupyter_ipynb/Entropy_and_information/generate_variables.py\n
index 4d89adc..3e15c62 100644\n
--- a/jupyter_ipynb/Entropy_and_information/generate_variables.py\n
+++ b/jupyter_ipynb/Entropy_and_information/generate_variables.py\n
@@ -8 +8 @@ import matplotlib.pyplot as plt\n
-\n
+import inspect\n
@@ -66,3 +66,2 @@ def generate_linear_causal_series(n=200, lags=[2, 4], coeffs=[0.6, 0.3], noise_l\n
-    print(f\"Expected Granger causality: X -> Y at lags {lags}\")\n
-    print(f\"No Granger causality: Y -> X\")\n
-    \n
+    print(f\"Expected Granger causality:\\nX -> Y at lags {lags}\\n\\nNo Granger causality:\\nY -> X\")\n
+\n
@@ -133,2 +132 @@ def generate_bidirectional_causality(n=200, xy_lags=[2], xy_coeffs=[0.4],\n
-    print(f\"Expected Granger causality: X -> Y at lags {xy_lags}\")\n
-    print(f\"Expected Granger causality: Y -> X at lags {yx_lags}\")\n
+    print(f\"Expected Granger causality:\\nX -> Y at lags {xy_lags}\\n Y -> X at lags {yx_lags}\")\n
@@ -213 +211 @@ def generate_common_cause(n=200, xz_lags=[2], xz_coeffs=[0.6],\n
-    print(\"No direct Granger causality between X and Y\")\n
+    print(\"\\nNo direct Granger causality:\\nX and Y\")\n
@@ -265,2 +263,2 @@ def generate_nonlinear_causality(n=200, lag=2, noise_level=0.2, seed=None):\n
-    print(f\"Expected non-linear Granger causality: X -> Y at lag {lag}\")\n
-    print(\"Standard linear Granger test might not fully capture this relationship\")\n
+    print(f\"Expected non-linear Granger causality:\\nX -> Y at lag {lag}\")\n
+    print(\"\\nStandard linear Granger test might not fully capture this relationship\")\n
@@ -326,2 +324,2 @@ def generate_seasonal_causality(n=300, period=24, lag=6, seasonal_strength=0.8,\n
-    print(f\"Expected Granger causality: X -> Y at lag {lag}\")\n
-    print(\"Both X and Y have seasonality - needs differencing to be stationary\")\n
+    print(f\"Expected Granger causality:\\nX -> Y at lag {lag}\")\n
+    print(\"\\nBoth X and Y have seasonality - needs differencing to be stationary\")\n
@@ -400 +398,3 @@ def test_granger_with_generated_data(data_generator, params, max_lag=10):\n
-    result = data_generator(**params)\n
+    sig = inspect.signature(data_generator)\n
+    args_generator = {k: v for k, v in params.items() if k in sig.parameters}\n
+    result = data_generator(**args_generator)\n
@@ -415 +415,2 @@ def test_granger_with_generated_data(data_generator, params, max_lag=10):\n
-    results = gc.full_analysis(max_lag=max_lag)\n
+    criterion = params['criterion'] if 'criterion' in params else 'BIC'\n
+    results = gc.full_analysis(max_lag=max_lag,criterion=criterion)\n
diff --git a/jupyter_ipynb/Entropy_and_information/granger.py b/jupyter_ipynb/Entropy_and_information/granger.py\n
index d6d7225..01b26d3 100644\n
--- a/jupyter_ipynb/Entropy_and_information/granger.py\n
+++ b/jupyter_ipynb/Entropy_and_information/granger.py\n
@@ -39,5 +38,0 @@ class GrangerCausalityAnalysis:\n
-        \n
-    def set_data(self, data):\n
-        \"\"\"Set time series data for analysis.\"\"\"\n
-        self.data = data\n
-        return self\n
@@ -63,22 +58,4 @@ class GrangerCausalityAnalysis:\n
-        \n
-        # If a specific series name is provided\n
-        if isinstance(series, str) and self.data is not None:\n
-            if series in self.data.columns:\n
-                series_data = self.data[series].dropna()\n
-                result = adfuller(series_data)\n
-                is_stationary = result[1] < alpha\n
-                results[series] = {\n
-                    'Test Statistic': result[0],\n
-                    'p-value': result[1],\n
-                    'Critical Values': result[4],\n
-                    'Stationary': is_stationary\n
-                }\n
-                \n
-                if verbose:\n
-                    print(f\"Series '{series}': {'Stationary' if is_stationary else 'Non-stationary'} \"\n
-                          f\"(p-value: {result[1]:.4f}, ADF: {result[0]:.4f})\")\n
-                return results\n
-        \n
-        # If a Series object is provided\n
-        elif isinstance(series, (pd.Series, np.ndarray)):\n
-            result = adfuller(series)\n
+\n
+        for column in self.data.columns:\n
+            series_data = self.data[column].dropna()\n
+            result = adfuller(series_data)\n
@@ -86 +63 @@ class GrangerCausalityAnalysis:\n
-            results['series'] = {\n
+            results[column] = {\n
@@ -94,20 +71,2 @@ class GrangerCausalityAnalysis:\n
-                print(f\"Series: {'Stationary' if is_stationary else 'Non-stationary'} \"\n
-                      f\"(p-value: {result[1]:.4f}, ADF: {result[0]:.4f})\")\n
-            return results\n
-        \n
-        # Check all columns in the dataframe\n
-        if self.data is not None:\n
-            for column in self.data.columns:\n
-                series_data = self.data[column].dropna()\n
-                result = adfuller(series_data)\n
-                is_stationary = result[1] < alpha\n
-                results[column] = {\n
-                    'Test Statistic': result[0],\n
-                    'p-value': result[1],\n
-                    'Critical Values': result[4],\n
-                    'Stationary': is_stationary\n
-                }\n
-                \n
-                if verbose:\n
-                    print(f\"Series '{column}': {'Stationary' if is_stationary else 'Non-stationary'} \"\n
-                          f\"(p-value: {result[1]:.4f}, ADF: {result[0]:.4f})\")\n
+                print(f\"Series '{column}': {'Stationary' if is_stationary else 'Non-stationary'} \"\n
+                        f\"(p-value: {result[1]:.4f}, ADF: {result[0]:.4f})\")\n
@@ -117 +76 @@ class GrangerCausalityAnalysis:\n
-    def make_stationary(self, max_diff=2, verbose=True):\n
+    def make_stationary(self, max_diff=2, verbose=True, regression='c', autolag='AIC'):\n
@@ -126,0 +86,12 @@ class GrangerCausalityAnalysis:\n
+        regression : str, optional\n
+            Regression type for the ADF test. Options:\n
+            'c' : constant only (default)\n
+            'ct' : constant and trend\n
+            'ctt' : constant, linear and quadratic trend\n
+            'n' : no regression components\n
+        autolag : str or None, optional\n
+            Method for lag selection in ADF test. Options:\n
+            'AIC' : Akaike Information Criterion (default)\n
+            'BIC' : Bayesian Information Criterion\n
+            't-stat' : Based on t-statistic significance\n
+            None : No automatic lag selection\n
@@ -132,3 +102,0 @@ class GrangerCausalityAnalysis:\n
-        if self.data is None:\n
-            raise ValueError(\"No data available. Please set data first.\")\n
-            \n
@@ -154 +122 @@ class GrangerCausalityAnalysis:\n
-                test_result = adfuller(series.dropna())\n
+                test_result = adfuller(series.dropna(),regression = regression,autolag=autolag)\n
@@ -189,7 +157 @@ class GrangerCausalityAnalysis:\n
-        \"\"\"\n
-        if self.stationary_data is None:\n
-            if self.data is not None:\n
-                self.make_stationary(verbose=False)\n
-            else:\n
-                raise ValueError(\"No data available. Please set data first.\")\n
-                \n
+        \"\"\"            \n
@@ -200,6 +162 @@ class GrangerCausalityAnalysis:\n
-        optimal_lags = {\n
-            'AIC': results.aic,\n
-            'BIC': results.bic,\n
-            'FPE': results.fpe,\n
-            'HQIC': results.hqic\n
-        }\n
+        optimal_lags = { 'AIC': results.aic, 'BIC': results.bic, 'FPE': results.fpe,'HQIC': results.hqic}\n
@@ -208 +164,0 @@ class GrangerCausalityAnalysis:\n
-\n
@@ -213,2 +169,2 @@ class GrangerCausalityAnalysis:\n
-            for criterion, lag in optimal_lags.items():\n
-                print(f\"  {criterion}: {lag}\")\n
+            for criterion_i, lag_i in optimal_lags.items():\n
+                print(f\"  {criterion_i}: {lag_i}\")\n
@@ -234,6 +189,0 @@ class GrangerCausalityAnalysis:\n
-        if self.stationary_data is None:\n
-            if self.data is not None:\n
-                self.make_stationary(verbose=False)\n
-            else:\n
-                raise ValueError(\"No data available. Please set data first.\")\n
-                \n
@@ -241,2 +190,0 @@ class GrangerCausalityAnalysis:\n
-            if self.optimal_lag is None:\n
-                self.select_lag_order(verbose=False)\n
@@ -314,7 +262 @@ class GrangerCausalityAnalysis:\n
-        \"\"\"\n
-        if self.stationary_data is None:\n
-            if self.data is not None:\n
-                self.make_stationary(verbose=False)\n
-            else:\n
-                raise ValueError(\"No data available. Please set data first.\")\n
-                \n
+        \"\"\"       \n
@@ -322,2 +263,0 @@ class GrangerCausalityAnalysis:\n
-            if self.optimal_lag is None:\n
-                self.select_lag_order(verbose=False)\n
@@ -326,4 +265,0 @@ class GrangerCausalityAnalysis:\n
-        # If no specific variable pairs are provided, use promising combinations\n
-        if variables is None:\n
-            variables = self.select_promising_combinations()\n
-            \n
@@ -366 +302 @@ class GrangerCausalityAnalysis:\n
-    def full_analysis(self, max_lag=10, max_diff=2, criterion='BIC'):\n
+    def full_analysis(self, max_lag=10, max_diff=2, criterion='BIC',make_stationnary_regression='c', make_stationnary_autolag='AIC'):\n
@@ -396 +332 @@ class GrangerCausalityAnalysis:\n
-            self.make_stationary(max_diff=max_diff)\n
+            self.make_stationary(max_diff=max_diff,regression=make_stationnary_regression, autolag=make_stationnary_autolag)\n
diff --git a/trainer.py b/trainer.py\n
index 6a0a285..f354e1b 100644\n
--- a/trainer.py\n
+++ b/trainer.py\n
@@ -28,0 +29 @@ from utils.metrics import evaluate_metrics\n
+from utils.utilities import load_inputs_from_dataloader\n
@@ -481,5 +482 @@ class Trainer(object):\n
-        inputs_i = [[x,y,x_c] for  x,y,x_c  in self.dataloader[training_mode]]\n
-        X = torch.cat([x for x,_,_ in inputs_i]).to(self.args.device)\n
-        Y = torch.cat([y for _,y,_ in inputs_i]).to(self.args.device)\n
-        nb_contextual = len(next(iter(self.dataloader[training_mode]))[2])\n
-        X_c = [torch.cat([x_c[k] for _,_,x_c in inputs_i]).to(self.args.device) for k in range(nb_contextual)]\n
+        X,Y,X_c,nb_contextual = load_inputs_from_dataloader(self.dataloader[training_mode],self.args.device)\n
diff --git a/utils/seasonal_decomposition.py b/utils/seasonal_decomposition.py\n
index 7307d82..ff0a459 100644\n
--- a/utils/seasonal_decomposition.py\n
+++ b/utils/seasonal_decomposition.py\n
@@ -13 +13,4 @@ import math\n
-from statsmodels.tsa.seasonal import seasonal_decompose, STL , MSTL\n
+try:\n
+    from statsmodels.tsa.seasonal import seasonal_decompose, STL , MSTL\n
+except ImportError:\n
+    print(\"statsmodels not installed or issue with version. Please install or update it to use seasonal decomposition.\")\n
diff --git a/utils/utilities.py b/utils/utilities.py\n
index 5864313..c4325ed 100644\n
--- a/utils/utilities.py\n
+++ b/utils/utilities.py\n
@@ -13,0 +14,8 @@ import inspect\n
+def load_inputs_from_dataloader(dataloader,device):\n
+        inputs_i = [[x,y,x_c] for  x,y,x_c  in dataloader]\n
+        X = torch.cat([x for x,_,_ in inputs_i]).to(device)\n
+        Y = torch.cat([y for _,y,_ in inputs_i]).to(device)\n
+        nb_contextual = len(next(iter(dataloader))[2])\n
+        X_c = [torch.cat([x_c[k] for _,_,x_c in inputs_i]).to(device) for k in range(nb_contextual)]\n
+        return X,Y,X_c,nb_contextual\n
+\n"
{"commit":"a970fb843bb852378cfce599ec7dfa2c15dd1937","author":"Fr-ocasting","date":"2025-04-09T13:49:45+02:00","message":"commit"},"diff":"diff --git a/jupyter_ipynb/Entropy_and_information/generate_variables.py b/jupyter_ipynb/Entropy_and_information/generate_variables.py\n
index 7339c68..4d89adc 100644\n
--- a/jupyter_ipynb/Entropy_and_information/generate_variables.py\n
+++ b/jupyter_ipynb/Entropy_and_information/generate_variables.py\n
@@ -4,0 +5,414 @@ from mpl_toolkits.mplot3d import Axes3D\n
+import numpy as np\n
+import pandas as pd\n
+import matplotlib.pyplot as plt\n
+\n
+import os, sys\n
+sys.path.append(os.path.abspath('../..'))\n
+from jupyter_ipynb.Entropy_and_information.granger import GrangerCausalityAnalysis\n
+\n
+def generate_linear_causal_series(n=200, lags=[2, 4], coeffs=[0.6, 0.3], noise_level=0.2, seed=None):\n
+    \"\"\"\n
+    Generate time series with a clear linear causal relationship: X -> Y\n
+    \n
+    Parameters:\n
+    -----------\n
+    n : int\n
+        Number of time points\n
+    lags : list\n
+        List of lags at which X influences Y\n
+    coeffs : list\n
+        Coefficients for each lag (strength of influence)\n
+    noise_level : float\n
+        Standard deviation of noise\n
+    seed : int, optional\n
+        Random seed\n
+        \n
+    Returns:\n
+    --------\n
+    t : array\n
+        Time points\n
+    x : array\n
+        Cause variable X\n
+    y : array\n
+        Effect variable Y (influenced by X)\n
+    \"\"\"\n
+    if seed is not None:\n
+        np.random.seed(seed)\n
+    \n
+    # Time points\n
+    t = np.arange(n)\n
+    \n
+    # Generate X as a stationary AR(1) process\n
+    x = np.zeros(n)\n
+    x[0] = np.random.randn()\n
+    for i in range(1, n):\n
+        x[i] = 0.5 * x[i-1] + np.random.randn() * noise_level\n
+    \n
+    # Generate Y influenced by lagged values of X\n
+    y = np.zeros(n)\n
+    y[0] = np.random.randn() * noise_level\n
+    \n
+    max_lag = max(lags)\n
+    for i in range(1, n):\n
+        # Autoregressive component for Y\n
+        y[i] = 0.2 * y[i-1] + np.random.randn() * noise_level\n
+        \n
+        # Add influence from X at specified lags\n
+        for lag, coeff in zip(lags, coeffs):\n
+            if i >= lag:\n
+                y[i] += coeff * x[i-lag]\n
+    \n
+    # Expected result: X Granger-causes Y at the specified lags\n
+    print(f\"Expected Granger causality: X -> Y at lags {lags}\")\n
+    print(f\"No Granger causality: Y -> X\")\n
+    \n
+    return t, x, y\n
+\n
+def generate_bidirectional_causality(n=200, xy_lags=[2], xy_coeffs=[0.4], \n
+                                    yx_lags=[3], yx_coeffs=[0.2], \n
+                                    noise_level=0.2, seed=None):\n
+    \"\"\"\n
+    Generate time series with bidirectional causality: X <-> Y with different strengths\n
+    \n
+    Parameters:\n
+    -----------\n
+    n : int\n
+        Number of time points\n
+    xy_lags, yx_lags : list\n
+        Lags for X->Y and Y->X influences\n
+    xy_coeffs, yx_coeffs : list\n
+        Coefficients for each lag (typically xy_coeffs > yx_coeffs for X->Y stronger than Y->X)\n
+    noise_level : float\n
+        Standard deviation of noise\n
+    seed : int, optional\n
+        Random seed\n
+        \n
+    Returns:\n
+    --------\n
+    t : array\n
+        Time points\n
+    x : array\n
+        X variable\n
+    y : array\n
+        Y variable\n
+    \"\"\"\n
+    if seed is not None:\n
+        np.random.seed(seed)\n
+    \n
+    t = np.arange(n)\n
+    x = np.zeros(n)\n
+    y = np.zeros(n)\n
+    \n
+    # Initialize with random values\n
+    x[0] = np.random.randn() * noise_level\n
+    y[0] = np.random.randn() * noise_level\n
+    \n
+    # Maximum lags\n
+    max_xy_lag = max(xy_lags) if xy_lags else 0\n
+    max_yx_lag = max(yx_lags) if yx_lags else 0\n
+    max_lag = max(max_xy_lag, max_yx_lag)\n
+    \n
+    # Fill in first values where lags don't apply yet\n
+    for i in range(1, max_lag+1):\n
+        x[i] = 0.3 * x[i-1] + np.random.randn() * noise_level\n
+        y[i] = 0.3 * y[i-1] + np.random.randn() * noise_level\n
+    \n
+    # Generate series with bidirectional influences\n
+    for i in range(max_lag+1, n):\n
+        # X with influence from Y\n
+        x[i] = 0.3 * x[i-1] + np.random.randn() * noise_level\n
+        for lag, coeff in zip(yx_lags, yx_coeffs):\n
+            x[i] += coeff * y[i-lag]\n
+        \n
+        # Y with influence from X\n
+        y[i] = 0.3 * y[i-1] + np.random.randn() * noise_level\n
+        for lag, coeff in zip(xy_lags, xy_coeffs):\n
+            y[i] += coeff * x[i-lag]\n
+    \n
+    # Expected results\n
+    print(f\"Expected Granger causality: X -> Y at lags {xy_lags}\")\n
+    print(f\"Expected Granger causality: Y -> X at lags {yx_lags}\")\n
+    print(f\"Stronger causality: {'X -> Y' if max(xy_coeffs) > max(yx_coeffs) else 'Y -> X'}\")\n
+    \n
+    return t, x, y\n
+\n
+def generate_common_cause(n=200, xz_lags=[2], xz_coeffs=[0.6], \n
+                          yz_lags=[3], yz_coeffs=[0.6],\n
+                          noise_level=0.2, seed=None):\n
+    \"\"\"\n
+    Generate time series where Z is a common cause for both X and Y: Z -> X and Z -> Y\n
+    No direct causal link between X and Y.\n
+    \n
+    Parameters:\n
+    -----------\n
+    n : int\n
+        Number of time points\n
+    xz_lags, yz_lags : list\n
+        Lags for Z->X and Z->Y influences\n
+    xz_coeffs, yz_coeffs : list\n
+        Coefficients for each lag\n
+    noise_level : float\n
+        Standard deviation of noise\n
+    seed : int, optional\n
+        Random seed\n
+        \n
+    Returns:\n
+    --------\n
+    t : array\n
+        Time points\n
+    x : array\n
+        X variable (affected by Z)\n
+    y : array\n
+        Y variable (affected by Z)\n
+    z : array\n
+        Z variable (common cause)\n
+    \"\"\"\n
+    if seed is not None:\n
+        np.random.seed(seed)\n
+    \n
+    t = np.arange(n)\n
+    \n
+    # Generate Z as a stationary AR(1) process\n
+    z = np.zeros(n)\n
+    z[0] = np.random.randn()\n
+    for i in range(1, n):\n
+        z[i] = 0.5 * z[i-1] + np.random.randn() * noise_level\n
+    \n
+    # Initialize X and Y\n
+    x = np.zeros(n)\n
+    y = np.zeros(n)\n
+    x[0] = np.random.randn() * noise_level\n
+    y[0] = np.random.randn() * noise_level\n
+    \n
+    # Maximum lags\n
+    max_xz_lag = max(xz_lags) if xz_lags else 0\n
+    max_yz_lag = max(yz_lags) if yz_lags else 0\n
+    max_lag = max(max_xz_lag, max_yz_lag)\n
+    \n
+    # Fill in first values\n
+    for i in range(1, max_lag+1):\n
+        x[i] = 0.3 * x[i-1] + np.random.randn() * noise_level\n
+        y[i] = 0.3 * y[i-1] + np.random.randn() * noise_level\n
+    \n
+    # Generate X and Y influenced by Z\n
+    for i in range(max_lag+1, n):\n
+        # X influenced by Z\n
+        x[i] = 0.3 * x[i-1] + np.random.randn() * noise_level\n
+        for lag, coeff in zip(xz_lags, xz_coeffs):\n
+            x[i] += coeff * z[i-lag]\n
+        \n
+        # Y influenced by Z\n
+        y[i] = 0.3 * y[i-1] + np.random.randn() * noise_level\n
+        for lag, coeff in zip(yz_lags, yz_coeffs):\n
+            y[i] += coeff * z[i-lag]\n
+    \n
+    # Expected results\n
+    print(\"Expected Granger causality:\")\n
+    print(f\"Z -> X at lags {xz_lags}\")\n
+    print(f\"Z -> Y at lags {yz_lags}\")\n
+    print(\"No direct Granger causality between X and Y\")\n
+    print(\"However, false positive X -> Y or Y -> X may appear due to common cause\")\n
+    \n
+    return t, x, y, z\n
+\n
+def generate_nonlinear_causality(n=200, lag=2, noise_level=0.2, seed=None):\n
+    \"\"\"\n
+    Generate time series with a non-linear causal relationship: X -> Y\n
+    \n
+    Parameters:\n
+    -----------\n
+    n : int\n
+        Number of time points\n
+    lag : int\n
+        Lag at which X influences Y\n
+    noise_level : float\n
+        Standard deviation of noise\n
+    seed : int, optional\n
+        Random seed\n
+        \n
+    Returns:\n
+    --------\n
+    t : array\n
+        Time points\n
+    x : array\n
+        Cause variable X\n
+    y : array\n
+        Effect variable Y (non-linearly influenced by X)\n
+    \"\"\"\n
+    if seed is not None:\n
+        np.random.seed(seed)\n
+    \n
+    t = np.arange(n)\n
+    \n
+    # Generate X as a stationary AR(1) process\n
+    x = np.zeros(n)\n
+    x[0] = np.random.randn()\n
+    for i in range(1, n):\n
+        x[i] = 0.5 * x[i-1] + np.random.randn() * noise_level\n
+    \n
+    # Generate Y influenced by non-linear function of X\n
+    y = np.zeros(n)\n
+    y[0] = np.random.randn() * noise_level\n
+    \n
+    for i in range(1, n):\n
+        # Autoregressive component\n
+        y[i] = 0.3 * y[i-1] + np.random.randn() * noise_level\n
+        \n
+        # Add non-linear influence from X\n
+        if i >= lag:\n
+            y[i] += 0.5 * np.sin(x[i-lag]) + 0.3 * x[i-lag]**2\n
+    \n
+    print(f\"Expected non-linear Granger causality: X -> Y at lag {lag}\")\n
+    print(\"Standard linear Granger test might not fully capture this relationship\")\n
+    \n
+    return t, x, y\n
+\n
+def generate_seasonal_causality(n=300, period=24, lag=6, seasonal_strength=0.8, \n
+                              causal_strength=0.5, noise_level=0.2, seed=None):\n
+    \"\"\"\n
+    Generate seasonal time series with causality: X -> Y\n
+    X has strong seasonality, and influences Y with a lag\n
+    \n
+    Parameters:\n
+    -----------\n
+    n : int\n
+        Number of time points\n
+    period : int\n
+        Seasonal period (e.g., 24 for daily data)\n
+    lag : int\n
+        Lag at which X influences Y\n
+    seasonal_strength : float\n
+        Strength of seasonal component\n
+    causal_strength : float\n
+        Strength of causal influence\n
+    noise_level : float\n
+        Standard deviation of noise\n
+    seed : int, optional\n
+        Random seed\n
+        \n
+    Returns:\n
+    --------\n
+    t : array\n
+        Time points\n
+    x : array\n
+        Seasonal cause variable X\n
+    y : array\n
+        Effect variable Y\n
+    \"\"\"\n
+    if seed is not None:\n
+        np.random.seed(seed)\n
+    \n
+    t = np.arange(n)\n
+    \n
+    # Generate X with seasonality + trend + noise\n
+    trend = 0.01 * t\n
+    seasonality = seasonal_strength * np.sin(2 * np.pi * t / period)\n
+    noise = np.random.normal(0, noise_level, n)\n
+    \n
+    x = trend + seasonality + noise\n
+    \n
+    # Generate Y influenced by lagged X + its own seasonality\n
+    y = np.zeros(n)\n
+    y_seasonality = 0.3 * np.sin(2 * np.pi * t / period + np.pi/3)  # Different phase\n
+    \n
+    for i in range(n):\n
+        # Y has some seasonality too\n
+        y[i] = y_seasonality[i] + np.random.normal(0, noise_level)\n
+        \n
+        # Add influence from X with lag\n
+        if i >= lag:\n
+            y[i] += causal_strength * x[i-lag]\n
+    \n
+    print(f\"Expected Granger causality: X -> Y at lag {lag}\")\n
+    print(\"Both X and Y have seasonality - needs differencing to be stationary\")\n
+    \n
+    return t, x, y\n
+\n
+def generate_intermittent_causality(n=400, active_periods=[(50, 100), (200, 250)], \n
+                                   lag=2, causal_strength=0.6, noise_level=0.2, seed=None):\n
+    \"\"\"\n
+    Generate time series where X -> Y only during specific time periods\n
+    \n
+    Parameters:\n
+    -----------\n
+    n : int\n
+        Number of time points\n
+    active_periods : list of tuples\n
+        List of (start, end) periods where causality is active\n
+    lag : int\n
+        Lag at which X influences Y\n
+    causal_strength : float\n
+        Strength of causal influence during active periods\n
+    noise_level : float\n
+        Standard deviation of noise\n
+    seed : int, optional\n
+        Random seed\n
+        \n
+    Returns:\n
+    --------\n
+    t : array\n
+        Time points\n
+    x : array\n
+        Cause variable X\n
+    y : array\n
+        Effect variable Y (intermittently influenced by X)\n
+    \"\"\"\n
+    if seed is not None:\n
+        np.random.seed(seed)\n
+    \n
+    t = np.arange(n)\n
+    \n
+    # Generate X as a stationary AR(1) process\n
+    x = np.zeros(n)\n
+    x[0] = np.random.randn()\n
+    for i in range(1, n):\n
+        x[i] = 0.5 * x[i-1] + np.random.randn() * noise_level\n
+    \n
+    # Generate Y with intermittent influence from X\n
+    y = np.zeros(n)\n
+    y[0] = np.random.randn() * noise_level\n
+    \n
+    # Create mask for active periods\n
+    is_active = np.zeros(n, dtype=bool)\n
+    for start, end in active_periods:\n
+        is_active[start:end] = True\n
+    \n
+    for i in range(1, n):\n
+        # Autoregressive component\n
+        y[i] = 0.3 * y[i-1] + np.random.randn() * noise_level\n
+        \n
+        # Add influence from X only during active periods\n
+        if i >= lag and is_active[i]:\n
+            y[i] += causal_strength * x[i-lag]\n
+    \n
+    print(f\"Expected intermittent Granger causality: X -> Y at lag {lag}\")\n
+    print(f\"Active during periods: {active_periods}\")\n
+    print(\"Standard Granger test on full series might show weaker causality\")\n
+    \n
+    return t, x, y\n
+\n
+# Helper function to demonstrate and test\n
+def test_granger_with_generated_data(data_generator, params, max_lag=10):\n
+    \"\"\"\n
+    Generate data, visualize it, and run Granger causality test\n
+    \"\"\"\n
+    # Generate data\n
+    result = data_generator(**params)\n
+    t = result[0]\n
+    \n
+    # Create DataFrame\n
+    columns = ['x', 'y'] if len(result) == 3 else ['x', 'y', 'z']\n
+    df = pd.DataFrame(dict(zip(columns, result[1:])), index=t)\n
+    \n
+    # Plot\n
+    plt.figure(figsize=(12, 6))\n
+    df.plot()\n
+    plt.title(f\"Generated time series: {data_generator.__name__}\")\n
+    plt.show()\n
+    \n
+    # Run Granger causality analysis\n
+    gc = GrangerCausalityAnalysis(df)\n
+    results = gc.full_analysis(max_lag=max_lag)\n
+    \n
+    return df, results\n
+\n"
{"commit":"3a4cb2cde4a0d0904655b5e1718763c7ef69f353","author":"Fr-ocasting","date":"2025-04-08T19:32:38+02:00","message":"commit"},"diff":"diff --git a/jupyter_ipynb/Entropy_and_information/generate_variables.py b/jupyter_ipynb/Entropy_and_information/generate_variables.py\n
index f599cb1..7339c68 100644\n
--- a/jupyter_ipynb/Entropy_and_information/generate_variables.py\n
+++ b/jupyter_ipynb/Entropy_and_information/generate_variables.py\n
@@ -102,0 +103,45 @@ def plot_lorenz(x_noisy,y_noisy,z_noisy):\n
+\n
+def load_variables_with_lagged_peak(seed=123,max_lag = 6, same_amplitude_through_lag = False,same_amplitude_per_lag=False,random_amplitude=False):\n
+    np.random.seed(seed)\n
+\n
+    # Paramètres\n
+    days = 30\n
+    hours_per_day = 24\n
+    N = days * hours_per_day  # 720\n
+\n
+    # On crée deux séries\n
+    X_lagged_peak = np.random.rand(N) * 20  # baseline random\n
+    Y_random_peak = np.random.rand(N) * 10\n
+\n
+    # On ajoute des pics dans Y et des pics correspondants dans X avec des décalages variés\n
+    amplitudes = np.arange(20,100)\n
+    np.random.shuffle(amplitudes)\n
+    for d in range(days):\n
+        # Index jour = d * 24 à d*24+23\n
+        start = d * hours_per_day\n
+\n
+        lag = np.random.randint(1, max_lag)  # Décalage de 1 à 5 heures\n
+        \n
+        # Pic dans Y (à un moment aléatoire de la journée)\n
+        y_peak_hour = np.random.randint(6, 22)  # Pic entre 6h et 22h\n
+        y_peak_idx = start + y_peak_hour\n
+        x_peak_idx = start + y_peak_hour - lag  # X a un pic avant Y\n
+\n
+        if same_amplitude_through_lag:\n
+            Y_random_peak[y_peak_idx] += 30.0  # Pic dans Y\n
+            if x_peak_idx >= start:\n
+                X_lagged_peak[x_peak_idx] += 50.0  # Pic plus fort dans X\n
+        if same_amplitude_per_lag:\n
+            Y_random_peak[y_peak_idx] += amplitudes[lag]\n
+            if x_peak_idx >= start:\n
+                X_lagged_peak[x_peak_idx] += amplitudes[lag]*5/3  # Pic plus fort dans X\n
+        if random_amplitude:\n
+            Y_random_peak[y_peak_idx] += np.random.randint(20, 100)\n
+            if x_peak_idx >= start:\n
+                X_lagged_peak[x_peak_idx] += np.random.randint(20, 100)  # Pic plus fort dans X\n
+\n
+        \n
+        # S'assurer que l'index est valide (ne pas dépasser le jour précédent)\n
+\n
+    return X_lagged_peak,Y_random_peak\n
+\n
@@ -167,0 +213,29 @@ def load_variables_sinusoidales(n=100,T=2*np.pi,lag=np.pi/3,noise=True,seed=42,c\n
+\n
+def load_sinusoidales_sum(n=100,T=2*np.pi,lag=np.pi/3,noise=True,seed=42,alpha= [0.5,0.5,3]):\n
+    \"\"\"\n
+    Génère 3 séries temporelles: x et z sont des sinusoïdes, \n
+    y est la somme de x, z et d'une troisième sinusoïde.\n
+    :param n: nombre de points\n
+    :param T: longueur de la série\n
+    :param lag: décalage de phase\n
+    :param noise: si True, ajoute du bruit\n
+    :param seed: pour la reproductibilité\n
+    :return: t, x, y, z\n
+    \"\"\"\n
+    np.random.seed(seed)\n
+    t = np.linspace(0, T, n)  # n points from 0 to T\n
+\n
+    x = np.sin(t + lag)\n
+    z = np.sin(2*t + 2*lag)\n
+    \n
+    third_sin = np.sin(3*t)\n
+    \n
+    # y est la somme de x, z et third_sin\n
+    y = alpha[0]*np.sin(t) + alpha[1]*np.sin(2*t) + alpha[2]*third_sin\n
+    \n
+    # Ajout du bruit si demandé\n
+    if noise:\n
+        x += 0.05 * np.random.randn(len(t))\n
+        z += 0.05 * np.random.randn(len(t))\n
+        y += 0.05 * np.random.randn(len(t))\n
+    return t,x,y,z\n
diff --git a/jupyter_ipynb/Entropy_and_information/granger.py b/jupyter_ipynb/Entropy_and_information/granger.py\n
new file mode 100644\n
index 0000000..d6d7225\n
--- /dev/null\n
+++ b/jupyter_ipynb/Entropy_and_information/granger.py\n
@@ -0,0 +1,479 @@\n
+import numpy as np\n
+import pandas as pd\n
+import matplotlib.pyplot as plt\n
+from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n
+from statsmodels.tsa.api import VAR\n
+from statsmodels.tsa.vector_ar.var_model import VARResults\n
+from statsmodels.tools.eval_measures import rmse, aic\n
+import warnings\n
+warnings.filterwarnings('ignore')\n
+\n
+\n
+class GrangerCausalityAnalysis:\n
+    \"\"\"\n
+    A class for performing comprehensive Granger causality analysis on time series data.\n
+    \n
+    This class implements a step-by-step approach:\n
+    1. Check stationarity of the time series\n
+    2. Correct for non-stationarity if needed\n
+    3. Select optimal lag order using information criteria\n
+    4. Perform preliminary VAR analysis\n
+    5. Test Granger causality on promising variable combinations\n
+    \"\"\"\n
+    \n
+    def __init__(self, data=None):\n
+        \"\"\"\n
+        Initialize the GrangerCausalityAnalysis class.\n
+        \n
+        Parameters:\n
+        -----------\n
+        data : pandas.DataFrame, optional\n
+            DataFrame containing the time series data\n
+        \"\"\"\n
+        self.data = data\n
+        self.stationary_data = None\n
+        self.var_model = None\n
+        self.optimal_lag = None\n
+        self.diff_order = {}\n
+        self.causality_results = {}\n
+        \n
+    def set_data(self, data):\n
+        \"\"\"Set time series data for analysis.\"\"\"\n
+        self.data = data\n
+        return self\n
+    \n
+    def check_stationarity(self, series=None, alpha=0.05, verbose=True):\n
+        \"\"\"\n
+        Check stationarity of time series using Augmented Dickey-Fuller test.\n
+        \n
+        Parameters:\n
+        -----------\n
+        series : pandas.Series or str, optional\n
+            Series to test or column name in self.data\n
+        alpha : float, optional\n
+            Significance level for the test\n
+        verbose : bool, optional\n
+            Whether to print results\n
+            \n
+        Returns:\n
+        --------\n
+        dict : Results of stationarity tests\n
+        \"\"\"\n
+        results = {}\n
+        \n
+        # If a specific series name is provided\n
+        if isinstance(series, str) and self.data is not None:\n
+            if series in self.data.columns:\n
+                series_data = self.data[series].dropna()\n
+                result = adfuller(series_data)\n
+                is_stationary = result[1] < alpha\n
+                results[series] = {\n
+                    'Test Statistic': result[0],\n
+                    'p-value': result[1],\n
+                    'Critical Values': result[4],\n
+                    'Stationary': is_stationary\n
+                }\n
+                \n
+                if verbose:\n
+                    print(f\"Series '{series}': {'Stationary' if is_stationary else 'Non-stationary'} \"\n
+                          f\"(p-value: {result[1]:.4f}, ADF: {result[0]:.4f})\")\n
+                return results\n
+        \n
+        # If a Series object is provided\n
+        elif isinstance(series, (pd.Series, np.ndarray)):\n
+            result = adfuller(series)\n
+            is_stationary = result[1] < alpha\n
+            results['series'] = {\n
+                'Test Statistic': result[0],\n
+                'p-value': result[1],\n
+                'Critical Values': result[4],\n
+                'Stationary': is_stationary\n
+            }\n
+            \n
+            if verbose:\n
+                print(f\"Series: {'Stationary' if is_stationary else 'Non-stationary'} \"\n
+                      f\"(p-value: {result[1]:.4f}, ADF: {result[0]:.4f})\")\n
+            return results\n
+        \n
+        # Check all columns in the dataframe\n
+        if self.data is not None:\n
+            for column in self.data.columns:\n
+                series_data = self.data[column].dropna()\n
+                result = adfuller(series_data)\n
+                is_stationary = result[1] < alpha\n
+                results[column] = {\n
+                    'Test Statistic': result[0],\n
+                    'p-value': result[1],\n
+                    'Critical Values': result[4],\n
+                    'Stationary': is_stationary\n
+                }\n
+                \n
+                if verbose:\n
+                    print(f\"Series '{column}': {'Stationary' if is_stationary else 'Non-stationary'} \"\n
+                          f\"(p-value: {result[1]:.4f}, ADF: {result[0]:.4f})\")\n
+                    \n
+        return results\n
+    \n
+    def make_stationary(self, max_diff=2, verbose=True):\n
+        \"\"\"\n
+        Transform non-stationary series to stationary by differencing.\n
+        \n
+        Parameters:\n
+        -----------\n
+        max_diff : int, optional\n
+            Maximum number of differences to apply\n
+        verbose : bool, optional\n
+            Whether to print information about the differencing process\n
+            \n
+        Returns:\n
+        --------\n
+        pandas.DataFrame : Stationary data\n
+        \"\"\"\n
+        if self.data is None:\n
+            raise ValueError(\"No data available. Please set data first.\")\n
+            \n
+        stationary_data = self.data.copy()\n
+        \n
+        # Check and transform each column\n
+        for column in stationary_data.columns:\n
+            is_stationary = False\n
+            d = 0\n
+            series = stationary_data[column]\n
+            \n
+            # Check initial stationarity\n
+            test_result = adfuller(series.dropna())\n
+            is_stationary = test_result[1] < 0.05\n
+            \n
+            # Apply differencing until stationary or max_diff reached\n
+            while not is_stationary and d < max_diff:\n
+                # Apply differencing\n
+                series = series.diff().dropna()\n
+                d += 1\n
+                \n
+                # Check if stationary after differencing\n
+                test_result = adfuller(series.dropna())\n
+                is_stationary = test_result[1] < 0.05\n
+            \n
+            # Store differencing order\n
+            self.diff_order[column] = d\n
+            \n
+            # Apply the differencing to the data\n
+            if d > 0:\n
+                stationary_data[column] = stationary_data[column].diff(d).dropna()\n
+                \n
+        # Align all series after differencing (they might have different lengths)\n
+        self.stationary_data = stationary_data.dropna()\n
+        \n
+        if verbose:\n
+            print(\"Differencing applied:\")\n
+            for var, order in self.diff_order.items():\n
+                print(f\"  {var}: {order} difference(s)\")\n
+            print(f\"Final data shape: {self.stationary_data.shape}\")\n
+            \n
+        return self.stationary_data\n
+    \n
+    def select_lag_order(self, max_lag=10, criterion='BIC', verbose=True):\n
+        \"\"\"\n
+        Select optimal lag order for VAR model using information criteria.\n
+        \n
+        Parameters:\n
+        -----------\n
+        max_lag : int, optional\n
+            Maximum lag order to consider\n
+        verbose : bool, optional\n
+            Whether to print information about lag selection\n
+            \n
+        Returns:\n
+        --------\n
+        dict : Optimal lag orders according to different criteria\n
+        \"\"\"\n
+        if self.stationary_data is None:\n
+            if self.data is not None:\n
+                self.make_stationary(verbose=False)\n
+            else:\n
+                raise ValueError(\"No data available. Please set data first.\")\n
+                \n
+        model = VAR(self.stationary_data)\n
+        results = model.select_order(maxlags=max_lag)\n
+        \n
+        # Get the optimal lags according to different criteria\n
+        optimal_lags = {\n
+            'AIC': results.aic,\n
+            'BIC': results.bic,\n
+            'FPE': results.fpe,\n
+            'HQIC': results.hqic\n
+        }\n
+        \n
+        # Use BIC as default (tends to be more conservative)\n
+\n
+        self.optimal_lag = optimal_lags[criterion]\n
+            \n
+        if verbose:\n
+            print(\"Optimal lag selection by information criteria:\")\n
+            for criterion, lag in optimal_lags.items():\n
+                print(f\"  {criterion}: {lag}\")\n
+            print(f\"Selected lag ({criterion}): {self.optimal_lag}\")\n
+            \n
+        return optimal_lags\n
+    \n
+    def perform_var_analysis(self, lag=None, verbose=True,):\n
+        \"\"\"\n
+        Perform preliminary VAR analysis.\n
+        \n
+        Parameters:\n
+        -----------\n
+        lag : int, optional\n
+            Lag order for VAR model (uses optimal lag if None)\n
+        verbose : bool, optional\n
+            Whether to print VAR analysis results\n
+            \n
+        Returns:\n
+        --------\n
+        statsmodels.tsa.vector_ar.var_model.VARResults : VAR model results\n
+        \"\"\"\n
+        if self.stationary_data is None:\n
+            if self.data is not None:\n
+                self.make_stationary(verbose=False)\n
+            else:\n
+                raise ValueError(\"No data available. Please set data first.\")\n
+                \n
+        if lag is None:\n
+            if self.optimal_lag is None:\n
+                self.select_lag_order(verbose=False)\n
+            lag = self.optimal_lag\n
+            \n
+        model = VAR(self.stationary_data)\n
+        self.var_model = model.fit(lag)\n
+        \n
+        if verbose:\n
+            print(f\"\\nVAR Analysis Summary (lag order = {lag}):\")\n
+            print(\"----------------------------------------\")\n
+            print(f\"Number of observations: {self.var_model.nobs}\")\n
+            print(f\"Log likelihood: {self.var_model.llf:.4f}\")\n
+            print(f\"AIC: {self.var_model.aic:.4f}\")\n
+            print(f\"BIC: {self.var_model.bic:.4f}\")\n
+            #print(f\"Durbin-Watson: {self.var_model.durbin_watson()}\")\n
+            \n
+        return self.var_model\n
+    \n
+    def select_promising_combinations(self, threshold=0.05):\n
+        \"\"\"\n
+        Select promising variable combinations based on VAR results.\n
+        \n
+        Parameters:\n
+        -----------\n
+        threshold : float, optional\n
+            P-value threshold for significance\n
+            \n
+        Returns:\n
+        --------\n
+        list : List of promising (cause, effect) combinations\n
+        \"\"\"\n
+        if self.var_model is None:\n
+            self.perform_var_analysis(verbose=False)\n
+            \n
+        promising_combinations = []\n
+        variable_names = self.stationary_data.columns\n
+        \n
+        # Analyze coefficients from VAR model\n
+        for i, effect in enumerate(variable_names):\n
+            for j, cause in enumerate(variable_names):\n
+                if i == j:  # Skip self-causality\n
+                    continue\n
+                    \n
+                # Check if coefficients of cause are significant in effect equation\n
+                # Get positions for the cause variable in each lag\n
+                cause_indices = range(j, len(variable_names) * self.var_model.k_ar, len(variable_names))\n
+                \n
+                # Extract coefficients and p-values for these positions in the effect equation\n
+                coeffs = [self.var_model.params.iloc[idx, i] for idx in cause_indices]\n
+                p_values = [self.var_model.pvalues.iloc[idx, i] for idx in cause_indices]\n
+                \n
+                # If any coefficient is significant, add to promising combinations\n
+                if any(p < threshold for p in p_values):\n
+                    promising_combinations.append((cause, effect))\n
+        \n
+        return promising_combinations\n
+    \n
+    def test_granger_causality(self, variables=None, max_lag=None, verbose=True):\n
+        \"\"\"\n
+        Perform Granger causality tests.\n
+        \n
+        Parameters:\n
+        -----------\n
+        variables : list of tuples, optional\n
+            List of (cause, effect) tuples to test\n
+        max_lag : int, optional\n
+            Maximum lag to test (uses optimal lag if None)\n
+        verbose : bool, optional\n
+            Whether to print causality test results\n
+            \n
+        Returns:\n
+        --------\n
+        dict : Granger causality test results\n
+        \"\"\"\n
+        if self.stationary_data is None:\n
+            if self.data is not None:\n
+                self.make_stationary(verbose=False)\n
+            else:\n
+                raise ValueError(\"No data available. Please set data first.\")\n
+                \n
+        if max_lag is None:\n
+            if self.optimal_lag is None:\n
+                self.select_lag_order(verbose=False)\n
+            max_lag = self.optimal_lag\n
+            \n
+        # If no specific variable pairs are provided, use promising combinations\n
+        if variables is None:\n
+            variables = self.select_promising_combinations()\n
+            \n
+        # If still None (e.g., no promising combinations), test all pairs\n
+        if not variables:\n
+            columns = list(self.stationary_data.columns)\n
+            variables = [(x, y) for x in columns for y in columns if x != y]\n
+            \n
+        # Perform Granger causality tests\n
+        results = {}\n
+        for cause, effect in variables:\n
+            # Test if cause Granger-causes effect\n
+            data = self.stationary_data[[effect, cause]]  # [y, x] tests if x Granger-causes y\n
+            gc_result = grangercausalitytests(data, maxlag=max_lag, verbose=False)\n
+            \n
+            # Extract p-values for each lag (using F-test)\n
+            p_values = {lag: round(result[0]['ssr_ftest'][1], 4) for lag, result in gc_result.items()}\n
+            \n
+            # Determine if there's causality at any lag\n
+            causality = any(p < 0.05 for p in p_values.values())\n
+            min_p = min(p_values.values())\n
+            significant_lags = [lag for lag, p in p_values.items() if p < 0.05]\n
+            \n
+            # Store results\n
+            results[(cause, effect)] = {\n
+                'p_values': p_values,\n
+                'causality': causality,\n
+                'min_p_value': min_p,\n
+                'significant_lags': significant_lags\n
+            }\n
+            \n
+            if verbose:\n
+                status = \"Granger causes\" if causality else \"does NOT Granger cause\"\n
+                print(f\"{cause} {status} {effect} (min p-value: {min_p:.4f}, \"\n
+                      f\"significant lags: {significant_lags if significant_lags else 'none'})\")\n
+        \n
+        self.causality_results = results\n
+        return results\n
+    \n
+    def full_analysis(self, max_lag=10, max_diff=2, criterion='BIC'):\n
+        \"\"\"\n
+        Perform complete Granger causality analysis pipeline.\n
+        \n
+        Parameters:\n
+        -----------\n
+        max_lag : int, optional\n
+            Maximum lag order to consider\n
+        max_diff : int, optional\n
+            Maximum differencing order\n
+            \n
+        Returns:\n
+        --------\n
+        dict : Analysis results\n
+        \"\"\"\n
+        if self.data is None:\n
+            raise ValueError(\"No data available. Please set data first.\")\n
+            \n
+        print(\"====== GRANGER CAUSALITY ANALYSIS ======\\n\")\n
+        \n
+        # Step 1: Check stationarity\n
+        print(\"STEP 1: Checking stationarity of time series\")\n
+        print(\"-------------------------------------------\")\n
+        stationarity_results = self.check_stationarity()\n
+        print()\n
+        \n
+        # Step 2: Make data stationary if needed\n
+        print(\"STEP 2: Making time series stationary\")\n
+        print(\"------------------------------------\")\n
+        if any(not result['Stationary'] for result in stationarity_results.values()):\n
+            self.make_stationary(max_diff=max_diff)\n
+        else:\n
+            self.stationary_data = self.data.copy()\n
+            print(\"All series are already stationary. No differencing applied.\")\n
+        print()\n
+        \n
+        # Step 3: Select optimal lag order\n
+        print(\"STEP 3: Selecting optimal lag order\")\n
+        print(\"---------------------------------\")\n
+        lag_criteria = self.select_lag_order(max_lag=max_lag, criterion=criterion)\n
+        print()\n
+        \n
+        # Step 4: Perform VAR analysis\n
+        print(\"STEP 4: Performing VAR analysis\")\n
+        print(\"-----------------------------\")\n
+        var_model = self.perform_var_analysis()\n
+        print()\n
+        \n
+        # Step 5: Test Granger causality\n
+        print(\"STEP 5: Testing Granger causality\")\n
+        print(\"-------------------------------\")\n
+        promising = self.select_promising_combinations()\n
+        \n
+        if promising:\n
+            print(f\"Found {len(promising)} promising variable combinations:\")\n
+            for cause, effect in promising:\n
+                print(f\"  {cause} → {effect}\")\n
+            print(\"\\nGranger causality test results:\")\n
+            print(\"-----------------------------\")\n
+        else:\n
+            print(\"No promising combinations found. Testing all variable pairs.\")\n
+            \n
+        causality_results = self.test_granger_causality(promising)\n
+        \n
+        # Summarize results\n
+        print(\"\\nSUMMARY:\")\n
+        print(\"--------\")\n
+        causal_pairs = [(cause, effect) for (cause, effect), result in self.causality_results.items() \n
+                        if result['causality']]\n
+        \n
+        if causal_pairs:\n
+            print(\"Detected Granger causal relationships:\")\n
+            for cause, effect in causal_pairs:\n
+                print(f\"  {cause} → {effect}\")\n
+        else:\n
+            print(\"No significant Granger causal relationships detected.\")\n
+            \n
+        return {\n
+            'stationarity': stationarity_results,\n
+            'differencing': self.diff_order,\n
+            'optimal_lag': self.optimal_lag,\n
+            'var_model': self.var_model,\n
+            'causality_results': self.causality_results\n
+        }\n
+\n
+\n
+if __name__ == \"__main__\":\n
+    from generate_variables import load_variables_sinusoidales\n
+    t,x,y,z = load_variables_sinusoidales(n=100,T=2*np.pi,lag=np.pi/3,noise=True,seed=42,cos=True)\n
+\n
+    # Create a DataFrame with your time series\n
+    df = pd.DataFrame({\n
+        'x': x,\n
+        'y': y,\n
+        'z': z\n
+    }, index=t)\n
+\n
+    # Initialize the Granger causality analysis with your data\n
+    gc = GrangerCausalityAnalysis(df)\n
+\n
+    # Run the full analysis pipeline\n
+    results = gc.full_analysis()\n
+\n
+    # Or run individual steps as needed:\n
+    # gc.check_stationarity()\n
+    # gc.make_stationary()\n
+    # gc.select_lag_order()\n
+    # gc.perform_var_analysis()\n
+    # promising_combinations = gc.select_promising_combinations()\n
+    # gc.test_granger_causality(promising_combinations)\n
+\n
+    causal_relationships = [(cause, effect) for (cause, effect), result \n
+                        in results['causality_results'].items() \n
+                        if result['causality']]\n
\\ No newline at end of file\n"
]
