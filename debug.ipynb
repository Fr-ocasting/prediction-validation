{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Points à vérifier dans mon code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver un moyen d'uniformiser 'time_step_per_hour' pour toute les données, genre dénominateur commun etc.\n",
    "dates_verif_obj = DatesVerifFeatureVect(dataset.df_dates, Weeks = args.W, Days = args.D, historical_len = args.H, step_ahead = args.step_ahead, time_step_per_hour = dataset.time_step_per_hour)\n",
    "dataset.get_feature_vect(invalid_dates)\n",
    "# ========== .... ==========\n",
    "\n",
    "# Passage dans split Tensor: \n",
    "# >>>>  Consiste à Normalizer un input (Ici 'U') selon une certaine référence (ici train_input)\n",
    "# >>>>  Puis de renovyer les U_train/U_valid/U_test \n",
    "# >>>>\n",
    "# >>>>   Pour se faire, ils calculent d'abord les stats (min,max,mean,std) sur le tensor de référence train_input\n",
    "# >>>>   Puis, à chacun des split U_Train/U_Valid/U_Test, normalisation selon les statistiques.\n",
    "\n",
    "\n",
    "# ==== ATTENTION il faudra sans doute récupérer les stats simplement et pas normalizer la ref\n",
    "# ==== Calcul inutile ici \n",
    "\n",
    "\n",
    "# ========= PERSONAL_INPUT =========\n",
    "# >>>> \n",
    "# >>>> Il faut absolument pouvoir prendre en compte les invalid dates mixée de tout le monde\n",
    "# >>>> Probablement le faire en amont lors qu'on load les dataset, on load les invalid dates associées\n",
    "# >>>> Puis on fait l'union des invalid dates\n",
    "# >>>> Et après ça on train/valid/test split normalize tout.\n",
    "# >>>> Idem pour le coverage pour que ça corresponde à tout le monde.  \n",
    "# >>>> \n",
    "# >>>> Génère des input de taille différentes selon l'historique demandé. \n",
    "# >>>> Cela ne va pas convenir si jamais je travail avec du calendar class ou autre.\n",
    "# >>>> Il faut pouvoir ajouter une variable 'indices_train' etc qui prend en compte les indices \n",
    "# >>>> enregistrée au préalable (sur par exemple une autre dataset)\n",
    "# >>>>\n",
    "# >>>> Ou alors me baser sur la target:  le df_verif[-1]\n",
    "# ========= ............. ========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Test pour voir si tout le pre-processing fonctionne: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DL_class import InvalidDatesCleaner,DatesVerifFeatureVect\n",
    "from dataset import TrainValidTest_Split_Normalize,PersonnalInput\n",
    "from calendar_class import get_time_slots_labels\n",
    "import os\n",
    "import pandas as pd \n",
    "import torch\n",
    "import pickle \n",
    "import numpy as np \n",
    "\n",
    "from paths import folder_path,file_name\n",
    "from config import get_args\n",
    "from utilities_DL import get_DataSet_and_invalid_dates, match_period_coverage_with_netmob\n",
    "\n",
    "\n",
    "# ==================== Load Subway-in Dataset : ====================\n",
    "# Load config\n",
    "model_name = 'STGCN' #'CNN'\n",
    "netmob = True\n",
    "args = get_args(model_name)\n",
    "#args = get_args(model_name = model_name,learn_graph_structure = True)  # MTGNN\n",
    "\n",
    "# Modification : \n",
    "args.K_fold = 1\n",
    "args.ray = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-step per hour: 4.0\n",
      "coverage period: 2019-03-16 00:00:00 - 2019-03-17 00:45:00\n",
      "Load des données NetMob .pt impossible. Création d'un random Tensor\n",
      "Init NetMob Dataset:  torch.Size([100, 40, 2, 22, 22])\n",
      "Number of Nan Value:  tensor(0)\n",
      "Total Number of Elements:  3872000 \n",
      "\n",
      "Tackling Subway Data:\n",
      "Tackling reference for normalization\n",
      "\n",
      " Tackling Training Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      " Tackling Validation Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      " Tackling Testing Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "Tackling reference for normalization\n",
      "\n",
      " Tackling Training Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      " Tackling Validation Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      " Tackling Testing Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      "U size:  torch.Size([94, 40, 6]) Utarget size:  torch.Size([94, 40, 1])\n",
      "U_train size:  torch.Size([48, 40, 6]) Utarget_train size:  torch.Size([48, 40, 1])\n",
      "U_valid size:  torch.Size([16, 40, 6]) Utarget_valid size:  torch.Size([16, 40, 1])\n",
      "U_test size:  torch.Size([17, 40, 6]) Utarget_test size:  torch.Size([17, 40, 1])\n",
      "U_train min:  tensor(0., dtype=torch.float64) U_train max:  tensor(1., dtype=torch.float64)\n",
      "U_valid min:  tensor(0.0447, dtype=torch.float64) U_valid max:  tensor(4.1455, dtype=torch.float64)\n",
      "U_test min:  tensor(0., dtype=torch.float64) U_test max:  tensor(4.1892, dtype=torch.float64)\n",
      "\n",
      "No Contextual Data has been considered\n",
      "\n",
      "Tackling NetMob Data: \n",
      "Tackling reference for normalization\n",
      "\n",
      " Tackling Training Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      " Tackling Validation Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      " Tackling Testing Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "Tackling reference for normalization\n",
      "\n",
      " Tackling Training Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      " Tackling Validation Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      " Tackling Testing Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      "U size:  torch.Size([94, 40, 2, 22, 22, 6]) Utarget size:  torch.Size([94, 40, 2, 22, 22, 1])\n",
      "U_train size:  torch.Size([48, 40, 2, 22, 22, 6]) Utarget_train size:  torch.Size([48, 40, 2, 22, 22, 1])\n",
      "U_valid size:  torch.Size([16, 40, 2, 22, 22, 6]) Utarget_valid size:  torch.Size([16, 40, 2, 22, 22, 1])\n",
      "U_test size:  torch.Size([17, 40, 2, 22, 22, 6]) Utarget_test size:  torch.Size([17, 40, 2, 22, 22, 1])\n",
      "U_train min:  tensor(0.) U_train max:  tensor(1.)\n",
      "U_valid min:  tensor(-0.0670) U_valid max:  tensor(1.0675)\n",
      "U_test min:  tensor(-0.0902) U_test max:  tensor(1.1110)\n",
      "\n",
      "No Contextual Data has been considered\n",
      "\n",
      "Tackling Calendar Data\n",
      "\n",
      " Tackling Training Set\n"
     ]
    }
   ],
   "source": [
    "# Load Init DataSet\n",
    "coverage = match_period_coverage_with_netmob(file_name)\n",
    "if not torch.cuda.is_available(): \n",
    "    coverage = coverage[:100]\n",
    "    args.W = 0\n",
    "    args.D = 0\n",
    "\n",
    "\n",
    "dataset,invalid_dates = get_DataSet_and_invalid_dates(args.abs_path, folder_path,file_name,\n",
    "                                                      args.W,args.D,args.H,args.step_ahead,\n",
    "                                                      single_station = False,coverage_period = coverage)\n",
    "# ==================== ........................ ====================\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    data_folder_path = '../../../data/' \n",
    "else:\n",
    "    data_folder_path = '../../Data/'\n",
    "\n",
    "\n",
    "# === INIT ====\n",
    "save_folder = f\"{data_folder_path}NetMob_tensor/\"\n",
    "netmob_data_folder_path = f\"{data_folder_path}NetMob/\"\n",
    "step_south_north = 287  # Incremente by 287-ids when passing from south to north. \n",
    "epsilon=1000  #epsilon : radius, in meter (1000m) \n",
    "# W,H = 2*(epsilon//100 + 1), 2*(epsilon//100 + 1)\n",
    "\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "# === .... ===\n",
    "\n",
    "\n",
    "# ===== Load NetMob Data: =====\n",
    "# NetMob Tensor : [T,N,C,H,W]\n",
    "# dims : [0,-2,-1]  -> dimension for which we want to retrieve stats \n",
    "try :\n",
    "    netmob_T = torch.stack([torch.load(f\"{save_folder}station_{station}.pt\") for station in ref_subway.COD_TRG])\n",
    "    netmob_T = netmob_T.permute(1,0,*range(2, netmob_T.dim()))\n",
    "\n",
    "except:\n",
    "    netmob_T = torch.randn(dataset.length,40,2,22,22)\n",
    "    print(\"Load des données NetMob .pt impossible. Création d'un random Tensor\")\n",
    "\n",
    "print('Init NetMob Dataset: ', netmob_T.size())\n",
    "print('Number of Nan Value: ',torch.isnan(netmob_T).sum())\n",
    "print('Total Number of Elements: ', netmob_T.numel(),'\\n')\n",
    "# ===== ....... =====\n",
    "\n",
    "\n",
    "print('Tackling Subway Data:')\n",
    "subway_ds = PersonnalInput(invalid_dates, tensor = dataset.raw_values, dates = dataset.df_dates,\n",
    "                           time_step_per_hour = dataset.time_step_per_hour,Weeks = args.W, Days = args.D, historical_len = args.H,step_ahead = args.step_ahead)\n",
    "subway_ds.preprocess(args.train_prop,args.valid_prop,args.test_prop,dims_agg=[0])\n",
    "\n",
    "print('\\nTackling NetMob Data: ')\n",
    "NetMob_ds = PersonnalInput(invalid_dates, tensor = netmob_T, dates = dataset.df_dates,\n",
    "                           time_step_per_hour = dataset.time_step_per_hour,Weeks = args.W, Days = args.D, historical_len = args.H,step_ahead = args.step_ahead)\n",
    "NetMob_ds.preprocess(args.train_prop,args.valid_prop,args.test_prop,dims_agg=[0,3,4])\n",
    "\n",
    "\n",
    "print('\\nTackling Calendar Data')\n",
    "time_slots_labels,dic_class2rpz,dic_rpz2class,nb_words_embedding = get_time_slots_labels(subway_ds)\n",
    "calendar_tensor = time_slots_labels[args.calendar_class]\n",
    "\n",
    "splitter = TrainValidTest_Split_Normalize(calendar_tensor,\n",
    "                            first_train = subway_ds.first_train_U, last_train= subway_ds.last_train_U,\n",
    "                            first_valid= subway_ds.first_valid_U, last_valid = subway_ds.last_valid_U,\n",
    "                            first_test = subway_ds.first_test_U, last_test = subway_ds.last_test_U)\n",
    "\n",
    "train_tensor_ds,valid_tensor_ds,test_tensor_ds = splitter.load_normalize_tensor_datasets(normalize = False)\n",
    "calendar_U_train,calendar_U_valid,calendar_U_test = train_tensor_ds.tensor,valid_tensor_ds.tensor,test_tensor_ds.tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DL_class import FeatureVectorBuilder,DatesVerifFeatureVect\n",
    "from train_valid_test_split import train_valid_test_split_iterative_method\n",
    "from loader import DictDataLoader\n",
    "from save_results import save_object,read_object,Dataset_get_save_folder\n",
    "from calendar_class import get_time_slots_labels\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 40, 6]) torch.Size([24, 40, 1]) torch.Size([24, 40, 2, 22, 22, 6]) torch.Size([24])\n"
     ]
    }
   ],
   "source": [
    "from loader import DictDataLoader\n",
    "# Train, Valid, Test split : \n",
    "train_tuple = subway_ds.U_train, subway_ds.Utarget_train, dict(netmob = NetMob_ds.U_train,calendar = calendar_U_train) #, calendar = calendar[train_subset]\n",
    "valid_tuple = subway_ds.U_valid, subway_ds.Utarget_valid, dict(netmob = NetMob_ds.U_valid, calendar = calendar_U_valid ) #, calendar = calendar[train_subset]\n",
    "test_tuple = subway_ds.U_test, subway_ds.Utarget_test, dict(netmob = NetMob_ds.U_test, calendar = calendar_U_test) #, calendar = calendar[train_subset]\n",
    "\n",
    "# Load DictDataLoader: \n",
    "DictDataLoader_object = DictDataLoader(train_tuple, valid_tuple, test_tuple, args)\n",
    "dict_dataloader = DictDataLoader_object.get_dictdataloader()\n",
    "\n",
    "X_subway_b, Y_subway_b, contextual_data_b = next(iter(dict_dataloader['train']))\n",
    "X_netmob_b = contextual_data_b[0]\n",
    "X_calendar_b = contextual_data_b[1]\n",
    "\n",
    "print(X_subway_b.size(),Y_subway_b.size(),X_netmob_b.size(),X_calendar_b.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essais avec NetMob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tackling Training Set\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "min(): Expected reduction dim 2 to have non-zero size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vc/gn83ctb97rd5lmlbh8djj9fr0000gr/T/ipykernel_1447/1599376891.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Split DataSet and Normalize accoding Stats from Training Set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_normalize_tensor_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;31m# Define DictDataLoader :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mload_normalize_tensor_datasets\u001b[0;34m(self, mini, maxi, mean, std, normalize, feature_vect)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminmaxnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_vect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_vect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mini'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mnormalize_tensor\u001b[0;34m(self, tensor, dims, minmaxnorm, standardize, reverse, feature_vect)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mini'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'maxi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'std'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mreshaped_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshaped_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mnormalized_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminmaxnorm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_vect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mget_stats\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;34m''' Return Min, Max, Mean and Std of inputs through the choosen dimension 'dims' (which have been flattened)'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mini'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0;31m#self.mini = inputs.min(dims).values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'maxi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: min(): Expected reduction dim 2 to have non-zero size."
     ]
    }
   ],
   "source": [
    "T = netmob_T.size(0)\n",
    "\n",
    "# Tackle a specific fold : \n",
    "netmob_T1 = netmob_T[:100]\n",
    "\n",
    "# Init :\n",
    "dims = [0,3,4]#[0,-2,-1]\n",
    "minmaxnorm = True\n",
    "standardize = False\n",
    "\n",
    "# ============ Load Train/Valid/Test Indices and removed forbidden dates : ============\n",
    "# invalid dates = \n",
    "# invalid_indices = get_indices_from_dates(invalid)\n",
    "indices = np.arange(T)\n",
    "np.random.shuffle(indices)\n",
    "invalid_indices = indices[:100]\n",
    "\n",
    "# Get Split indices :\n",
    "train_indices = np.arange(50)\n",
    "valid_indices = np.arange(60,70)\n",
    "test_indices = np.arange(80,100)\n",
    "\n",
    "# Remove invalid_dates from indices :\n",
    "cleaner = InvalidDatesCleaner(invalid_indices = invalid_indices)\n",
    "\n",
    "# >>>> Le cleaner prend un ensemble d'indice qui est interdit (les indices correspondant au dates qui sont interdites)\n",
    "# >>>> On lui passe ensemble un ensemble d'indices (ex : ceux qui correspondent à l'extraction de U_train)\n",
    "# >>>> Pour lequels il va masqué/supprimer les indices interdit\n",
    "# >>>> On ce sert ensuite de cet ensemble pour extraire le bon U_train, avec les valeurs qui sont effectivement interdite. \n",
    "\n",
    "\n",
    "train_indices = cleaner.clean_indices(train_indices)\n",
    "valid_indices = cleaner.clean_indices(valid_indices)\n",
    "test_indices = cleaner.clean_indices(test_indices)\n",
    "# ============ .......................................................... ============\n",
    "\n",
    "# Load Splitter Object\n",
    "splitter = TrainValidTest_Split_Normalize(netmob_T1,dims,train_indices, valid_indices, test_indices,minmaxnorm=minmaxnorm,standardize=standardize)\n",
    "\n",
    "# Split DataSet and Normalize accoding Stats from Training Set \n",
    "train_dataset,valid_dataset,test_dataset = splitter.load_normalize_tensor_datasets()\n",
    "# Define DictDataLoader :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      " Tackling Training Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      " Tackling Validation Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      " Tackling Testing Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      " Tackling Training Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      " Tackling Validation Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      " Tackling Testing Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      "U size:  torch.Size([5662, 40, 8]) Utarget size:  torch.Size([5662, 40, 1])\n",
      "U_train size:  torch.Size([2934, 40, 8]) Utarget_train size:  torch.Size([2934, 40, 1])\n",
      "U_valid size:  torch.Size([978, 40, 8]) Utarget_valid size:  torch.Size([978, 40, 1])\n",
      "U_test size:  torch.Size([979, 40, 8]) Utarget_test size:  torch.Size([979, 40, 1])\n",
      "U_train min:  tensor(0., dtype=torch.float64) U_train max:  tensor(1., dtype=torch.float64)\n",
      "U_valid min:  tensor(0., dtype=torch.float64) U_valid max:  tensor(1.7234, dtype=torch.float64)\n",
      "U_test min:  tensor(0., dtype=torch.float64) U_test max:  tensor(1.3909, dtype=torch.float64)\n",
      "No Contextual Data has been considered\n"
     ]
    }
   ],
   "source": [
    "# ========== Comparaison avec le df_verif sur les timestamp dates:  ==========\n",
    "time_slots_labels,dic_class2rpz,dic_rpz2class,nb_words_embedding = dataset.split_normalize_load_feature_vect(invalid_dates,args.train_prop,args.valid_prop,args.test_prop,dims_agg = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'min'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vc/gn83ctb97rd5lmlbh8djj9fr0000gr/T/ipykernel_3691/626835638.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m subway_ds = PersonnalInput(tensor = dataset.raw_values, invalid_dates = invalid_dates, dates = dataset.df_dates,\n\u001b[0;32m---> 12\u001b[0;31m                            time_step_per_hour = dataset.time_step_per_hour,Weeks = args.W, Days = args.D, historical_len = args.H,step_ahead = args.step_ahead)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0msubway_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vc/gn83ctb97rd5lmlbh8djj9fr0000gr/T/ipykernel_3691/626835638.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, df, tensor, invalid_dates, dates, time_step_per_hour, Weeks, Days, historical_len, step_ahead)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPersonnalInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataSet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvalid_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime_step_per_hour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mWeeks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistorical_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep_ahead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPersonnalInput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_step_per_hour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_step_per_hour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWeeks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWeeks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistorical_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistorical_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep_ahead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_ahead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvalid_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minvalid_dates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, df, tensor, dates, init_df, mini, maxi, mean, normalized, time_step_per_hour, train_df, cleaned_df, Weeks, Days, historical_len, step_ahead)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDay_nb_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mmini\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'min'"
     ]
    }
   ],
   "source": [
    "from dataset import DataSet\n",
    "class PersonnalInput(object):\n",
    "    def __init__(self,dataset, invalid_dates):\n",
    "        super(PersonnalInput,self).__init__()\n",
    "        self.invalid_dates = invalid_dates\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def preprocess(self,train_prop,valid_prop,test_prop,dims_agg):\n",
    "        self.dataset.split_normalize_load_feature_vect(self.invalid_dates,train_prop,valid_prop,test_prop,dims_agg)\n",
    "\n",
    "subway_ds = PersonnalInput(dataset,invalid_dates)\n",
    "subway_ds.preprocess(args.train_prop,args.valid_prop,args.test_prop,[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "T,N,C,H,W,L = 100,40,2,20,20,6\n",
    "\n",
    "# Test avec subway-in\n",
    "X = torch.randn(T,N,L)\n",
    "I = torch.randn(T,N)\n",
    "dims = [0]  # min = [N]\n",
    "S = torch.randn(N)\n",
    "# ...\n",
    "\n",
    "tensor_ds = TensorDataset(X,mini=S,maxi=S*S,mean=S,std=S, normalized = False)\n",
    "tensor_ds.normalize_tensor(dims, minmaxnorm= True)\n",
    "\n",
    "\n",
    "# Test avec NetMob Image\n",
    "X = torch.randn(T,N,C,H,W,L)\n",
    "I = torch.randn(T,N,C,H,W)\n",
    "dims = [0,3,4]  # min = [N]\n",
    "S = torch.randn(N,C)\n",
    "# ....\n",
    "\n",
    "\n",
    "tensor_ds = TensorDataset(X,mini=S,maxi=S*S,mean=S,std=S, normalized = False)\n",
    "tensor_ds.normalize_tensor(dims, minmaxnorm= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tackling Training Set\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vc/gn83ctb97rd5lmlbh8djj9fr0000gr/T/ipykernel_3785/3379316797.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_dataset_get_tensor_and_train_valid_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minvalid_dates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_prop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mclean_dataset_get_tensor_and_train_valid_test_split\u001b[0;34m(self, df, invalid_dates, train_prop, valid_prop, test_prop, normalize)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_valid_test_split_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_prop\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create df_train,df_valid,df_test, df_verif_train, df_verif_valid, df_verif_test, and dates limits for each df and each tensor U\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Récupère U_test, Utarget_test, NetMob_test, Weather_test etc....  dans 'dataset_init.contextual_tensors.items()'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36msplit_tensors\u001b[0;34m(self, normalize)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0;34m''' Split input tensors  in Train/Valid/Test part '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;31m# Get U_train, U_valid, U_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_train_valid_test_tensor_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'U'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mref_for_normalization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;31m# Get Utarget_train, Utarget_valid, Utarget_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mset_train_valid_test_tensor_attribute\u001b[0;34m(self, name, tensor, dims, ref_for_normalization, normalize)\u001b[0m\n\u001b[1;32m    601\u001b[0m                                     minmaxnorm = True,standardize = False)\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0mtrain_tensor_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_tensor_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_tensor_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_normalize_tensor_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf\"{name}_train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tensor_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf\"{name}_valid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_tensor_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mload_normalize_tensor_datasets\u001b[0;34m(self, mini, maxi, mean, std, normalize)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminmaxnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mini'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mnormalize_tensor\u001b[0;34m(self, dims, minmaxnorm, standardize, reverse)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mnormalized_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshaped_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminmaxnorm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;31m# reshape-back, inverse-permute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, inputs, minmaxnorm, standardize, reverse)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# MinMax Normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mminmaxnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mstacked_mini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshaped_inputs_dim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0mstacked_maxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshaped_inputs_dim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.float64"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tackling Training Set\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vc/gn83ctb97rd5lmlbh8djj9fr0000gr/T/ipykernel_2908/1601370515.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_normalize_load_feature_vect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minvalid_dates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_prop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_prop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_prop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36msplit_normalize_load_feature_vect\u001b[0;34m(self, args, invalid_dates, train_prop, valid_prop, test_prop)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;31m# Get all the splitted train/valid/test input tensors. Normalize Them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;31m# ================ FAIRE QULEQUE CHOSE POUR LE TIME-SLOTS LABELS. ESSAYER DE LES INTEGRER DANS LE CONTEXTUAL TENSORS  ================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36msplit_tensors\u001b[0;34m(self, normalize)\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;34m''' Split input tensors  in Train/Valid/Test part '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0;31m# Get U_train, U_valid, U_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_train_valid_test_tensor_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'U'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mref_for_normalization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;31m# Get Utarget_train, Utarget_valid, Utarget_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mset_train_valid_test_tensor_attribute\u001b[0;34m(self, name, tensor, dims, ref_for_normalization, normalize)\u001b[0m\n\u001b[1;32m    636\u001b[0m                                     minmaxnorm = True,standardize = False)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mtrain_tensor_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_tensor_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_tensor_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_normalize_tensor_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf\"{name}_train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tensor_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf\"{name}_valid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_tensor_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mload_normalize_tensor_datasets\u001b[0;34m(self, mini, maxi, mean, std, normalize)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminmaxnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mini'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mnormalize_tensor\u001b[0;34m(self, dims, minmaxnorm, standardize, reverse)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mnormalized_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshaped_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminmaxnorm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;31m# reshape-back, inverse-permute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, inputs, minmaxnorm, standardize, reverse)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# MinMax Normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mminmaxnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mstacked_mini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshaped_inputs_dim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0mstacked_maxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshaped_inputs_dim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.float64"
     ]
    }
   ],
   "source": [
    "dataset.split_normalize_load_feature_vect(args,invalid_dates,args.train_prop, args.valid_prop, args.test_prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pour le cas des NetMob Data : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-03-16 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-03-16 00:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-03-16 00:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-03-16 00:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-03-16 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>2019-05-31 22:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7388</th>\n",
       "      <td>2019-05-31 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7389</th>\n",
       "      <td>2019-05-31 23:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7390</th>\n",
       "      <td>2019-05-31 23:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7391</th>\n",
       "      <td>2019-05-31 23:45:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7392 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date\n",
       "0    2019-03-16 00:00:00\n",
       "1    2019-03-16 00:15:00\n",
       "2    2019-03-16 00:30:00\n",
       "3    2019-03-16 00:45:00\n",
       "4    2019-03-16 01:00:00\n",
       "...                  ...\n",
       "7387 2019-05-31 22:45:00\n",
       "7388 2019-05-31 23:00:00\n",
       "7389 2019-05-31 23:15:00\n",
       "7390 2019-05-31 23:30:00\n",
       "7391 2019-05-31 23:45:00\n",
       "\n",
       "[7392 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t-672</th>\n",
       "      <th>t-96</th>\n",
       "      <th>t-6</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t+0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>2019-03-16 00:00:00</td>\n",
       "      <td>2019-03-22 00:00:00</td>\n",
       "      <td>2019-03-22 22:30:00</td>\n",
       "      <td>2019-03-22 22:45:00</td>\n",
       "      <td>2019-03-22 23:00:00</td>\n",
       "      <td>2019-03-22 23:15:00</td>\n",
       "      <td>2019-03-22 23:30:00</td>\n",
       "      <td>2019-03-22 23:45:00</td>\n",
       "      <td>2019-03-23 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>2019-03-16 00:15:00</td>\n",
       "      <td>2019-03-22 00:15:00</td>\n",
       "      <td>2019-03-22 22:45:00</td>\n",
       "      <td>2019-03-22 23:00:00</td>\n",
       "      <td>2019-03-22 23:15:00</td>\n",
       "      <td>2019-03-22 23:30:00</td>\n",
       "      <td>2019-03-22 23:45:00</td>\n",
       "      <td>2019-03-23 00:00:00</td>\n",
       "      <td>2019-03-23 00:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>2019-03-16 00:30:00</td>\n",
       "      <td>2019-03-22 00:30:00</td>\n",
       "      <td>2019-03-22 23:00:00</td>\n",
       "      <td>2019-03-22 23:15:00</td>\n",
       "      <td>2019-03-22 23:30:00</td>\n",
       "      <td>2019-03-22 23:45:00</td>\n",
       "      <td>2019-03-23 00:00:00</td>\n",
       "      <td>2019-03-23 00:15:00</td>\n",
       "      <td>2019-03-23 00:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>2019-03-16 00:45:00</td>\n",
       "      <td>2019-03-22 00:45:00</td>\n",
       "      <td>2019-03-22 23:15:00</td>\n",
       "      <td>2019-03-22 23:30:00</td>\n",
       "      <td>2019-03-22 23:45:00</td>\n",
       "      <td>2019-03-23 00:00:00</td>\n",
       "      <td>2019-03-23 00:15:00</td>\n",
       "      <td>2019-03-23 00:30:00</td>\n",
       "      <td>2019-03-23 00:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>2019-03-16 01:00:00</td>\n",
       "      <td>2019-03-22 01:00:00</td>\n",
       "      <td>2019-03-22 23:30:00</td>\n",
       "      <td>2019-03-22 23:45:00</td>\n",
       "      <td>2019-03-23 00:00:00</td>\n",
       "      <td>2019-03-23 00:15:00</td>\n",
       "      <td>2019-03-23 00:30:00</td>\n",
       "      <td>2019-03-23 00:45:00</td>\n",
       "      <td>2019-03-23 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>2019-05-24 22:45:00</td>\n",
       "      <td>2019-05-30 22:45:00</td>\n",
       "      <td>2019-05-31 21:15:00</td>\n",
       "      <td>2019-05-31 21:30:00</td>\n",
       "      <td>2019-05-31 21:45:00</td>\n",
       "      <td>2019-05-31 22:00:00</td>\n",
       "      <td>2019-05-31 22:15:00</td>\n",
       "      <td>2019-05-31 22:30:00</td>\n",
       "      <td>2019-05-31 22:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7388</th>\n",
       "      <td>2019-05-24 23:00:00</td>\n",
       "      <td>2019-05-30 23:00:00</td>\n",
       "      <td>2019-05-31 21:30:00</td>\n",
       "      <td>2019-05-31 21:45:00</td>\n",
       "      <td>2019-05-31 22:00:00</td>\n",
       "      <td>2019-05-31 22:15:00</td>\n",
       "      <td>2019-05-31 22:30:00</td>\n",
       "      <td>2019-05-31 22:45:00</td>\n",
       "      <td>2019-05-31 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7389</th>\n",
       "      <td>2019-05-24 23:15:00</td>\n",
       "      <td>2019-05-30 23:15:00</td>\n",
       "      <td>2019-05-31 21:45:00</td>\n",
       "      <td>2019-05-31 22:00:00</td>\n",
       "      <td>2019-05-31 22:15:00</td>\n",
       "      <td>2019-05-31 22:30:00</td>\n",
       "      <td>2019-05-31 22:45:00</td>\n",
       "      <td>2019-05-31 23:00:00</td>\n",
       "      <td>2019-05-31 23:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7390</th>\n",
       "      <td>2019-05-24 23:30:00</td>\n",
       "      <td>2019-05-30 23:30:00</td>\n",
       "      <td>2019-05-31 22:00:00</td>\n",
       "      <td>2019-05-31 22:15:00</td>\n",
       "      <td>2019-05-31 22:30:00</td>\n",
       "      <td>2019-05-31 22:45:00</td>\n",
       "      <td>2019-05-31 23:00:00</td>\n",
       "      <td>2019-05-31 23:15:00</td>\n",
       "      <td>2019-05-31 23:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7391</th>\n",
       "      <td>2019-05-24 23:45:00</td>\n",
       "      <td>2019-05-30 23:45:00</td>\n",
       "      <td>2019-05-31 22:15:00</td>\n",
       "      <td>2019-05-31 22:30:00</td>\n",
       "      <td>2019-05-31 22:45:00</td>\n",
       "      <td>2019-05-31 23:00:00</td>\n",
       "      <td>2019-05-31 23:15:00</td>\n",
       "      <td>2019-05-31 23:30:00</td>\n",
       "      <td>2019-05-31 23:45:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5662 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   t-672                t-96                 t-6  \\\n",
       "672  2019-03-16 00:00:00 2019-03-22 00:00:00 2019-03-22 22:30:00   \n",
       "673  2019-03-16 00:15:00 2019-03-22 00:15:00 2019-03-22 22:45:00   \n",
       "674  2019-03-16 00:30:00 2019-03-22 00:30:00 2019-03-22 23:00:00   \n",
       "675  2019-03-16 00:45:00 2019-03-22 00:45:00 2019-03-22 23:15:00   \n",
       "676  2019-03-16 01:00:00 2019-03-22 01:00:00 2019-03-22 23:30:00   \n",
       "...                  ...                 ...                 ...   \n",
       "7387 2019-05-24 22:45:00 2019-05-30 22:45:00 2019-05-31 21:15:00   \n",
       "7388 2019-05-24 23:00:00 2019-05-30 23:00:00 2019-05-31 21:30:00   \n",
       "7389 2019-05-24 23:15:00 2019-05-30 23:15:00 2019-05-31 21:45:00   \n",
       "7390 2019-05-24 23:30:00 2019-05-30 23:30:00 2019-05-31 22:00:00   \n",
       "7391 2019-05-24 23:45:00 2019-05-30 23:45:00 2019-05-31 22:15:00   \n",
       "\n",
       "                     t-5                 t-4                 t-3  \\\n",
       "672  2019-03-22 22:45:00 2019-03-22 23:00:00 2019-03-22 23:15:00   \n",
       "673  2019-03-22 23:00:00 2019-03-22 23:15:00 2019-03-22 23:30:00   \n",
       "674  2019-03-22 23:15:00 2019-03-22 23:30:00 2019-03-22 23:45:00   \n",
       "675  2019-03-22 23:30:00 2019-03-22 23:45:00 2019-03-23 00:00:00   \n",
       "676  2019-03-22 23:45:00 2019-03-23 00:00:00 2019-03-23 00:15:00   \n",
       "...                  ...                 ...                 ...   \n",
       "7387 2019-05-31 21:30:00 2019-05-31 21:45:00 2019-05-31 22:00:00   \n",
       "7388 2019-05-31 21:45:00 2019-05-31 22:00:00 2019-05-31 22:15:00   \n",
       "7389 2019-05-31 22:00:00 2019-05-31 22:15:00 2019-05-31 22:30:00   \n",
       "7390 2019-05-31 22:15:00 2019-05-31 22:30:00 2019-05-31 22:45:00   \n",
       "7391 2019-05-31 22:30:00 2019-05-31 22:45:00 2019-05-31 23:00:00   \n",
       "\n",
       "                     t-2                 t-1                 t+0  \n",
       "672  2019-03-22 23:30:00 2019-03-22 23:45:00 2019-03-23 00:00:00  \n",
       "673  2019-03-22 23:45:00 2019-03-23 00:00:00 2019-03-23 00:15:00  \n",
       "674  2019-03-23 00:00:00 2019-03-23 00:15:00 2019-03-23 00:30:00  \n",
       "675  2019-03-23 00:15:00 2019-03-23 00:30:00 2019-03-23 00:45:00  \n",
       "676  2019-03-23 00:30:00 2019-03-23 00:45:00 2019-03-23 01:00:00  \n",
       "...                  ...                 ...                 ...  \n",
       "7387 2019-05-31 22:15:00 2019-05-31 22:30:00 2019-05-31 22:45:00  \n",
       "7388 2019-05-31 22:30:00 2019-05-31 22:45:00 2019-05-31 23:00:00  \n",
       "7389 2019-05-31 22:45:00 2019-05-31 23:00:00 2019-05-31 23:15:00  \n",
       "7390 2019-05-31 23:00:00 2019-05-31 23:15:00 2019-05-31 23:30:00  \n",
       "7391 2019-05-31 23:15:00 2019-05-31 23:30:00 2019-05-31 23:45:00  \n",
       "\n",
       "[5662 rows x 9 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df_verif"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "preprocessingclone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
