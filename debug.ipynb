{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-step per hour: 4.0\n",
      "coverage period: 2019-01-01 00:00:00 - 2020-01-01 00:00:00\n",
      "Time-step per hour: 4.0\n",
      "coverage period: 2019-03-16 00:00:00 - 2019-05-31 23:45:00\n",
      "Load impossible. Création d'un random Tensor\n",
      "Init NetMob Dataset:  torch.Size([7300, 40, 2, 22, 22])\n",
      "Number of Nan Value:  tensor(0)\n",
      "Total Number of Elements:  282656000\n",
      "Tackling Training Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "Tackling Validation Set\n",
      "Tackling Testing Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from DL_class import InvalidDatesCleaner,DatesVerifFeatureVect\n",
    "from dataset import TrainValidTest_Split_Normalize\n",
    "\n",
    "import os\n",
    "import pandas as pd \n",
    "import torch\n",
    "import pickle \n",
    "import numpy as np \n",
    "\n",
    "from paths import folder_path,file_name\n",
    "from config import get_args\n",
    "from utilities_DL import get_DataSet_and_invalid_dates, match_period_coverage_with_netmob\n",
    "\n",
    "\n",
    "# ==================== Load Subway-in Dataset : ====================\n",
    "# Load config\n",
    "model_name = 'STGCN' #'CNN'\n",
    "netmob = True\n",
    "args = get_args(model_name)\n",
    "#args = get_args(model_name = model_name,learn_graph_structure = True)  # MTGNN\n",
    "\n",
    "# Modification : \n",
    "args.K_fold = 1\n",
    "args.ray = False\n",
    "\n",
    "# Load Init DataSet \n",
    "dataset,invalid_dates = get_DataSet_and_invalid_dates(args.abs_path, folder_path,file_name,\n",
    "                                                      args.W,args.D,args.H,args.step_ahead,\n",
    "                                                      single_station = False,coverage_period = None)\n",
    "\n",
    "coverage = match_period_coverage_with_netmob(dataset)\n",
    "\n",
    "dataset,invalid_dates = get_DataSet_and_invalid_dates(args.abs_path, folder_path,file_name,\n",
    "                                                      args.W,args.D,args.H,args.step_ahead,\n",
    "                                                      single_station = False,coverage_period = coverage)\n",
    "\n",
    "# ==================== ........................ ====================\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    data_folder_path = '../../../data/' \n",
    "else:\n",
    "    data_folder_path = '../../Data/'\n",
    "\n",
    "\n",
    "# === INIT ====\n",
    "save_folder = f\"{data_folder_path}NetMob_tensor/\"\n",
    "netmob_data_folder_path = f\"{data_folder_path}NetMob/\"\n",
    "step_south_north = 287  # Incremente by 287-ids when passing from south to north. \n",
    "epsilon=1000  #epsilon : radius, in meter (1000m) \n",
    "# W,H = 2*(epsilon//100 + 1), 2*(epsilon//100 + 1)\n",
    "\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "# === .... ===\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===== Load NetMob Data: =====\n",
    "# NetMob Tensor : [T,N,C,H,W]\n",
    "# dims : [0,-2,-1]  -> dimension for which we want to retrieve stats \n",
    "try :\n",
    "    netmob_T = torch.stack([torch.load(f\"{save_folder}station_{station}.pt\") for station in ref_subway.COD_TRG])\n",
    "    netmob_T = netmob_T.permute(1,0,*range(2, netmob_T.dim()))\n",
    "\n",
    "except:\n",
    "    netmob_T = torch.randn(7300,40,2,22,22)\n",
    "    print(\"Load impossible. Création d'un random Tensor\")\n",
    "\n",
    "print('Init NetMob Dataset: ', netmob_T.size())\n",
    "print('Number of Nan Value: ',torch.isnan(netmob_T).sum())\n",
    "print('Total Number of Elements: ', netmob_T.numel() )\n",
    "# ===== ....... =====\n",
    "T = netmob_T.size(0)\n",
    "\n",
    "# Tackle a specific fold : \n",
    "netmob_T1 = netmob_T[:100]\n",
    "\n",
    "# Init :\n",
    "dims = [0,-2,-1]\n",
    "minmaxnorm = True\n",
    "standardize = False\n",
    "\n",
    "# ============ Load Train/Valid/Test Indices and removed forbidden dates : ============\n",
    "# invalid dates = \n",
    "# invalid_indices = get_indices_from_dates(invalid)\n",
    "indices = np.arange(T)\n",
    "np.random.shuffle(indices)\n",
    "invalid_indices = indices[:100]\n",
    "\n",
    "# Get Split indices :\n",
    "train_indices = np.arange(50)\n",
    "valid_indices = np.arange(60,70)\n",
    "test_indices = np.arange(80,100)\n",
    "\n",
    "# Remove invalid_dates from indices :\n",
    "cleaner = InvalidDatesCleaner(invalid_indices = invalid_indices)\n",
    "\n",
    "train_indices = cleaner.clean_indices(train_indices)\n",
    "valid_indices = cleaner.clean_indices(valid_indices)\n",
    "test_indices = cleaner.clean_indices(test_indices)\n",
    "# ============ .......................................................... ============\n",
    "\n",
    "# Load Splitter Object\n",
    "splitter = TrainValidTest_Split_Normalize(netmob_T1,dims,train_indices, valid_indices, test_indices,minmaxnorm=minmaxnorm,standardize=standardize)\n",
    "\n",
    "\n",
    "# Split DataSet and Normalize accoding Stats from Training Set \n",
    "train_dataset,valid_dataset,test_dataset = splitter.load_normalize_tensor_datasets()\n",
    "\n",
    "# Define DictDataLoader :\n",
    "\n",
    "\n",
    "\n",
    "# ========== Comparaison avec le df_verif sur les timestamp dates:  ==========\n",
    "# Trouver un moyen d'uniformiser 'time_step_per_hour' pour toute les données, genre dénominateur commun etc.\n",
    "dates_verif_obj = DatesVerifFeatureVect(dataset.df_dates, Weeks = args.W, Days = args.D, historical_len = args.H, step_ahead = args.step_ahead, time_step_per_hour = dataset.time_step_per_hour)\n",
    "dataset.get_feature_vect(invalid_dates)\n",
    "# ========== .... =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "clean_dataset_get_tensor_and_train_valid_test_split() missing 1 required positional argument: 'tuple_agg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vc/gn83ctb97rd5lmlbh8djj9fr0000gr/T/ipykernel_5013/703466621.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_no_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_dataset_get_tensor_and_train_valid_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minvalid_dates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_prop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: clean_dataset_get_tensor_and_train_valid_test_split() missing 1 required positional argument: 'tuple_agg'"
     ]
    }
   ],
   "source": [
    "dataset_no_norm = dataset.clean_dataset_get_tensor_and_train_valid_test_split(dataset.df,invalid_dates,args.train_prop,args.valid_prop,args.test_prop, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subway-in: torch.Size([100, 40, 6])\n",
      "NetMob1: torch.Size([100, 40, 2, 20, 20])\n",
      "NetMob2: torch.Size([100, 40, 2, 20, 20])\n",
      "NetMob3: torch.Size([100, 40, 2, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def repeat_mini(X,S, dims):\n",
    "    '''\n",
    "    args\n",
    "    ----\n",
    "    X : feature vector              >>>> torch.randn(T,N,C,H,W,L)\n",
    "    I : Train input                 >>>> torch.randn(T,N,C,H,W)\n",
    "    S : statistics (mini,mean...)   >>>> torch.randn(N,H)\n",
    "    dims : dimension for which we have aggregated >>>> [0,2,4] \n",
    "    '''\n",
    "    reshaped_vector, repeat_vector = [1]*X.dim(),[1]*X.dim()\n",
    "    conj_dims = [x for x in np.arange(feature_vect.dim()-1) if not x in dims]\n",
    "    #print('reshaped_vector dimension: ',len(reshaped_vector))\n",
    "    #print('dims : ',dims)\n",
    "    for k,c in enumerate(conj_dims):\n",
    "        #print('k: ',k)\n",
    "        #print('c: ',c)\n",
    "        reshaped_vector[c] = S.size(k)\n",
    "    for k,c in enumerate(X.size()):\n",
    "        if reshaped_vector[k] == 1:\n",
    "            repeat_vector[k] = c\n",
    "\n",
    "    #print('S.size: ',S.size())\n",
    "    #print('reshaped_vector: ',reshaped_vector)\n",
    "    S = S.reshape(tuple(reshaped_vector))\n",
    "    S = S.repeat(tuple(repeat_vector))\n",
    "    return(S)\n",
    "\n",
    "\n",
    "T,N,C,H,W,L = 100,40,2,20,20,6\n",
    "\n",
    "# Test avec subway-in\n",
    "feature_vect = torch.randn(T,N,L)\n",
    "train_input = torch.randn(T,N)\n",
    "dims = [0]  # min = [N]\n",
    "mini = torch.randn(N)\n",
    "\n",
    "repeated_mini = repeat_mini(feature_vect,mini, dims)\n",
    "print('Subway-in:', repeated_mini.size())\n",
    "# ...\n",
    "\n",
    "\n",
    "# Test avec NetMob Image\n",
    "train_input = torch.randn(T,N,C,H,W)\n",
    "feature_vect = torch.randn(T,N,C,H,W,L)\n",
    "dims = [0,3,4]  # min = [N]\n",
    "\n",
    "mini = torch.randn(N,C)\n",
    "\n",
    "repeated_mini = repeat_mini(train_input,mini, dims)\n",
    "print('NetMob1:', repeated_mini.size())\n",
    "# ...\n",
    "\n",
    "\n",
    "\n",
    "# Test avec NetMob sous une autre forme d'aggregation\n",
    "train_input = torch.randn(T,N,C,H,W)\n",
    "feature_vect = torch.randn(T,N,C,H,W,L)\n",
    "dims = [0,2,4]  # min = [N]\n",
    "\n",
    "mini = torch.randn(N,H)\n",
    "\n",
    "repeated_mini = repeat_mini(train_input,mini, dims)\n",
    "print('NetMob2:',repeated_mini.size())\n",
    "# ...\n",
    "\n",
    "\n",
    "# Test avec NetMob sous une autre forme d'aggregation\n",
    "train_input = torch.randn(T,N,C,H,W)\n",
    "feature_vect = torch.randn(T,N,C,H,W,L)\n",
    "dims = [0]  # min = [N]\n",
    "\n",
    "mini = torch.randn(N,C,H,W)\n",
    "\n",
    "repeated_mini = repeat_mini(train_input,mini, dims)\n",
    "print('NetMob3:',repeated_mini.size())\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import TensorDataset\n",
    "tensor = dataset_no_norm.train_input\n",
    "tensor_ds = TensorDataset(tensor,mini=None,maxi=None,mean=None,std=None, normalized = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_ds.mini = reshaped_inputs.min(-1).values  \n",
    "tensor_ds.maxi = reshaped_inputs.max(-1).values  \n",
    "tensor_ds.mean = reshaped_inputs.mean(-1).values  \n",
    "tensor_ds.std = reshaped_inputs.std(-1).values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n"
     ]
    }
   ],
   "source": [
    "# Normalize\n",
    "normalized_tensor_1 = tensor_ds.transform(reshaped_inputs,minmaxnorm,standardize,reverse=False)\n",
    "\n",
    "# reshape-back, inverse-permute\n",
    "normalized_tensor_2 = tensor_ds.inverse_reshape_permute(normalized_tensor_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 3606])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_tensor_1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3606, 40])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_tensor_2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (40) must match the size of tensor b (3606) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vc/gn83ctb97rd5lmlbh8djj9fr0000gr/T/ipykernel_4323/3111385966.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnormalized_tensor_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminmaxnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnormalized_tensor_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mnormalize_tensor\u001b[0;34m(self, dims, minmaxnorm, standardize, reverse)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mnormalized_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshaped_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminmaxnorm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;31m# reshape-back, inverse-permute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, inputs, minmaxnorm, standardize, reverse)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked_maxi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstacked_mini\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstacked_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 \u001b[0moutput_with_nan_and_inf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstacked_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked_maxi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstacked_mini\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Sometimes issues when divided by 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtackle_nan_inf_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_with_nan_and_inf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (40) must match the size of tensor b (3606) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "normalized_tensor_ds = tensor_ds.normalize_tensor(dims = [0], minmaxnorm=True)\n",
    "normalized_tensor_ds.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6., 2., 0.,  ..., 7., 3., 6.], dtype=torch.float64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_tensor_ds.mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = torch.randn(100,40)\n",
    "values_bis = values.transpose(1,0)\n",
    "mini  = torch.randn(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.df\n",
    "reindex = dataset.df_verif.stack().unique()[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataSet' object has no attribute 'train_input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vc/gn83ctb97rd5lmlbh8djj9fr0000gr/T/ipykernel_4323/1420198651.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# dataset.train_valid_test_split_indices(self,train_prop,valid_prop,test_prop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataSet' object has no attribute 'train_input'"
     ]
    }
   ],
   "source": [
    "# dataset.train_valid_test_split_indices(self,train_prop,valid_prop,test_prop)\n",
    "dataset.train_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2019-03-22T07:45:00.000000000', '2019-03-23T07:45:00.000000000',\n",
       "       '2019-03-16T08:00:00.000000000', '2019-03-22T08:00:00.000000000',\n",
       "       '2019-03-23T08:00:00.000000000', '2019-03-16T08:15:00.000000000',\n",
       "       '2019-03-22T08:15:00.000000000', '2019-03-23T08:15:00.000000000',\n",
       "       '2019-03-16T08:30:00.000000000', '2019-03-22T08:30:00.000000000'],\n",
       "      dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_indices = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Station</th>\n",
       "      <th>Ampère Victor Hugo</th>\n",
       "      <th>Bellecour</th>\n",
       "      <th>Brotteaux</th>\n",
       "      <th>Charpennes</th>\n",
       "      <th>Cordeliers</th>\n",
       "      <th>Croix Paquet</th>\n",
       "      <th>Croix-Rousse</th>\n",
       "      <th>Cuire</th>\n",
       "      <th>Cusset</th>\n",
       "      <th>Debourg</th>\n",
       "      <th>...</th>\n",
       "      <th>Part-Dieu</th>\n",
       "      <th>Perrache</th>\n",
       "      <th>Place Guichard</th>\n",
       "      <th>Place Jean Jaurès</th>\n",
       "      <th>République Villeurbanne</th>\n",
       "      <th>Sans Souci</th>\n",
       "      <th>Saxe - Gambetta</th>\n",
       "      <th>Stade de Gerland</th>\n",
       "      <th>Valmy</th>\n",
       "      <th>Vieux Lyon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-03-16 00:00:00</th>\n",
       "      <td>34.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>155.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-16 00:15:00</th>\n",
       "      <td>40.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>77.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-16 00:30:00</th>\n",
       "      <td>17.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-16 00:45:00</th>\n",
       "      <td>1.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-16 01:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31 22:45:00</th>\n",
       "      <td>28.0</td>\n",
       "      <td>355.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>...</td>\n",
       "      <td>92.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31 23:00:00</th>\n",
       "      <td>34.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>74.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31 23:15:00</th>\n",
       "      <td>28.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>...</td>\n",
       "      <td>104.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31 23:30:00</th>\n",
       "      <td>26.0</td>\n",
       "      <td>342.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>...</td>\n",
       "      <td>135.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31 23:45:00</th>\n",
       "      <td>18.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>...</td>\n",
       "      <td>66.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7392 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Station              Ampère Victor Hugo  Bellecour  Brotteaux  Charpennes  \\\n",
       "2019-03-16 00:00:00                34.0      396.0       37.0       164.0   \n",
       "2019-03-16 00:15:00                40.0      298.0       56.0       164.0   \n",
       "2019-03-16 00:30:00                17.0      258.0       24.0        74.0   \n",
       "2019-03-16 00:45:00                 1.0       44.0        3.0        13.0   \n",
       "2019-03-16 01:00:00                 0.0        0.0        0.0         3.0   \n",
       "...                                 ...        ...        ...         ...   \n",
       "2019-05-31 22:45:00                28.0      355.0       37.0       163.0   \n",
       "2019-05-31 23:00:00                34.0      262.0       37.0       157.0   \n",
       "2019-05-31 23:15:00                28.0      325.0       37.0       154.0   \n",
       "2019-05-31 23:30:00                26.0      342.0       42.0       132.0   \n",
       "2019-05-31 23:45:00                18.0      399.0       20.0        86.0   \n",
       "\n",
       "Station              Cordeliers  Croix Paquet  Croix-Rousse  Cuire  Cusset  \\\n",
       "2019-03-16 00:00:00       143.0           6.0          33.0   17.0    24.0   \n",
       "2019-03-16 00:15:00       139.0           3.0          46.0    7.0    20.0   \n",
       "2019-03-16 00:30:00       100.0          19.0           6.0    0.0     4.0   \n",
       "2019-03-16 00:45:00        13.0           1.0           6.0    3.0     2.0   \n",
       "2019-03-16 01:00:00         0.0           0.0           0.0    0.0     5.0   \n",
       "...                         ...           ...           ...    ...     ...   \n",
       "2019-05-31 22:45:00       133.0           1.0          44.0   10.0    42.0   \n",
       "2019-05-31 23:00:00       155.0           9.0          51.0   14.0    30.0   \n",
       "2019-05-31 23:15:00       126.0          13.0          45.0    8.0    28.0   \n",
       "2019-05-31 23:30:00       101.0           5.0          49.0   16.0    14.0   \n",
       "2019-05-31 23:45:00       107.0           8.0          50.0    9.0    24.0   \n",
       "\n",
       "Station              Debourg  ...  Part-Dieu  Perrache  Place Guichard  \\\n",
       "2019-03-16 00:00:00     40.0  ...       68.0     144.0            26.0   \n",
       "2019-03-16 00:15:00     46.0  ...       77.0     145.0            31.0   \n",
       "2019-03-16 00:30:00     13.0  ...       43.0      55.0            15.0   \n",
       "2019-03-16 00:45:00      2.0  ...        6.0       5.0             3.0   \n",
       "2019-03-16 01:00:00      0.0  ...        0.0       0.0             4.0   \n",
       "...                      ...  ...        ...       ...             ...   \n",
       "2019-05-31 22:45:00     79.0  ...       92.0     224.0           129.0   \n",
       "2019-05-31 23:00:00     66.0  ...       74.0     261.0           124.0   \n",
       "2019-05-31 23:15:00     44.0  ...      104.0     140.0            66.0   \n",
       "2019-05-31 23:30:00     58.0  ...      135.0     185.0            57.0   \n",
       "2019-05-31 23:45:00     41.0  ...       66.0     124.0            70.0   \n",
       "\n",
       "Station              Place Jean Jaurès  République Villeurbanne  Sans Souci  \\\n",
       "2019-03-16 00:00:00               48.0                     53.0        55.0   \n",
       "2019-03-16 00:15:00               32.0                     45.0        55.0   \n",
       "2019-03-16 00:30:00               19.0                      4.0        45.0   \n",
       "2019-03-16 00:45:00                4.0                      0.0        13.0   \n",
       "2019-03-16 01:00:00                4.0                      2.0         2.0   \n",
       "...                                ...                      ...         ...   \n",
       "2019-05-31 22:45:00               34.0                     32.0        58.0   \n",
       "2019-05-31 23:00:00               42.0                     24.0        27.0   \n",
       "2019-05-31 23:15:00               33.0                     14.0        45.0   \n",
       "2019-05-31 23:30:00               23.0                     16.0        25.0   \n",
       "2019-05-31 23:45:00               24.0                     33.0        37.0   \n",
       "\n",
       "Station              Saxe - Gambetta  Stade de Gerland  Valmy  Vieux Lyon  \n",
       "2019-03-16 00:00:00            112.0              59.0   71.0       155.0  \n",
       "2019-03-16 00:15:00            159.0              56.0   41.0       149.0  \n",
       "2019-03-16 00:30:00             98.0               4.0   72.0       127.0  \n",
       "2019-03-16 00:45:00             17.0               4.0    3.0        12.0  \n",
       "2019-03-16 01:00:00              0.0               0.0    1.0         0.0  \n",
       "...                              ...               ...    ...         ...  \n",
       "2019-05-31 22:45:00            152.0              83.0   49.0        86.0  \n",
       "2019-05-31 23:00:00            127.0              52.0   53.0        98.0  \n",
       "2019-05-31 23:15:00            140.0              46.0   57.0       110.0  \n",
       "2019-05-31 23:30:00            198.0              49.0   53.0       119.0  \n",
       "2019-05-31 23:45:00            165.0              44.0   36.0       104.0  \n",
       "\n",
       "[7392 rows x 40 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reindex(self.df_verif_train.stack().unique())\n",
    "        self.df_valid = self.df.reindex(self.df_verif_valid.stack().unique()) if valid_prop > 1e-3 else None\n",
    "        self.df_test = self.df.reindex(self.df_verif_test.stack().unique()) if test_prop > 1e-3 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  268.,  1342.,   255.,  1329.,   974.,    47.,   298.,   377.,   320.,\n",
       "           504.,   406.,   414.,   885.,  1186.,   804.,   561.,  1510.,  1062.,\n",
       "           587., 10798.,   433.,  1036.,  1072.,   751.,   545.,   528.,   419.,\n",
       "           480.,   480.,   334.,  1366.,  1057.,   396.,   575.,   400.,   816.,\n",
       "           771.,   987.,   454.,   601.], dtype=torch.float64),\n",
       " tensor(52.6216, dtype=torch.float64))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(dataset_no_norm.df_train.values).max(dim = 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 52.6216, 293.6610,  52.4313, 254.9218, 139.7208,   8.3684,  53.1447,\n",
       "         46.3802,  59.7228,  83.3747,  69.9714,  92.0631, 140.9368, 239.9919,\n",
       "        130.1312, 117.5363, 255.4895, 211.9190, 105.1930, 267.2897,  48.4344,\n",
       "        210.3718, 170.5582, 158.2803, 101.5777,  42.1111,  86.7357,  88.9991,\n",
       "         86.1424,  58.0196, 286.4249, 218.4956,  76.9825, 108.5609,  71.4679,\n",
       "        133.8276, 149.7332,  71.5599,  90.7328,  84.7020], dtype=torch.float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(dataset_no_norm.df_train.values).std(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 54.3095, 308.4409,  48.4077, 271.0086, 136.6765,   8.9445,  60.1783,\n",
       "         37.6090,  63.1620,  82.7944,  70.6380,  91.0107, 119.5044, 201.6920,\n",
       "        117.0514, 115.0198, 230.7813, 208.1015, 111.5652, 156.0616,  42.3608,\n",
       "        251.7402, 167.9920, 149.7220, 108.6614,  38.6057,  91.7052,  87.2338,\n",
       "         83.1197,  55.9168, 295.6901, 246.4996,  70.7958,  98.2704,  70.5975,\n",
       "        123.9393, 177.1622,  54.1563,  97.9735,  90.2060], dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(dataset_no_norm.df_train.values).mean(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Station\n",
       "Ampère Victor Hugo                 268.0\n",
       "Bellecour                         1342.0\n",
       "Brotteaux                          255.0\n",
       "Charpennes                        1329.0\n",
       "Cordeliers                         974.0\n",
       "Croix Paquet                        47.0\n",
       "Croix-Rousse                       298.0\n",
       "Cuire                              377.0\n",
       "Cusset                             320.0\n",
       "Debourg                            504.0\n",
       "Flachet                            406.0\n",
       "Foch                               414.0\n",
       "Gare d'Oullins                     885.0\n",
       "Gare de Vaise                     1186.0\n",
       "Gare de Vénissieux                 804.0\n",
       "Garibaldi                          561.0\n",
       "Gorge de Loup                     1510.0\n",
       "Grange Blanche                    1062.0\n",
       "Gratte Ciel                        587.0\n",
       "Guillotière                      10798.0\n",
       "Hénon                              433.0\n",
       "Hôtel de ville - Louis Pradel     1036.0\n",
       "Jean Macé                         1072.0\n",
       "La soie                            751.0\n",
       "Laurent Bonnevay                   545.0\n",
       "Laënnec                            528.0\n",
       "Masséna                            419.0\n",
       "Mermoz - Pinel                     480.0\n",
       "Monplaisir Lumière                 480.0\n",
       "Parilly                            334.0\n",
       "Part-Dieu                         1366.0\n",
       "Perrache                          1057.0\n",
       "Place Guichard                     396.0\n",
       "Place Jean Jaurès                  575.0\n",
       "République Villeurbanne            400.0\n",
       "Sans Souci                         816.0\n",
       "Saxe - Gambetta                    771.0\n",
       "Stade de Gerland                   987.0\n",
       "Valmy                              454.0\n",
       "Vieux Lyon                         601.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_no_norm.df_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tackling Training Set\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vc/gn83ctb97rd5lmlbh8djj9fr0000gr/T/ipykernel_3785/3379316797.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_dataset_get_tensor_and_train_valid_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minvalid_dates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_prop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mclean_dataset_get_tensor_and_train_valid_test_split\u001b[0;34m(self, df, invalid_dates, train_prop, valid_prop, test_prop, normalize)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_valid_test_split_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_prop\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create df_train,df_valid,df_test, df_verif_train, df_verif_valid, df_verif_test, and dates limits for each df and each tensor U\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Récupère U_test, Utarget_test, NetMob_test, Weather_test etc....  dans 'dataset_init.contextual_tensors.items()'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36msplit_tensors\u001b[0;34m(self, normalize)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0;34m''' Split input tensors  in Train/Valid/Test part '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;31m# Get U_train, U_valid, U_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_train_valid_test_tensor_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'U'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mref_for_normalization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;31m# Get Utarget_train, Utarget_valid, Utarget_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mset_train_valid_test_tensor_attribute\u001b[0;34m(self, name, tensor, dims, ref_for_normalization, normalize)\u001b[0m\n\u001b[1;32m    601\u001b[0m                                     minmaxnorm = True,standardize = False)\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0mtrain_tensor_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_tensor_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_tensor_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_normalize_tensor_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf\"{name}_train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tensor_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf\"{name}_valid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_tensor_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mload_normalize_tensor_datasets\u001b[0;34m(self, mini, maxi, mean, std, normalize)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminmaxnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mini'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mnormalize_tensor\u001b[0;34m(self, dims, minmaxnorm, standardize, reverse)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mnormalized_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshaped_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminmaxnorm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;31m# reshape-back, inverse-permute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, inputs, minmaxnorm, standardize, reverse)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# MinMax Normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mminmaxnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mstacked_mini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshaped_inputs_dim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0mstacked_maxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshaped_inputs_dim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.float64"
     ]
    }
   ],
   "source": [
    "dataset_norm = dataset.clean_dataset_get_tensor_and_train_valid_test_split(dataset.df,invalid_dates,args.train_prop,args.valid_prop,args.test_prop, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tackling Training Set\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vc/gn83ctb97rd5lmlbh8djj9fr0000gr/T/ipykernel_2908/1601370515.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_normalize_load_feature_vect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minvalid_dates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_prop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_prop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_prop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36msplit_normalize_load_feature_vect\u001b[0;34m(self, args, invalid_dates, train_prop, valid_prop, test_prop)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;31m# Get all the splitted train/valid/test input tensors. Normalize Them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;31m# ================ FAIRE QULEQUE CHOSE POUR LE TIME-SLOTS LABELS. ESSAYER DE LES INTEGRER DANS LE CONTEXTUAL TENSORS  ================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36msplit_tensors\u001b[0;34m(self, normalize)\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;34m''' Split input tensors  in Train/Valid/Test part '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0;31m# Get U_train, U_valid, U_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_train_valid_test_tensor_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'U'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mref_for_normalization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;31m# Get Utarget_train, Utarget_valid, Utarget_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mset_train_valid_test_tensor_attribute\u001b[0;34m(self, name, tensor, dims, ref_for_normalization, normalize)\u001b[0m\n\u001b[1;32m    636\u001b[0m                                     minmaxnorm = True,standardize = False)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mtrain_tensor_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_tensor_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_tensor_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_normalize_tensor_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf\"{name}_train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tensor_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf\"{name}_valid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_tensor_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mload_normalize_tensor_datasets\u001b[0;34m(self, mini, maxi, mean, std, normalize)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminmaxnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mini'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mnormalize_tensor\u001b[0;34m(self, dims, minmaxnorm, standardize, reverse)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mnormalized_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshaped_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminmaxnorm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;31m# reshape-back, inverse-permute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, inputs, minmaxnorm, standardize, reverse)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# MinMax Normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mminmaxnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mstacked_mini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshaped_inputs_dim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0mstacked_maxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshaped_inputs_dim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.float64"
     ]
    }
   ],
   "source": [
    "dataset.split_normalize_load_feature_vect(args,invalid_dates,args.train_prop, args.valid_prop, args.test_prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pour le cas des NetMob Data : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-03-16 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-03-16 00:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-03-16 00:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-03-16 00:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-03-16 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>2019-05-31 22:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7388</th>\n",
       "      <td>2019-05-31 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7389</th>\n",
       "      <td>2019-05-31 23:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7390</th>\n",
       "      <td>2019-05-31 23:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7391</th>\n",
       "      <td>2019-05-31 23:45:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7392 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date\n",
       "0    2019-03-16 00:00:00\n",
       "1    2019-03-16 00:15:00\n",
       "2    2019-03-16 00:30:00\n",
       "3    2019-03-16 00:45:00\n",
       "4    2019-03-16 01:00:00\n",
       "...                  ...\n",
       "7387 2019-05-31 22:45:00\n",
       "7388 2019-05-31 23:00:00\n",
       "7389 2019-05-31 23:15:00\n",
       "7390 2019-05-31 23:30:00\n",
       "7391 2019-05-31 23:45:00\n",
       "\n",
       "[7392 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t-672</th>\n",
       "      <th>t-96</th>\n",
       "      <th>t-6</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t+0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>2019-03-16 00:00:00</td>\n",
       "      <td>2019-03-22 00:00:00</td>\n",
       "      <td>2019-03-22 22:30:00</td>\n",
       "      <td>2019-03-22 22:45:00</td>\n",
       "      <td>2019-03-22 23:00:00</td>\n",
       "      <td>2019-03-22 23:15:00</td>\n",
       "      <td>2019-03-22 23:30:00</td>\n",
       "      <td>2019-03-22 23:45:00</td>\n",
       "      <td>2019-03-23 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>2019-03-16 00:15:00</td>\n",
       "      <td>2019-03-22 00:15:00</td>\n",
       "      <td>2019-03-22 22:45:00</td>\n",
       "      <td>2019-03-22 23:00:00</td>\n",
       "      <td>2019-03-22 23:15:00</td>\n",
       "      <td>2019-03-22 23:30:00</td>\n",
       "      <td>2019-03-22 23:45:00</td>\n",
       "      <td>2019-03-23 00:00:00</td>\n",
       "      <td>2019-03-23 00:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>2019-03-16 00:30:00</td>\n",
       "      <td>2019-03-22 00:30:00</td>\n",
       "      <td>2019-03-22 23:00:00</td>\n",
       "      <td>2019-03-22 23:15:00</td>\n",
       "      <td>2019-03-22 23:30:00</td>\n",
       "      <td>2019-03-22 23:45:00</td>\n",
       "      <td>2019-03-23 00:00:00</td>\n",
       "      <td>2019-03-23 00:15:00</td>\n",
       "      <td>2019-03-23 00:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>2019-03-16 00:45:00</td>\n",
       "      <td>2019-03-22 00:45:00</td>\n",
       "      <td>2019-03-22 23:15:00</td>\n",
       "      <td>2019-03-22 23:30:00</td>\n",
       "      <td>2019-03-22 23:45:00</td>\n",
       "      <td>2019-03-23 00:00:00</td>\n",
       "      <td>2019-03-23 00:15:00</td>\n",
       "      <td>2019-03-23 00:30:00</td>\n",
       "      <td>2019-03-23 00:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>2019-03-16 01:00:00</td>\n",
       "      <td>2019-03-22 01:00:00</td>\n",
       "      <td>2019-03-22 23:30:00</td>\n",
       "      <td>2019-03-22 23:45:00</td>\n",
       "      <td>2019-03-23 00:00:00</td>\n",
       "      <td>2019-03-23 00:15:00</td>\n",
       "      <td>2019-03-23 00:30:00</td>\n",
       "      <td>2019-03-23 00:45:00</td>\n",
       "      <td>2019-03-23 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>2019-05-24 22:45:00</td>\n",
       "      <td>2019-05-30 22:45:00</td>\n",
       "      <td>2019-05-31 21:15:00</td>\n",
       "      <td>2019-05-31 21:30:00</td>\n",
       "      <td>2019-05-31 21:45:00</td>\n",
       "      <td>2019-05-31 22:00:00</td>\n",
       "      <td>2019-05-31 22:15:00</td>\n",
       "      <td>2019-05-31 22:30:00</td>\n",
       "      <td>2019-05-31 22:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7388</th>\n",
       "      <td>2019-05-24 23:00:00</td>\n",
       "      <td>2019-05-30 23:00:00</td>\n",
       "      <td>2019-05-31 21:30:00</td>\n",
       "      <td>2019-05-31 21:45:00</td>\n",
       "      <td>2019-05-31 22:00:00</td>\n",
       "      <td>2019-05-31 22:15:00</td>\n",
       "      <td>2019-05-31 22:30:00</td>\n",
       "      <td>2019-05-31 22:45:00</td>\n",
       "      <td>2019-05-31 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7389</th>\n",
       "      <td>2019-05-24 23:15:00</td>\n",
       "      <td>2019-05-30 23:15:00</td>\n",
       "      <td>2019-05-31 21:45:00</td>\n",
       "      <td>2019-05-31 22:00:00</td>\n",
       "      <td>2019-05-31 22:15:00</td>\n",
       "      <td>2019-05-31 22:30:00</td>\n",
       "      <td>2019-05-31 22:45:00</td>\n",
       "      <td>2019-05-31 23:00:00</td>\n",
       "      <td>2019-05-31 23:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7390</th>\n",
       "      <td>2019-05-24 23:30:00</td>\n",
       "      <td>2019-05-30 23:30:00</td>\n",
       "      <td>2019-05-31 22:00:00</td>\n",
       "      <td>2019-05-31 22:15:00</td>\n",
       "      <td>2019-05-31 22:30:00</td>\n",
       "      <td>2019-05-31 22:45:00</td>\n",
       "      <td>2019-05-31 23:00:00</td>\n",
       "      <td>2019-05-31 23:15:00</td>\n",
       "      <td>2019-05-31 23:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7391</th>\n",
       "      <td>2019-05-24 23:45:00</td>\n",
       "      <td>2019-05-30 23:45:00</td>\n",
       "      <td>2019-05-31 22:15:00</td>\n",
       "      <td>2019-05-31 22:30:00</td>\n",
       "      <td>2019-05-31 22:45:00</td>\n",
       "      <td>2019-05-31 23:00:00</td>\n",
       "      <td>2019-05-31 23:15:00</td>\n",
       "      <td>2019-05-31 23:30:00</td>\n",
       "      <td>2019-05-31 23:45:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5662 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   t-672                t-96                 t-6  \\\n",
       "672  2019-03-16 00:00:00 2019-03-22 00:00:00 2019-03-22 22:30:00   \n",
       "673  2019-03-16 00:15:00 2019-03-22 00:15:00 2019-03-22 22:45:00   \n",
       "674  2019-03-16 00:30:00 2019-03-22 00:30:00 2019-03-22 23:00:00   \n",
       "675  2019-03-16 00:45:00 2019-03-22 00:45:00 2019-03-22 23:15:00   \n",
       "676  2019-03-16 01:00:00 2019-03-22 01:00:00 2019-03-22 23:30:00   \n",
       "...                  ...                 ...                 ...   \n",
       "7387 2019-05-24 22:45:00 2019-05-30 22:45:00 2019-05-31 21:15:00   \n",
       "7388 2019-05-24 23:00:00 2019-05-30 23:00:00 2019-05-31 21:30:00   \n",
       "7389 2019-05-24 23:15:00 2019-05-30 23:15:00 2019-05-31 21:45:00   \n",
       "7390 2019-05-24 23:30:00 2019-05-30 23:30:00 2019-05-31 22:00:00   \n",
       "7391 2019-05-24 23:45:00 2019-05-30 23:45:00 2019-05-31 22:15:00   \n",
       "\n",
       "                     t-5                 t-4                 t-3  \\\n",
       "672  2019-03-22 22:45:00 2019-03-22 23:00:00 2019-03-22 23:15:00   \n",
       "673  2019-03-22 23:00:00 2019-03-22 23:15:00 2019-03-22 23:30:00   \n",
       "674  2019-03-22 23:15:00 2019-03-22 23:30:00 2019-03-22 23:45:00   \n",
       "675  2019-03-22 23:30:00 2019-03-22 23:45:00 2019-03-23 00:00:00   \n",
       "676  2019-03-22 23:45:00 2019-03-23 00:00:00 2019-03-23 00:15:00   \n",
       "...                  ...                 ...                 ...   \n",
       "7387 2019-05-31 21:30:00 2019-05-31 21:45:00 2019-05-31 22:00:00   \n",
       "7388 2019-05-31 21:45:00 2019-05-31 22:00:00 2019-05-31 22:15:00   \n",
       "7389 2019-05-31 22:00:00 2019-05-31 22:15:00 2019-05-31 22:30:00   \n",
       "7390 2019-05-31 22:15:00 2019-05-31 22:30:00 2019-05-31 22:45:00   \n",
       "7391 2019-05-31 22:30:00 2019-05-31 22:45:00 2019-05-31 23:00:00   \n",
       "\n",
       "                     t-2                 t-1                 t+0  \n",
       "672  2019-03-22 23:30:00 2019-03-22 23:45:00 2019-03-23 00:00:00  \n",
       "673  2019-03-22 23:45:00 2019-03-23 00:00:00 2019-03-23 00:15:00  \n",
       "674  2019-03-23 00:00:00 2019-03-23 00:15:00 2019-03-23 00:30:00  \n",
       "675  2019-03-23 00:15:00 2019-03-23 00:30:00 2019-03-23 00:45:00  \n",
       "676  2019-03-23 00:30:00 2019-03-23 00:45:00 2019-03-23 01:00:00  \n",
       "...                  ...                 ...                 ...  \n",
       "7387 2019-05-31 22:15:00 2019-05-31 22:30:00 2019-05-31 22:45:00  \n",
       "7388 2019-05-31 22:30:00 2019-05-31 22:45:00 2019-05-31 23:00:00  \n",
       "7389 2019-05-31 22:45:00 2019-05-31 23:00:00 2019-05-31 23:15:00  \n",
       "7390 2019-05-31 23:00:00 2019-05-31 23:15:00 2019-05-31 23:30:00  \n",
       "7391 2019-05-31 23:15:00 2019-05-31 23:30:00 2019-05-31 23:45:00  \n",
       "\n",
       "[5662 rows x 9 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df_verif"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "preprocessingclone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
