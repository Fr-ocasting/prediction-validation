{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Points à vérifier dans mon code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DatesVerifFeatureVect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vc/gn83ctb97rd5lmlbh8djj9fr0000gr/T/ipykernel_2573/412254403.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Trouver un moyen d'uniformiser 'time_step_per_hour' pour toute les données, genre dénominateur commun etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdates_verif_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatesVerifFeatureVect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWeeks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistorical_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_ahead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_ahead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_step_per_hour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_step_per_hour\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_vect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid_dates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ========== .... ==========\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DatesVerifFeatureVect' is not defined"
     ]
    }
   ],
   "source": [
    "from DL_class import InvalidDatesCleaner,DatesVerifFeatureVect\n",
    "# Trouver un moyen d'uniformiser 'time_step_per_hour' pour toute les données, genre dénominateur commun etc.\n",
    "dates_verif_obj = DatesVerifFeatureVect(dataset.df_dates, Weeks = args.W, Days = args.D, historical_len = args.H, step_ahead = args.step_ahead, time_step_per_hour = dataset.time_step_per_hour)\n",
    "dataset.get_feature_vect(invalid_dates)\n",
    "# ========== .... ==========\n",
    "\n",
    "# Passage dans split Tensor: \n",
    "# >>>>  Consiste à Normalizer un input (Ici 'U') selon une certaine référence (ici train_input)\n",
    "# >>>>  Puis de renovyer les U_train/U_valid/U_test \n",
    "# >>>>\n",
    "# >>>>   Pour se faire, ils calculent d'abord les stats (min,max,mean,std) sur le tensor de référence train_input\n",
    "# >>>>   Puis, à chacun des split U_Train/U_Valid/U_Test, normalisation selon les statistiques.\n",
    "\n",
    "\n",
    "# ==== ATTENTION il faudra sans doute récupérer les stats simplement et pas normalizer la ref\n",
    "# ==== Calcul inutile ici \n",
    "\n",
    "\n",
    "# ========= PERSONAL_INPUT =========\n",
    "# >>>> \n",
    "# >>>> PB_1 : Il faut absolument pouvoir prendre en compte les invalid dates de tout le monde.\n",
    "# >>>> 1. Il le faire en amont. i.e lorsqu'on load les données, on load aussi les invalid dates associées\n",
    "# >>>> 2. Puis on fait l'union des invalid dates\n",
    "# >>>> 3. Après ça on train/valid/test split normalize tout.\n",
    "# >>>> \n",
    "# >>>> PB_2: Idem pour le coverage pour que ça corresponde à tout le monde.  \n",
    "# >>>> \n",
    "# >>>> PB_3 : Génère des input de taille différentes selon l'historique demandé. \n",
    "# >>>> Cela ne va pas convenir si jamais je travail avec du calendar class ou autre.\n",
    "# >>>> 1. On peut donc enregistrer l'ensemble des invalid-dates\n",
    "# >>>> 2. Créer les séquences/input_tensor associées \n",
    "# >>>> 3. Pour chaque donnée, on ne prend peut être pas en compte les même historiques. \n",
    "# >>>>    Donc on peut faire une liste de date 'D' correspondant à l'intersection de toute les target (df_verif[-1] ...?)\n",
    "# >>>> 4. Puis on extrait l'ensemble des sequences de chaque Data sur D \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultats de Normalization qu'on est censé conserver : \n",
    "**Tackling Subway Data:**\n",
    "- U_train min:  tensor(0.) U_train max:  tensor(1.)\n",
    "- U_valid min:  tensor(0.0447) U_valid max:  tensor(4.1455)\n",
    "- U_test min:  tensor(0.) U_test max:  tensor(4.1892)\n",
    "\n",
    "**Tackling NetMob Data:** \n",
    "- U_train min:  tensor(0.) U_train max:  tensor(1.)\n",
    "- U_valid min:  tensor(-0.1006) U_valid max:  tensor(1.0808)\n",
    "- U_test min:  tensor(-0.1259) U_test max:  tensor(1.1083)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement avec Subway pour voir si tout est bien conservé "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'pynvml' is not available on this environment.\n",
      "Training and Hyper-parameter tuning with Ray is not possible\n"
     ]
    }
   ],
   "source": [
    "from dataset import TrainValidTest_Split_Normalize,PersonnalInput\n",
    "from loader import DictDataLoader\n",
    "from trainer import Trainer \n",
    "\n",
    "from plotting_bokeh import generate_bokeh\n",
    "from calendar_class import get_time_slots_labels\n",
    "import os\n",
    "import pandas as pd \n",
    "import torch\n",
    "import pickle \n",
    "import numpy as np \n",
    "\n",
    "from paths import folder_path,file_name\n",
    "from config import get_args\n",
    "from utilities_DL import get_DataSet_and_invalid_dates, match_period_coverage_with_netmob,get_loss,load_model_and_optimizer,get_model_loss_args_emb_opts\n",
    "\n",
    "\n",
    "def load_subway_in(folder_path,file_name,args,coverage):\n",
    "    '''Tackling Subway_in data'''\n",
    "    dataset,invalid_dates = get_DataSet_and_invalid_dates(args.abs_path, folder_path,file_name,\n",
    "                                                        args.W,args.D,args.H,args.step_ahead,\n",
    "                                                        single_station = False,coverage_period = coverage)\n",
    "\n",
    "    subway_ds = PersonnalInput(invalid_dates,args, tensor = dataset.raw_values, dates = dataset.df_dates,\n",
    "                            time_step_per_hour = dataset.time_step_per_hour,Weeks = args.W, Days = args.D, historical_len = args.H,step_ahead = args.step_ahead,minmaxnorm = True ,dims=[0])\n",
    "    subway_ds.preprocess(args.train_prop,args.valid_prop,args.test_prop)\n",
    "    return(subway_ds,dataset,invalid_dates)\n",
    "\n",
    "def load_calendar(subway_ds):\n",
    "    '''Tackling Calendar Data''' \n",
    "    time_slots_labels,dic_class2rpz,dic_rpz2class,nb_words_embedding = get_time_slots_labels(subway_ds)\n",
    "    tensor_limits_keeper = subway_ds.tensor_limits_keeper\n",
    "\n",
    "    dict_calendar_U_train,dict_calendar_U_valid,dict_calendar_U_test = {},{},{}\n",
    "    for calendar_class in [0,1,2,3]:\n",
    "        calendar_tensor = time_slots_labels[calendar_class] #args.calendar_class\n",
    "\n",
    "        splitter = TrainValidTest_Split_Normalize(calendar_tensor,\n",
    "                                    first_train = tensor_limits_keeper.first_train_U, last_train= tensor_limits_keeper.last_train_U,\n",
    "                                    first_valid= tensor_limits_keeper.first_valid_U, last_valid = tensor_limits_keeper.last_valid_U,\n",
    "                                    first_test = tensor_limits_keeper.first_test_U, last_test = tensor_limits_keeper.last_test_U)\n",
    "\n",
    "        train_tensor_ds,valid_tensor_ds,test_tensor_ds = splitter.split_normalize_tensor_datasets(normalizer = None)\n",
    "        calendar_U_train,calendar_U_valid,calendar_U_test = train_tensor_ds.tensor,valid_tensor_ds.tensor,test_tensor_ds.tensor\n",
    "        dict_calendar_U_train[calendar_class] = calendar_U_train\n",
    "        dict_calendar_U_valid[calendar_class] = calendar_U_valid\n",
    "        dict_calendar_U_test[calendar_class] = calendar_U_test\n",
    "    return(dict_calendar_U_train,dict_calendar_U_valid,dict_calendar_U_test,dic_class2rpz,dic_rpz2class,nb_words_embedding)\n",
    "\n",
    "\n",
    "def load_netmob_data(dataset,invalid_dates,args,save_folder,ref_subway = None):\n",
    "    '''Load NetMob Data:\n",
    "    outputs:\n",
    "    --------\n",
    "    # NetMob Tensor : [T,N,C,H,W]\n",
    "    # dims : [0,3,4] #[0,-2,-1]  -> dimension for which we want to retrieve stats \n",
    "    '''\n",
    "    try :\n",
    "        netmob_T = torch.stack([torch.load(f\"{save_folder}station_{station}.pt\") for station in ref_subway.COD_TRG])\n",
    "        netmob_T = netmob_T.permute(1,0,*range(2, netmob_T.dim()))\n",
    "\n",
    "    except:\n",
    "        netmob_T = torch.randn(dataset.length,40,2,22,22)\n",
    "        print(\"Load des données NetMob .pt impossible. Création d'un random Tensor\")\n",
    "\n",
    "    print('Init NetMob Dataset: ', netmob_T.size())\n",
    "    print('Number of Nan Value: ',torch.isnan(netmob_T).sum())\n",
    "    print('Total Number of Elements: ', netmob_T.numel(),'\\n')\n",
    "\n",
    "    NetMob_ds = PersonnalInput(invalid_dates,args, tensor = netmob_T, dates = dataset.df_dates,\n",
    "                           time_step_per_hour = dataset.time_step_per_hour,Weeks = args.W, Days = args.D, historical_len = args.H,step_ahead = args.step_ahead,minmaxnorm = True,dims =[0,3,4])\n",
    "    NetMob_ds.preprocess(args.train_prop,args.valid_prop,args.test_prop)\n",
    "\n",
    "    return(NetMob_ds)\n",
    "\n",
    "\n",
    "def add_contextual_data(dataset_names,subway_ds,NetMob_ds,dict_calendar_U_train,dict_calendar_U_valid,dict_calendar_U_test):\n",
    "    # === Define DataLoader : \n",
    "    contextual_tensors,positions = {},{}\n",
    "    if 'calendar' in dataset_names:\n",
    "        contextual_tensors = {f'calendar_{calendar_class}': {'train': dict_calendar_U_train[calendar_class],\n",
    "                                        'valid': dict_calendar_U_valid[calendar_class],\n",
    "                                        'test': dict_calendar_U_test[calendar_class]} for calendar_class in dict_calendar_U_train.keys()\n",
    "                                        } \n",
    "        pos_calendar = list(contextual_tensors.keys()).index(f'calendar_{args.calendar_class}')\n",
    "        pos_calibration_calendar = list(contextual_tensors.keys()).index(f'calendar_{0}')\n",
    "        positions['calendar'] = pos_calendar\n",
    "        positions['calibration_calendar'] = pos_calibration_calendar\n",
    "\n",
    "    if 'netmob' in dataset_names:\n",
    "        contextual_tensors.update({'netmob': {'train': NetMob_ds.U_train,\n",
    "                                        'valid': NetMob_ds.U_valid,\n",
    "                                        'test': NetMob_ds.U_test}\n",
    "                                        }\n",
    "                                        )\n",
    "        \n",
    "        pos_netmob = list(contextual_tensors.keys()).index('netmob')\n",
    "        positions['netmob'] = pos_netmob\n",
    "\n",
    "\n",
    "\n",
    "    subway_ds.contextual_tensors = contextual_tensors\n",
    "    subway_ds.get_dataloader()\n",
    "\n",
    "    return(subway_ds,positions)\n",
    "\n",
    "def load_everything(dataset_names,folder_path,file_name,args,coverage,data_folder_path):\n",
    "    subway_ds,dataset,invalid_dates = load_subway_in(folder_path,file_name,args,coverage)\n",
    "    # Calendar: \n",
    "    if 'calendar' in dataset_names:\n",
    "        dict_calendar_U_train,dict_calendar_U_valid,dict_calendar_U_test,dic_class2rpz,dic_rpz2class,nb_words_embedding = load_calendar(subway_ds)\n",
    "        args.time_embedding = True\n",
    "    else:\n",
    "        dict_calendar_U_train,dict_calendar_U_valid,dict_calendar_U_test,dic_class2rpz,dic_rpz2class,nb_words_embedding = None,None,None,None,None,None\n",
    "        args.time_embedding = False\n",
    "    # ...\n",
    "\n",
    "    # Netmob: \n",
    "    if 'netmob' in dataset_names:\n",
    "        NetMob_ds = load_netmob_data(dataset,invalid_dates,args,data_folder_path,ref_subway = None)\n",
    "    else:\n",
    "        NetMob_ds = None\n",
    "    # ...\n",
    "    \n",
    "    subway_ds,positions = add_contextual_data(dataset_names,subway_ds,NetMob_ds,dict_calendar_U_train,dict_calendar_U_valid,dict_calendar_U_test)\n",
    "    return(args,subway_ds,positions,nb_words_embedding,dic_class2rpz)\n",
    "\n",
    "def get_small_ds(small_ds,coverage,args):\n",
    "    if small_ds:\n",
    "        coverage = coverage[:100]\n",
    "        args.W = 0\n",
    "        args.D = 0\n",
    "        print('Seulement les 100 premiers time-slots sont utilisés.')\n",
    "    return(coverage,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-step per hour: 4.0\n",
      "coverage period: 2019-03-16 00:00:00 - 2019-05-31 23:45:00\n",
      "\n",
      " Tackling Training Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      " Tackling Validation Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      " Tackling Testing Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      " Tackling Training Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      " Tackling Validation Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      " Tackling Testing Set\n",
      "Values with issues:  0.000%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "\n",
      "U size:  torch.Size([5662, 40, 8]) Utarget size:  torch.Size([5662, 40, 1])\n",
      "U_train size:  torch.Size([2934, 40, 8]) Utarget_train size:  torch.Size([2934, 40, 1])\n",
      "U_valid size:  torch.Size([978, 40, 8]) Utarget_valid size:  torch.Size([978, 40, 1])\n",
      "U_test size:  torch.Size([979, 40, 8]) Utarget_test size:  torch.Size([979, 40, 1])\n",
      "U_train min:  tensor(0.) U_train max:  tensor(1.)\n",
      "U_valid min:  tensor(0.) U_valid max:  tensor(1.7234)\n",
      "U_test min:  tensor(0.) U_test max:  tensor(1.3909)\n",
      "number of total parameters: 223617\n",
      "number of trainable parameters: 223617\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\romai\\.conda\\envs\\torch_gpd\\Lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "c:\\Users\\romai\\.conda\\envs\\torch_gpd\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 \n",
      " min\\epoch : 0.11\n",
      "Estimated time for training: 0.2min \n",
      "Training Throughput:550.17 sequences per seconds\n",
      ">>> Training complete in: 0:00:12.767819\n",
      ">>> Training performance time: min 0.0911562442779541 avg 0.11427712440490723 seconds (+/- 0.010279948523108184)\n",
      ">>> Loading performance time: min 0.0 avg 0.06547629314920177 seconds (+/- 0.11405821582350045)\n",
      ">>> Forward performance time: 0.038658744686252466 seconds (+/- 0.0053195722119483944)\n",
      ">>> Backward performance time: 0.07584838814787813 seconds (+/- 0.00675943584299493)\n",
      ">>> Plotting performance time: 0.0 seconds (+/- 0.0)\n",
      ">>> Saving performance time: 0.007451176643371582 seconds (+/- 0.007451176643371582)\n",
      ">>> PI-tracking performance time: 0.0 seconds (+/- 0.0)\n",
      ">>> Scheduler-update performance time: 0.0 seconds (+/- 0.0)\n",
      "Proportion of time consumed for Loading: 36.0%\n",
      "Proportion of time consumed for Forward: 21.6%\n",
      "Proportion of time consumed for Backward: 42.1%\n",
      "Proportion of time consumed for Plotting: 0.0%\n",
      "Proportion of time consumed for CheckPoint Saving: 0.2%\n",
      "Proportion of time consumed for Tracking PI: 0.0%\n",
      "Proportion of time consumed for Update Scheduler: 0.0%\n",
      "Proportion of time consumed for Read all data on GPU: 0.0%\n",
      "\n",
      "Max GPU memory allocated: 0.0 GB\n",
      "Max GPU memory cached: 0.0 GB\n",
      "Max CPU memory allocated: 0.348480224609375 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load config\n",
    "model_name = 'STGCN' #'CNN'\n",
    "netmob = True\n",
    "args = get_args(model_name)\n",
    "#args = get_args(model_name = model_name,learn_graph_structure = True)  # MTGNN\n",
    "\n",
    "# Modification : \n",
    "args.K_fold = 1\n",
    "args.ray = False\n",
    "args.epochs = 2\n",
    "args.loss_function_type = 'MSE'\n",
    "\n",
    "small_ds = False\n",
    "\n",
    "coverage = match_period_coverage_with_netmob(file_name)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    data_folder_path = '../../../data/' \n",
    "else:\n",
    "    data_folder_path = '../../Data/'\n",
    "\n",
    "(coverage,args) = get_small_ds(small_ds,coverage,args)\n",
    "dataset_names = ['subway_in']#['subway_in','netmob','calendar']\n",
    "\n",
    "args,subway_ds, positions,nb_words_embedding,dic_class2rpz = load_everything(dataset_names,folder_path,file_name,args,coverage,data_folder_path)\n",
    "# Load Model, Optimizer, Scheduler: \n",
    "loss_function,model,optimizer,scheduler,args_embedding = get_model_loss_args_emb_opts(args,nb_words_embedding,dic_class2rpz,n_vertex = subway_ds.raw_values.size(1))\n",
    "# Load trainer: \n",
    "trainer = Trainer(subway_ds,model,args,optimizer,loss_function,scheduler = None,args_embedding  =args_embedding,dic_class2rpz = dic_class2rpz,show_figure = True,positions = positions)# Ajoute dans trainer, if calibration_prop is not None .... et on modifie le dataloader en ajoutant un clabration set\n",
    "# Train Model \n",
    "trainer.train_and_valid(mod = 1000)  # Récupère les conformity scores sur I1, avec les estimations faites precedemment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\romai\\Code\\prediction-validation\\trainer.py:203: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  print(f\"epoch: {epoch} \\n min\\epoch : {'{0:.2f}'.format((time.time()-t0)/60)}\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m     Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      5\u001b[0m station \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 6\u001b[0m pi,pi_cqr \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_bokeh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdic_class2rpz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mstation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mshow_figure\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43msave_plot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\romai\\Code\\prediction-validation\\plotting_bokeh.py:15\u001b[0m, in \u001b[0;36mgenerate_bokeh\u001b[1;34m(trainer, data_loader, dataset, Q, args, dic_class2rpz, trial_id, trial_save, station, show_figure, save_plot, pos_calibration_calendar_class)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_bokeh\u001b[39m(trainer,data_loader,dataset,Q,args,dic_class2rpz,trial_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,trial_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m ,station\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,show_figure \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,save_plot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,pos_calibration_calendar_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 15\u001b[0m     pi,pi_cqr,p1 \u001b[38;5;241m=\u001b[39m \u001b[43mplot_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstation\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpos_calibration_calendar_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     p2 \u001b[38;5;241m=\u001b[39m plot_loss(trainer)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mtime_embedding:\n",
      "File \u001b[1;32mc:\\Users\\romai\\Code\\prediction-validation\\plotting_bokeh.py:180\u001b[0m, in \u001b[0;36mplot_prediction\u001b[1;34m(trainer, dataset, Q, args, station, pos_calibration_calendar_class, location)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_prediction\u001b[39m(trainer,dataset,Q,args,station,pos_calibration_calendar_class,location \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_right\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 180\u001b[0m     (preds,Y_true,T_labels) \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(preds\u001b[38;5;241m.\u001b[39msize()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    182\u001b[0m         preds \u001b[38;5;241m=\u001b[39m preds\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\romai\\Code\\prediction-validation\\trainer.py:545\u001b[0m, in \u001b[0;36mTrainer.testing\u001b[1;34m(self, allow_dropout, training_mode, X, Y_true, T_labels)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtesting\u001b[39m(\u001b[38;5;28mself\u001b[39m, allow_dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, training_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m,X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, Y_true \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, T_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m): \u001b[38;5;66;03m#metrics= ['mse','mae']\u001b[39;00m\n\u001b[1;32m--> 545\u001b[0m     (test_pred,Y_true,T_labels) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mallow_dropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtraining_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43mT_labels\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Get Normalized Pred and Y_true\u001b[39;00m\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;66;03m# Set feature_vect = True cause output last dimension = 2 if quantile_loss or = 1.\u001b[39;00m\n\u001b[0;32m    548\u001b[0m     test_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mnormalizer\u001b[38;5;241m.\u001b[39munormalize_tensor(inputs \u001b[38;5;241m=\u001b[39m test_pred,feature_vect \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m#  device = self.args.device\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\romai\\Code\\prediction-validation\\trainer.py:535\u001b[0m, in \u001b[0;36mTrainer.test_prediction\u001b[1;34m(self, allow_dropout, training_mode, X, Y_true, T_labels)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():       \n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 535\u001b[0m         data \u001b[38;5;241m=\u001b[39m [[x_b,y_b,contextual_b[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_calendar]] \u001b[38;5;28;01mfor\u001b[39;00m  x_b,y_b,contextual_b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader[training_mode]]\n\u001b[0;32m    536\u001b[0m         X,Y_true,T_labels\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x_b \u001b[38;5;28;01mfor\u001b[39;00m [x_b,_,_] \u001b[38;5;129;01min\u001b[39;00m data])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice),torch\u001b[38;5;241m.\u001b[39mcat([y_b \u001b[38;5;28;01mfor\u001b[39;00m [_,y_b,_] \u001b[38;5;129;01min\u001b[39;00m data])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice), torch\u001b[38;5;241m.\u001b[39mcat([t_b \u001b[38;5;28;01mfor\u001b[39;00m [_,_,t_b] \u001b[38;5;129;01min\u001b[39;00m data])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs_embedding : \n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "if args.loss_function_type == 'quantile':\n",
    "    Q = trainer.conformal_calibration(args.alpha,conformity_scores_type =args.conformity_scores_type, quantile_method = args.quantile_method)  # calibration for PI 90%\n",
    "else:\n",
    "    Q = None\n",
    "station = 0\n",
    "pi,pi_cqr = generate_bokeh(trainer,trainer.dataloader,\n",
    "                                    trainer.dataset,Q,args,trainer.dic_class2rpz,\n",
    "                                    station = station,\n",
    "                                    show_figure = True,\n",
    "                                    save_plot = False\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST avec plusieurs contextual Tensors: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_tensors = {'calendar': {'train': calendar_U_train,\n",
    "                                   'valid': calendar_U_valid,\n",
    "                                   'test': calendar_U_test},\n",
    "                      \n",
    "                                 }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essais avec NetMob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tackling Training Set\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "min(): Expected reduction dim 2 to have non-zero size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vc/gn83ctb97rd5lmlbh8djj9fr0000gr/T/ipykernel_1447/1599376891.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Split DataSet and Normalize accoding Stats from Training Set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_normalize_tensor_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;31m# Define DictDataLoader :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mload_normalize_tensor_datasets\u001b[0;34m(self, mini, maxi, mean, std, normalize, feature_vect)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminmaxnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_vect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_vect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mini'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mnormalize_tensor\u001b[0;34m(self, tensor, dims, minmaxnorm, standardize, reverse, feature_vect)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mini'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'maxi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'std'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mreshaped_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshaped_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mnormalized_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminmaxnorm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_vect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/dataset.py\u001b[0m in \u001b[0;36mget_stats\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;34m''' Return Min, Max, Mean and Std of inputs through the choosen dimension 'dims' (which have been flattened)'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mini'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0;31m#self.mini = inputs.min(dims).values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'maxi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: min(): Expected reduction dim 2 to have non-zero size."
     ]
    }
   ],
   "source": [
    "T = netmob_T.size(0)\n",
    "\n",
    "# Tackle a specific fold : \n",
    "netmob_T1 = netmob_T[:100]\n",
    "\n",
    "# Init :\n",
    "dims = [0,3,4]#[0,-2,-1]\n",
    "minmaxnorm = True\n",
    "standardize = False\n",
    "\n",
    "# ============ Load Train/Valid/Test Indices and removed forbidden dates : ============\n",
    "# invalid dates = \n",
    "# invalid_indices = get_indices_from_dates(invalid)\n",
    "indices = np.arange(T)\n",
    "np.random.shuffle(indices)\n",
    "invalid_indices = indices[:100]\n",
    "\n",
    "# Get Split indices :\n",
    "train_indices = np.arange(50)\n",
    "valid_indices = np.arange(60,70)\n",
    "test_indices = np.arange(80,100)\n",
    "\n",
    "# Remove invalid_dates from indices :\n",
    "cleaner = InvalidDatesCleaner(invalid_indices = invalid_indices)\n",
    "\n",
    "# >>>> Le cleaner prend un ensemble d'indice qui est interdit (les indices correspondant au dates qui sont interdites)\n",
    "# >>>> On lui passe ensemble un ensemble d'indices (ex : ceux qui correspondent à l'extraction de U_train)\n",
    "# >>>> Pour lequels il va masqué/supprimer les indices interdit\n",
    "# >>>> On ce sert ensuite de cet ensemble pour extraire le bon U_train, avec les valeurs qui sont effectivement interdite. \n",
    "\n",
    "\n",
    "train_indices = cleaner.clean_indices(train_indices)\n",
    "valid_indices = cleaner.clean_indices(valid_indices)\n",
    "test_indices = cleaner.clean_indices(test_indices)\n",
    "# ============ .......................................................... ============\n",
    "\n",
    "# Load Splitter Object\n",
    "splitter = TrainValidTest_Split_Normalize(netmob_T1,dims,train_indices, valid_indices, test_indices,minmaxnorm=minmaxnorm,standardize=standardize)\n",
    "\n",
    "# Split DataSet and Normalize accoding Stats from Training Set \n",
    "train_dataset,valid_dataset,test_dataset = splitter.load_normalize_tensor_datasets()\n",
    "# Define DictDataLoader :"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "preprocessingclone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
