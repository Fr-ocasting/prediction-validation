{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_netmob_data import load_subway_shp,load_netmob_gdf,find_ids_within_epsilon,tackle_all_days,build_image,get_station_data_and_permute_reshape\n",
    "from os import listdir\n",
    "import os \n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import pickle \n",
    "import os\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of NetMob Cell associated to a subway station: 318\n"
     ]
    }
   ],
   "source": [
    "data_folder_path = '../../../data/'\n",
    "save_folder = f\"{data_folder_path}NetMob_tensor/\"\n",
    "netmob_data_folder_path = f\"{data_folder_path}NetMob/\"\n",
    "step_south_north = 287  # Incremente by 287-ids when passing from south to north. \n",
    "epsilon=1000  #epsilon : radius, in meter (1000m) \n",
    "# W,H = 2*(epsilon//100 + 1), 2*(epsilon//100 + 1)\n",
    "\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "    \n",
    "# Load subway gdf adn NetMob gdf\n",
    "ref_subway = load_subway_shp(folder_path = data_folder_path)\n",
    "Netmob_gdf,working_zones = load_netmob_gdf(folder_path = netmob_data_folder_path,\n",
    "                             data_folder = '../Data/lyon_iris_shapefile/', \n",
    "                             geojson_path = 'Lyon.geojson',\n",
    "                             zones_path = 'lyon.shp')\n",
    "Netmob_gdf_dropped = Netmob_gdf.drop_duplicates(subset = ['tile_id'])  # Some Doubles are exis\n",
    "\n",
    "# Get Cell-Id within epsilon : \n",
    "result,joined = find_ids_within_epsilon(Netmob_gdf_dropped,ref_subway,epsilon=epsilon) \n",
    "maxi_nb_tile =  result.apply(lambda row: len(row.tile_id),axis=1).max()\n",
    "print(f\"Maximum number of NetMob Cell associated to a subway station: {maxi_nb_tile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joined.explore('COD_TRG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps = [app for app in listdir(netmob_data_folder_path) if ((app != 'Lyon.geojson') and (not app.startswith('.'))) ]   # Avoid hidden folder and Lyon.geojson\n",
    "Tensors = []\n",
    "# For each app\n",
    "if False:\n",
    "    for app in apps: \n",
    "        print('App: ',app)\n",
    "        metadata = {result['COD_TRG'][station_ind] : {} for station_ind in range(len(result))}\n",
    "        folder_days = [day for day in listdir(f'{netmob_data_folder_path}/{app}') if (not day.startswith('.')) ]\n",
    "        Tensors_days,metadata = tackle_all_days(result,metadata,netmob_data_folder_path,app,maxi_nb_tile,folder_days)\n",
    "        torch.save(Tensors_days,f\"{save_folder}{app}.pt\")\n",
    "        pickle.dump(metadata,open(f\"{save_folder}{app}_metadata.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tensor example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple_Video Tensor: torch.Size([77, 2, 43, 318, 96])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "app = 'Apple_Video'\n",
    "\n",
    "Apple_Video_meta = pickle.load(open(f\"{save_folder}{app}_metadata.pkl\",\"rb\"))\n",
    "Apple_Video = torch.load(f\"{save_folder}{app}.pt\")  #[day, transfer_mode, Station, Tile_id, (hour,minutes)]\n",
    "print(f\"{app} Tensor: {Apple_Video.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input adapted to station i:\n",
    "T = Apple_Video  #[Day, DL/UL, Nstation, Tile-id, (hour,minute)] \n",
    "metadata = Apple_Video_meta\n",
    "\n",
    "stations = list(metadata.keys())\n",
    "i = 0\n",
    "station = stations[i]\n",
    "tile_ids = np.array(metadata[station]['tile_id'])\n",
    "\n",
    "T_i = get_station_data_and_permute_reshape(T,i)\n",
    "resized_T_i = build_image(T_i,tile_ids,epsilon,step_south_north)\n",
    "#print(f\"Ti: {T_i.size()}\")\n",
    "#print(f\"resized_T_i: {resized_T_i.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for i in range(len(ref_subway)):\n",
    "        List_channel_of_station_i = []\n",
    "        for app in apps: \n",
    "\n",
    "            metadata = pickle.load(open(f\"{save_folder}{app}_metadata.pkl\",\"rb\"))\n",
    "            T = torch.load(f\"{save_folder}{app}.pt\")  #[day, transfer_mode, Station, Tile_id, (hour,minutes)]\n",
    "\n",
    "            stations = list(metadata.keys())\n",
    "            station = stations[i]\n",
    "            tile_ids = np.array(metadata[station]['tile_id'])\n",
    "\n",
    "            T_i = get_station_data_and_permute_reshape(T,i)\n",
    "            resized_T_i = build_image(T_i,tile_ids,epsilon,step_south_north)\n",
    "            List_channel_of_station_i.append(resized_T_i)\n",
    "\n",
    "\n",
    "        name_save = f\"station_{station}\"\n",
    "        print(name_save)\n",
    "        Station_i_with_all_channel = torch.cat(List_channel_of_station_i, dim=1)\n",
    "        torch.save(Station_i_with_all_channel,f\"{save_folder}{name_save}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7392, 136, 22, 22])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station = 'AMP'\n",
    "T_amp = torch.load(f\"{save_folder}station_{station}.pt\")  #[day, transfer_mode, Station, Tile_id, (hour,minutes)]\n",
    "T_amp.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader From NetMob Tensor  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7392, 43, 136, 22, 22])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataLoader ...\n",
    "netmob_T = torch.stack([torch.load(f\"{save_folder}station_{station}.pt\") for station in ref_subway.COD_TRG])\n",
    "netmob_T = netmob_T.permute(1,0,*range(2, netmob_T.dim()))\n",
    "netmob_T.size()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Trafic DataSet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Revoir le 'get_DataSet_and_invalid_dates' pour prendre en compte directement la periode qui nous intéresse.\n",
    " - Gerer le split K-fold pour K > 1, essayer de sortir directement la dataset NetMob découpée\n",
    " - Gerer que le découpage Calibration (random choice) doit être le même pour toute les dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Hyper-parameter tuning with Ray is not possible\n",
      "coverage period: 2019-01-01 00:00:00 - 2020-01-01 00:00:00\n",
      "Time-step per hour: 4.0\n",
      "coverage period: 2019-01-01 00:00:00 - 2020-01-01 00:00:00\n",
      "Time-step per hour: 4.0\n"
     ]
    }
   ],
   "source": [
    "from paths import folder_path,file_name\n",
    "from config import get_args\n",
    "from utilities_DL import get_DataSet_and_invalid_dates,match_period_coverage\n",
    "# Load config\n",
    "model_name = 'STGCN' #'CNN'\n",
    "netmob = True\n",
    "args = get_args(model_name)\n",
    "#args = get_args(model_name = model_name,learn_graph_structure = True)  # MTGNN\n",
    "\n",
    "# Modification : \n",
    "args.K_fold = 1\n",
    "args.ray = False\n",
    "\n",
    "# Load Init DataSet \n",
    "dataset,invalid_dates = get_DataSet_and_invalid_dates(args.abs_path, folder_path,file_name,\n",
    "                                                      args.W,args.D,args.H,args.step_ahead,\n",
    "                                                      single_station = False,coverage_period = None)\n",
    "\n",
    "# Get coverage period matching with NetMob and Traffic data\n",
    "coverage = match_period_coverage(dataset,netmob_T)\n",
    "\n",
    "\n",
    "# Load Restricted Dataset: \n",
    "dataset,invalid_dates = get_DataSet_and_invalid_dates(args.abs_path, folder_path,file_name,\n",
    "                                                      args.W,args.D,args.H,args.step_ahead,\n",
    "                                                      single_station = False,coverage_period = coverage)\n",
    "\n",
    "(Datasets,DataLoader_list,time_slots_labels_list,dic_class2rpz_list,dic_rpz2class_list,nb_words_embedding_list) =  dataset.split_K_fold(args,invalid_dates,netmob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DL_class import DataSet \n",
    "\n",
    "class NetMob_dataset(DataSet): #object\n",
    "    def __init__(self,U,args):\n",
    "        super(NetMob_dataset,self).__init__(pd.DataFrame())\n",
    "        self.args = args \n",
    "        self.U = U\n",
    "        \n",
    "    def get_splits_limits(self,dataset):\n",
    "        self.first_train_U  = dataset.first_train_U\n",
    "        self.last_train_U = dataset.last_train_U\n",
    "        \n",
    "        self.first_valid_U  = dataset.first_valid_U\n",
    "        self.last_valid_U = dataset.last_valid_U\n",
    "        \n",
    "        self.first_test_U = dataset.first_test_U\n",
    "        self.last_test_U = dataset.last_test_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U: torch.Size([7392, 43, 136, 22, 22])\n",
      "U train:  torch.Size([2934, 43, 136, 22, 22])\n",
      "U valid:  torch.Size([978, 43, 136, 22, 22])\n",
      "No Test set\n"
     ]
    }
   ],
   "source": [
    "# ici on récupère les train/valid/test split depuis l'objet 'Dataset'.\n",
    "# plusieurs problèmes:\n",
    "#\n",
    "#  !!!! - DataSet utilise d'abord un K-fold Split.  Et génère K Dataset. \n",
    "#        les splits limits sont données respectivement aux tailles des K datasets.\n",
    "#\n",
    "#  !!!! - Lorsqu'on génère le DataLoader avec DataSet, il y a un random-split pour la calibration set. \n",
    "#         Il faut que le split soit le même pour dataset et Netmob-dataset\n",
    "#\n",
    "#       - La couverture temporelle de dataset doit exactement matcher celle de dataset-netmob. \n",
    "#               -> Fait avec match_period_coverage(dataset,netmob_T)\n",
    "\n",
    "netmob_dataset = NetMob_dataset(netmob_T,args)\n",
    "netmob_dataset.get_splits_limits(Datasets[0])\n",
    "netmob_dataset.split_tensors()\n",
    "print('U:',netmob_dataset.U.size())\n",
    "print('U train: ',netmob_dataset.U_train.size())\n",
    "if hasattr(netmob_dataset,'U_valid'): \n",
    "    print('U valid: ',netmob_dataset.U_valid.size()) \n",
    "else:\n",
    "    print('No Validation set')\n",
    "if hasattr(netmob_dataset.U_test,'size'):\n",
    "    print('U test: ',netmob_dataset.U_test.size()) \n",
    "else:\n",
    "    print('No Test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DL_class import DictDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NetMob_Loader = DictDataLoader(netmob_dataset,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NetMob_dataset' object has no attribute 'Utarget_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m netmob_loader \u001b[38;5;241m=\u001b[39m \u001b[43mNetMob_Loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dictdataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/rrochas/uncertainty_quantification/DL_class.py:100\u001b[0m, in \u001b[0;36mDictDataLoader.get_dictdataloader\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mindices_cal \u001b[38;5;241m=\u001b[39m indices[split:]\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mindices_train \u001b[38;5;241m=\u001b[39m indices[:split]\n\u001b[0;32m--> 100\u001b[0m proper_set_x,proper_set_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mU_train[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mindices_train],\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUtarget_train\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mindices_train]\n\u001b[1;32m    101\u001b[0m calib_set_x,calib_set_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mU_train[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mindices_cal],\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mUtarget_train[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mindices_cal]\n\u001b[1;32m    102\u001b[0m time_slots_proper \u001b[38;5;241m=\u001b[39m {calendar_class: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mtime_slots_train[calendar_class][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mindices_train] \u001b[38;5;28;01mfor\u001b[39;00m calendar_class \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mnb_class)) } \n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NetMob_dataset' object has no attribute 'Utarget_train'"
     ]
    }
   ],
   "source": [
    "netmob_loader = NetMob_Loader.get_dictdataloader(args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(0, 7392, 136, 22, 22))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netmob_dataset.U_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Model: Capture NetMob Feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" A utiliser lorsqu'on a l'input Video de dimension B,T,C,H,W: \""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dl_models.ResNet_Encoder import ResnetEncoder,VPTREnc\n",
    "import torch.nn as nn \n",
    "''' Les Kernel size sont fixé pour de la vision prediction. A voir si c'est le plus pertinent, sans doute pas. \n",
    "Il n'y a probablement pas les mêmes régularités dans les images (ombrage, segmentation etc) que dans des données NetMob. '''\n",
    "T = T_amp[:32,:,:,:]\n",
    "B,C,H,W = T.size()\n",
    "ngf = 64  # the number of filters in the last conv layer#\n",
    "out_dim = 16 #256\n",
    "use_dropout = False \n",
    "\n",
    "padding_type='reflect' # ???\n",
    "n_downsampling = 3 # 1 -> H=W=11 / 2 -> H=W=6 / 3 -> H=W=3 /  4 -> H=W=2 ???\n",
    "n_resnet_blocks = 6 # 9\n",
    "#Init Model :\n",
    "resnet_encoder = ResnetEncoder(C, ngf, out_dim, n_downsampling = n_downsampling, norm_layer=nn.BatchNorm2d, use_dropout=use_dropout, padding_type=padding_type, n_resnet_blocks=n_resnet_blocks)\n",
    "''' A utiliser lorsqu'on a l'input Video de dimension B,T,C,H,W: '''\n",
    "#resnet_encoder = VPTREnc(C, ngf, out_dim, n_downsampling = n_downsampling, norm_layer=nn.BatchNorm2d, use_dropout=use_dropout, padding_type=padding_type, n_resnet_blocks=n_resnet_blocks)\n",
    "\n",
    "# On devrait probablement réduire le nombre de Channels, Puis flatten tout ça ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 6, 22, 22])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.randn(40,64,6,22,22)\n",
    "inputs[2].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Feature: torch.Size([32, 16, 3, 3]). Extracted feature resized: torch.Size([32, 144])\n"
     ]
    }
   ],
   "source": [
    "feat = resnet_encoder(T)\n",
    "flattened_feat = feat.flatten(start_dim = 1)\n",
    "\n",
    "print(f\"Extracted Feature: {feat.size()}. Extracted feature resized: {flattened_feat.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parallel_ResnetEncoder(nn.Module):\n",
    "    def __init__(self,stations,C, ngf, out_dim, n_downsampling, norm_layer, use_dropout, padding_type, n_resnet_blocks):\n",
    "        self.n_station = len(stations)\n",
    "        super(Parallel_ResnetEncoder,self).__init__(self)\n",
    "        self.parallel_models = nn.ModuleList([ResnetEncoder(C, ngf, out_dim, n_downsampling, norm_layer, use_dropout, padding_type, n_resnet_blocks) for _ in range(self.n_station)])\n",
    "        \n",
    "        \n",
    "    def forward(self,X_NetMob):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_NetMob --- (B,N, T, C, H, W)\n",
    "        Returns:\n",
    "            feat --- (B,N,Z)   \n",
    "            \n",
    "        B: Batch size\n",
    "        N: Number of spatial-units\n",
    "        T: Historical length \n",
    "        C: Init Channel (DL/UL *  Number of Apps)\n",
    "        H,W : Height and Width of NetMob Image \n",
    "        \n",
    "        Z: Expected Feature dim  (Latent dim)\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        X_NetMob = X_NetMob.permute(1, 0, *range(2, X_NetMob.dim()))  # (N,B, T, C, H, W)\n",
    "        \n",
    "        for k in range(self.n_station):\n",
    "            out = self.parallel_models[k](X_NetMob[k]) # (B, T, C, H, W) ->  (B, T, C_out, H_out, W_out)\n",
    "            outputs.append(out)\n",
    "        outputs = torch.stack(outputs)  # (N,B, T, C_out, H_out, W_out)\n",
    "        \n",
    "        outputs = outputs.flatten(start_dim = 2)  # (N,B, Z)   with Z = T*C_out*H_out*W_out\n",
    "        outputs = outputs.permute(1,0,2    # (B,N,Z)\n",
    "        return(outputs)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Risque d'être long. Peut-être qu'on va devoir commencer par un model commun au stations. Mais probablement pas idéal..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-2.0.1_py-3.10.5]",
   "language": "python",
   "name": "conda-env-pytorch-2.0.1_py-3.10.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
