{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_netmob_data import load_subway_shp,load_netmob_gdf,find_ids_within_epsilon,tackle_all_days,build_image,get_station_data_and_permute_reshape\n",
    "from loader import DictDataLoader\n",
    "from os import listdir\n",
    "import os \n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import pickle \n",
    "import os\n",
    "import numpy as np \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    data_folder_path = '../../../data/' \n",
    "else:\n",
    "    data_folder_path = '../../Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "../Data/lyon_iris_shapefile/lyon.shp: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_err.pyx\u001b[0m in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: ../Data/lyon_iris_shapefile/lyon.shp: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vc/gn83ctb97rd5lmlbh8djj9fr0000gr/T/ipykernel_3451/3221868070.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                              \u001b[0mdata_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../Data/lyon_iris_shapefile/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                              \u001b[0mgeojson_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Lyon.geojson'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                              zones_path = 'lyon.shp')\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mNetmob_gdf_dropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetmob_gdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'tile_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Some Doubles are exis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/load_netmob_data.py\u001b[0m in \u001b[0;36mload_netmob_gdf\u001b[0;34m(folder_path, data_folder, geojson_path, zones_path)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_netmob_gdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../Data/NetMob/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../Data/lyon_iris_shapefile/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeojson_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Lyon.geojson'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mzones_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'lyon.shp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mNetmob_gdf_joined\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mworking_zones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_netmob_restrained_to_lyon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgeojson_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mzones_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0mNetmob_gdf_joined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetmob_gdf_joined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tile_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'INSEE_COM'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NOM_COM'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NOM_IRIS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'geometry'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mNetmob_gdf_joined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeoDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetmob_gdf_joined\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'EPSG:4326'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Don't know why, but the geodataframe seem corrupted as we can't convert it into GeoJson, that's why we need to use \"gpd.GeoDataFrame()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/load_netmob_data.py\u001b[0m in \u001b[0;36mload_netmob_restrained_to_lyon\u001b[0;34m(folder_path, data_folder, geojson_path, zones_path)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0mNetmob_gdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'centroid_lonlat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetmob_gdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcentroid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0mNetmob_gdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetmob_gdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_geometry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'centroid_lonlat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0mNetmob_gdf_joined\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mworking_zones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestrain_netmob_to_Lyon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetmob_gdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mzones_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Associate an square to an IRIS when the centroid is inside it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetmob_gdf_joined\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mworking_zones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/load_netmob_data.py\u001b[0m in \u001b[0;36mrestrain_netmob_to_Lyon\u001b[0;34m(Netmob_gdf, folder_path, zones_path)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrestrain_netmob_to_Lyon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetmob_gdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mzones_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;34m''' Restraint \"Netmob_gdf\" to the working area '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0mworking_zones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{folder_path}{zones_path}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0mNetmob_gdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeoDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetmob_gdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0mNetmob_gdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'epsg:4326'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/preprocessingclone/lib/python3.7/site-packages/geopandas/io/file.py\u001b[0m in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfiona_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# In a future Fiona release the crs attribute of features will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/preprocessingclone/lib/python3.7/site-packages/fiona/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/preprocessingclone/lib/python3.7/site-packages/fiona/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             c = Collection(path, mode, driver=driver, encoding=encoding,\n\u001b[0;32m--> 265\u001b[0;31m                            layer=layer, enabled_drivers=enabled_drivers, **kwargs)\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/preprocessingclone/lib/python3.7/site-packages/fiona/collection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWritingSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDriverError\u001b[0m: ../Data/lyon_iris_shapefile/lyon.shp: No such file or directory"
     ]
    }
   ],
   "source": [
    "save_folder = f\"{data_folder_path}NetMob_tensor/\"\n",
    "netmob_data_folder_path = f\"{data_folder_path}NetMob/\"\n",
    "step_south_north = 287  # Incremente by 287-ids when passing from south to north. \n",
    "epsilon=1000  #epsilon : radius, in meter (1000m) \n",
    "# W,H = 2*(epsilon//100 + 1), 2*(epsilon//100 + 1)\n",
    "\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "    \n",
    "# Load subway gdf adn NetMob gdf\n",
    "ref_subway = load_subway_shp(folder_path = data_folder_path)\n",
    "Netmob_gdf,working_zones = load_netmob_gdf(folder_path = netmob_data_folder_path,\n",
    "                             data_folder = '../Data/lyon_iris_shapefile/', \n",
    "                             geojson_path = 'Lyon.geojson',\n",
    "                             zones_path = 'lyon.shp')\n",
    "Netmob_gdf_dropped = Netmob_gdf.drop_duplicates(subset = ['tile_id'])  # Some Doubles are exis\n",
    "\n",
    "# Get Cell-Id within epsilon : \n",
    "result,joined = find_ids_within_epsilon(Netmob_gdf_dropped,ref_subway,epsilon=epsilon) \n",
    "maxi_nb_tile =  result.apply(lambda row: len(row.tile_id),axis=1).max()\n",
    "print(f\"Maximum number of NetMob Cell associated to a subway station: {maxi_nb_tile}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max ou Std Norm sur une image. \n",
    "-> Standaridsation sur un ensemble de dim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'pynvml' is not available on this environment.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../Data/NetMob_tensor/station_AMP.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vc/gn83ctb97rd5lmlbh8djj9fr0000gr/T/ipykernel_3233/3463506229.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load NetMob Data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnetmob_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{save_folder}station_{station}.pt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mref_subway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOD_TRG\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mnetmob_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetmob_T\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetmob_T\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Init NetMob Dataset: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetmob_T\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vc/gn83ctb97rd5lmlbh8djj9fr0000gr/T/ipykernel_3233/3463506229.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load NetMob Data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnetmob_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{save_folder}station_{station}.pt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mref_subway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOD_TRG\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mnetmob_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetmob_T\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetmob_T\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Init NetMob Dataset: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetmob_T\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/preprocessingclone/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/preprocessingclone/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/preprocessingclone/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../Data/NetMob_tensor/station_AMP.pt'"
     ]
    }
   ],
   "source": [
    "from DL_class import TrainValidTest_Split_Normalize\n",
    "import torch \n",
    "\n",
    "# NetMob Tensor : [T,N,C,H,W]\n",
    "# dims : [0,-2,-1]  -> dimension for which we want to retrieve stats \n",
    "\n",
    "# Load NetMob Data:\n",
    "netmob_T = torch.stack([torch.load(f\"{save_folder}station_{station}.pt\") for station in ref_subway.COD_TRG])\n",
    "netmob_T = netmob_T.permute(1,0,*range(2, netmob_T.dim()))\n",
    "print('Init NetMob Dataset: ', netmob_T.size())\n",
    "print('Number of Nan Value: ',torch.isnan(netmob_T).sum())\n",
    "print('Total Number of Elements: ', netmob_T.numel() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tackle a specific fold : \n",
    "netmob_T1 = netmob_T[:100]\n",
    "\n",
    "# Init :\n",
    "dims = [0,-2,-1]\n",
    "minmaxnorm = True\n",
    "standardize = False\n",
    "\n",
    "# Get Split indices :\n",
    "train_indices = np.arange(50)\n",
    "valid_indices = np.arange(60,70)\n",
    "test_indices = np.arange(80,100)\n",
    "\n",
    "# Load Splitter Object\n",
    "splitter = TrainValidTest_Split_Normalize(netmob_T1,dims,train_indices, valid_indices, test_indices,minmaxnorm,standardize)\n",
    "\n",
    "# Split DataSet and Normalize accoding Stats from Training Set \n",
    "train_dataset,valid_dataset,test_dataset = splitter.load_normalize_tensor_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation des K-fold Split et des Train/Valid/Test Split récupéré sur la SubwayData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joined.explore('COD_TRG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps = [app for app in listdir(netmob_data_folder_path) if ((app != 'Lyon.geojson') and (not app.startswith('.'))) ]   # Avoid hidden folder and Lyon.geojson\n",
    "Tensors = []\n",
    "# For each app\n",
    "if False:\n",
    "    for app in apps: \n",
    "        print('App: ',app)\n",
    "        metadata = {result['COD_TRG'][station_ind] : {} for station_ind in range(len(result))}\n",
    "        folder_days = [day for day in listdir(f'{netmob_data_folder_path}/{app}') if (not day.startswith('.')) ]\n",
    "        Tensors_days,metadata = tackle_all_days(result,metadata,netmob_data_folder_path,app,maxi_nb_tile,folder_days)\n",
    "        torch.save(Tensors_days,f\"{save_folder}{app}.pt\")\n",
    "        pickle.dump(metadata,open(f\"{save_folder}{app}_metadata.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tensor example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple_Video Tensor: torch.Size([77, 2, 43, 318, 96])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "app = 'Apple_Video'\n",
    "\n",
    "Apple_Video_meta = pickle.load(open(f\"{save_folder}{app}_metadata.pkl\",\"rb\"))\n",
    "Apple_Video = torch.load(f\"{save_folder}{app}.pt\")  #[day, transfer_mode, Station, Tile_id, (hour,minutes)]\n",
    "print(f\"{app} Tensor: {Apple_Video.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input adapted to station i:\n",
    "T = Apple_Video  #[Day, DL/UL, Nstation, Tile-id, (hour,minute)] \n",
    "metadata = Apple_Video_meta\n",
    "\n",
    "stations = list(metadata.keys())\n",
    "i = 0\n",
    "station = stations[i]\n",
    "tile_ids = np.array(metadata[station]['tile_id'])\n",
    "\n",
    "T_i = get_station_data_and_permute_reshape(T,i)\n",
    "resized_T_i = build_image(T_i,tile_ids,epsilon,step_south_north)\n",
    "#print(f\"Ti: {T_i.size()}\")\n",
    "#print(f\"resized_T_i: {resized_T_i.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for i in range(len(ref_subway)):\n",
    "        List_channel_of_station_i = []\n",
    "        for app in apps: \n",
    "\n",
    "            metadata = pickle.load(open(f\"{save_folder}{app}_metadata.pkl\",\"rb\"))\n",
    "            T = torch.load(f\"{save_folder}{app}.pt\")  #[day, transfer_mode, Station, Tile_id, (hour,minutes)]\n",
    "\n",
    "            stations = list(metadata.keys())\n",
    "            station = stations[i]\n",
    "            tile_ids = np.array(metadata[station]['tile_id'])\n",
    "\n",
    "            T_i = get_station_data_and_permute_reshape(T,i)\n",
    "            resized_T_i = build_image(T_i,tile_ids,epsilon,step_south_north)\n",
    "            List_channel_of_station_i.append(resized_T_i)\n",
    "\n",
    "\n",
    "        name_save = f\"station_{station}\"\n",
    "        print(name_save)\n",
    "        Station_i_with_all_channel = torch.cat(List_channel_of_station_i, dim=1)\n",
    "        torch.save(Station_i_with_all_channel,f\"{save_folder}{name_save}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7392, 136, 22, 22])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station = 'AMP'\n",
    "T_amp = torch.load(f\"{save_folder}station_{station}.pt\")  #[day, transfer_mode, Station, Tile_id, (hour,minutes)]\n",
    "T_amp.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader From NetMob Tensor  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7392, 43, 136, 22, 22])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataLoader ...\n",
    "netmob_T = torch.stack([torch.load(f\"{save_folder}station_{station}.pt\") for station in ref_subway.COD_TRG])\n",
    "netmob_T = netmob_T.permute(1,0,*range(2, netmob_T.dim()))\n",
    "netmob_T.size()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Trafic DataSet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Revoir le 'get_DataSet_and_invalid_dates' pour prendre en compte directement la periode qui nous intéresse.\n",
    " - Gerer le split K-fold pour K > 1, essayer de sortir directement la dataset NetMob découpée\n",
    " - Gerer que le découpage Calibration (random choice) doit être le même pour toute les dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version 1.7.1 does not allow you to use lr-scheduler\n",
      "'pynvml' is not available on this environment.\n",
      "Time-step per hour: 4.0\n",
      "coverage period: 2019-01-01 00:00:00 - 2020-01-01 00:00:00\n",
      "Time-step per hour: 4.0\n",
      "coverage period: 2019-03-16 00:00:00 - 2019-05-31 23:45:00\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataSet' object has no attribute 'contextual_tensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vc/gn83ctb97rd5lmlbh8djj9fr0000gr/T/ipykernel_3375/2682348329.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m                                                       single_station = False,coverage_period = coverage)\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mDatasets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDataLoader_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime_slots_labels_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdic_class2rpz_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdic_rpz2class_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_words_embedding_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_K_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minvalid_dates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnetmob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/DL_class.py\u001b[0m in \u001b[0;36msplit_K_fold\u001b[0;34m(self, args, invalid_dates, netmob)\u001b[0m\n\u001b[1;32m    875\u001b[0m                                    step_ahead=self.step_ahead,time_step_per_hour=self.time_step_per_hour)\n\u001b[1;32m    876\u001b[0m         \u001b[0mdataset_init\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset_save_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset_get_save_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnetmob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetmob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m         \u001b[0mdata_loader_with_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_init\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_normalize_load_feature_vect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minvalid_dates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_prop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_prop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_prop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    878\u001b[0m         \u001b[0;31m# Fait la 'Hold-Out' séparation, pour enlever les dernier mois de TesT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mdf_hold_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdataset_init\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_test_date\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/DL_class.py\u001b[0m in \u001b[0;36msplit_normalize_load_feature_vect\u001b[0;34m(self, args, invalid_dates, train_prop, valid_prop, test_prop)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;31m# Split U in  U_train, U_valid, U_test thanks to 'df_verif' and the date limits of the df_train/df_valid/df_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;31m# ADD self.contextual_tensor_train/valid/test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;31m#   DataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Codes/Cleaned_Code/prediction_validation/DL_class.py\u001b[0m in \u001b[0;36msplit_tensors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m         \u001b[0;31m# Contextual Tensor:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontextual_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextual_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{name}_train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontextual_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_train_U\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_train_U\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{name}_valid\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontextual_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_valid_U\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_valid_U\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_valid_U\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataSet' object has no attribute 'contextual_tensors'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    netmob_T\n",
    "except:\n",
    "    netmob_T = torch.randn(200,40,3,22,22)\n",
    "\n",
    "from paths import folder_path,file_name\n",
    "from config import get_args\n",
    "from utilities_DL import get_DataSet_and_invalid_dates,match_period_coverage_with_netmob\n",
    "# Load config\n",
    "model_name = 'STGCN' #'CNN'\n",
    "netmob = True\n",
    "args = get_args(model_name)\n",
    "#args = get_args(model_name = model_name,learn_graph_structure = True)  # MTGNN\n",
    "\n",
    "# Modification : \n",
    "args.K_fold = 1\n",
    "args.ray = False\n",
    "\n",
    "# Load Init DataSet \n",
    "dataset,invalid_dates = get_DataSet_and_invalid_dates(args.abs_path, folder_path,file_name,\n",
    "                                                      args.W,args.D,args.H,args.step_ahead,\n",
    "                                                      single_station = False,coverage_period = None)\n",
    "\n",
    "# Get coverage period matching with NetMob and Traffic data\n",
    "coverage = match_period_coverage_with_netmob(dataset)\n",
    "\n",
    "\n",
    "# Load Restricted Dataset: \n",
    "dataset,invalid_dates = get_DataSet_and_invalid_dates(args.abs_path, folder_path,file_name,\n",
    "                                                      args.W,args.D,args.H,args.step_ahead,\n",
    "                                                      single_station = False,coverage_period = coverage)\n",
    "\n",
    "(Datasets,DataLoader_list,time_slots_labels_list,dic_class2rpz_list,dic_rpz2class_list,nb_words_embedding_list) =  dataset.split_K_fold(args,invalid_dates,netmob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DL_class import DataSet \n",
    "\n",
    "class NetMob_dataset(DataSet): #object\n",
    "    def __init__(self,U,args):\n",
    "        super(NetMob_dataset,self).__init__(pd.DataFrame())\n",
    "        self.args = args \n",
    "        self.U = U\n",
    "        \n",
    "    def get_splits_limits(self,dataset):\n",
    "        self.first_train_U  = dataset.first_train_U\n",
    "        self.last_train_U = dataset.last_train_U\n",
    "        \n",
    "        self.first_valid_U  = dataset.first_valid_U\n",
    "        self.last_valid_U = dataset.last_valid_U\n",
    "        \n",
    "        self.first_test_U = dataset.first_test_U\n",
    "        self.last_test_U = dataset.last_test_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U: torch.Size([7392, 43, 136, 22, 22])\n",
      "U train:  torch.Size([2934, 43, 136, 22, 22])\n",
      "U valid:  torch.Size([978, 43, 136, 22, 22])\n",
      "No Test set\n"
     ]
    }
   ],
   "source": [
    "# ici on récupère les train/valid/test split depuis l'objet 'Dataset'.\n",
    "# plusieurs problèmes:\n",
    "#\n",
    "#  !!!! - DataSet utilise d'abord un K-fold Split.  Et génère K Dataset. \n",
    "#        les splits limits sont données respectivement aux tailles des K datasets.\n",
    "#\n",
    "#  !!!! - Lorsqu'on génère le DataLoader avec DataSet, il y a un random-split pour la calibration set. \n",
    "#         Il faut que le split soit le même pour dataset et Netmob-dataset\n",
    "#\n",
    "#       - La couverture temporelle de dataset doit exactement matcher celle de dataset-netmob. \n",
    "#               -> Fait avec match_period_coverage(dataset,netmob_T)\n",
    "\n",
    "netmob_dataset = NetMob_dataset(netmob_T,args)\n",
    "netmob_dataset.get_splits_limits(Datasets[0])\n",
    "netmob_dataset.split_tensors()\n",
    "print('U:',netmob_dataset.U.size())\n",
    "print('U train: ',netmob_dataset.U_train.size())\n",
    "if hasattr(netmob_dataset,'U_valid'): \n",
    "    print('U valid: ',netmob_dataset.U_valid.size()) \n",
    "else:\n",
    "    print('No Validation set')\n",
    "if hasattr(netmob_dataset.U_test,'size'):\n",
    "    print('U test: ',netmob_dataset.U_test.size()) \n",
    "else:\n",
    "    print('No Test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NetMob_Loader = DictDataLoader(netmob_dataset,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NetMob_dataset' object has no attribute 'Utarget_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m netmob_loader \u001b[38;5;241m=\u001b[39m \u001b[43mNetMob_Loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dictdataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/rrochas/uncertainty_quantification/DL_class.py:100\u001b[0m, in \u001b[0;36mDictDataLoader.get_dictdataloader\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mindices_cal \u001b[38;5;241m=\u001b[39m indices[split:]\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mindices_train \u001b[38;5;241m=\u001b[39m indices[:split]\n\u001b[0;32m--> 100\u001b[0m proper_set_x,proper_set_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mU_train[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mindices_train],\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUtarget_train\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mindices_train]\n\u001b[1;32m    101\u001b[0m calib_set_x,calib_set_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mU_train[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mindices_cal],\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mUtarget_train[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mindices_cal]\n\u001b[1;32m    102\u001b[0m time_slots_proper \u001b[38;5;241m=\u001b[39m {calendar_class: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mtime_slots_train[calendar_class][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mindices_train] \u001b[38;5;28;01mfor\u001b[39;00m calendar_class \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mnb_class)) } \n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NetMob_dataset' object has no attribute 'Utarget_train'"
     ]
    }
   ],
   "source": [
    "netmob_loader = NetMob_Loader.get_dictdataloader(args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(0, 7392, 136, 22, 22))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netmob_dataset.U_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Model: Capture NetMob Feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" A utiliser lorsqu'on a l'input Video de dimension B,T,C,H,W: \""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dl_models.ResNet_Encoder import ResnetEncoder,VPTREnc\n",
    "import torch.nn as nn \n",
    "''' Les Kernel size sont fixé pour de la vision prediction. A voir si c'est le plus pertinent, sans doute pas. \n",
    "Il n'y a probablement pas les mêmes régularités dans les images (ombrage, segmentation etc) que dans des données NetMob. '''\n",
    "T = T_amp[:32,:,:,:]\n",
    "B,C,H,W = T.size()\n",
    "ngf = 64  # the number of filters in the last conv layer#\n",
    "out_dim = 16 #256\n",
    "use_dropout = False \n",
    "\n",
    "padding_type='reflect' # ???\n",
    "n_downsampling = 3 # 1 -> H=W=11 / 2 -> H=W=6 / 3 -> H=W=3 /  4 -> H=W=2 ???\n",
    "n_resnet_blocks = 6 # 9\n",
    "#Init Model :\n",
    "resnet_encoder = ResnetEncoder(C, ngf, out_dim, n_downsampling = n_downsampling, norm_layer=nn.BatchNorm2d, use_dropout=use_dropout, padding_type=padding_type, n_resnet_blocks=n_resnet_blocks)\n",
    "''' A utiliser lorsqu'on a l'input Video de dimension B,T,C,H,W: '''\n",
    "#resnet_encoder = VPTREnc(C, ngf, out_dim, n_downsampling = n_downsampling, norm_layer=nn.BatchNorm2d, use_dropout=use_dropout, padding_type=padding_type, n_resnet_blocks=n_resnet_blocks)\n",
    "\n",
    "# On devrait probablement réduire le nombre de Channels, Puis flatten tout ça ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 6, 22, 22])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.randn(40,64,6,22,22)\n",
    "inputs[2].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Feature: torch.Size([32, 16, 3, 3]). Extracted feature resized: torch.Size([32, 144])\n"
     ]
    }
   ],
   "source": [
    "feat = resnet_encoder(T)\n",
    "flattened_feat = feat.flatten(start_dim = 1)\n",
    "\n",
    "print(f\"Extracted Feature: {feat.size()}. Extracted feature resized: {flattened_feat.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parallel_ResnetEncoder(nn.Module):\n",
    "    def __init__(self,stations,C, ngf, out_dim, n_downsampling, norm_layer, use_dropout, padding_type, n_resnet_blocks):\n",
    "        self.n_station = len(stations)\n",
    "        super(Parallel_ResnetEncoder,self).__init__(self)\n",
    "        self.parallel_models = nn.ModuleList([ResnetEncoder(C, ngf, out_dim, n_downsampling, norm_layer, use_dropout, padding_type, n_resnet_blocks) for _ in range(self.n_station)])\n",
    "        \n",
    "        \n",
    "    def forward(self,X_NetMob):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_NetMob --- (B,N, T, C, H, W)\n",
    "        Returns:\n",
    "            feat --- (B,N,Z)   \n",
    "            \n",
    "        B: Batch size\n",
    "        N: Number of spatial-units\n",
    "        T: Historical length \n",
    "        C: Init Channel (DL/UL *  Number of Apps)\n",
    "        H,W : Height and Width of NetMob Image \n",
    "        \n",
    "        Z: Expected Feature dim  (Latent dim)\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        X_NetMob = X_NetMob.permute(1, 0, *range(2, X_NetMob.dim()))  # (N,B, T, C, H, W)\n",
    "        \n",
    "        for k in range(self.n_station):\n",
    "            out = self.parallel_models[k](X_NetMob[k]) # (B, T, C, H, W) ->  (B, T, C_out, H_out, W_out)\n",
    "            outputs.append(out)\n",
    "        outputs = torch.stack(outputs)  # (N,B, T, C_out, H_out, W_out)\n",
    "        \n",
    "        outputs = outputs.flatten(start_dim = 2)  # (N,B, Z)   with Z = T*C_out*H_out*W_out\n",
    "        outputs = outputs.permute(1,0,2    # (B,N,Z)\n",
    "        return(outputs)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Risque d'être long. Peut-être qu'on va devoir commencer par un model commun au stations. Mais probablement pas idéal..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "preprocessingclone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
