{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os \n",
    "import sys \n",
    "from torch.optim import SGD \n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "from load_DataSet_subway_15 import replace_negative, load_data_and_pivot, load_normalized_dataset,load_subway_shp\n",
    "\n",
    "# Data Loader\n",
    "from DL_utilities import DictDataLoader,Trainer\n",
    "from utilities import evaluate_metrics\n",
    "\n",
    "# Load Model \n",
    "notebook_dir = os.getcwd()\n",
    "code_dir = os.path.abspath(os.path.join(notebook_dir, '../'))\n",
    "if code_dir not in sys.path:\n",
    "    sys.path.insert(0,code_dir)\n",
    "\n",
    "try : \n",
    "    from Ray.Ray_tune.dl_models.CNN_based_model import cnn_perso\n",
    "    from Ray.Ray_tune.dl_models.RNN_based_model import rnn_perso\n",
    "    from Ray.Ray_tune.dl_models.GCN_based_model import graphconv\n",
    "except : \n",
    "    from Ray_tune.dl_models.CNN_based_model import cnn_perso\n",
    "    from Ray_tune.dl_models.RNN_based_model import rnn_perso\n",
    "    from Ray_tune.dl_models.GCN_based_model import graphconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_DataSet_subway_15 import load_adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of time-slot: 7392\n"
     ]
    }
   ],
   "source": [
    "# Init\n",
    "folder_path = 'data/'\n",
    "file_name = 'Metro_15min_mar2019_mai2019.csv'\n",
    "station_location_name = 'ref_subway.csv'\n",
    "time_step_per_hour=4\n",
    "H,W,D = 6,1,1\n",
    "step_ahead = 1\n",
    "train_prop = 0.6\n",
    "start,end = datetime(2019,3,16),datetime(2019,6,1)\n",
    "reindex = pd.date_range(start,end,freq = f'{60/time_step_per_hour}min')\n",
    "print(f'Number of time-slot: {4*24*(end-start).days}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DataSet, Feature Vector, Adjacency Matrix, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U shape: torch.Size([5663, 40, 8]) and Target shape: torch.Size([5663, 40, 1])\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "subway_in,subway_out = load_data_and_pivot(folder_path, file_name, reindex)\n",
    "df_locations = load_subway_shp(folder_path,station_location_name)\n",
    "# Pre-processing\n",
    "subway_in = replace_negative(subway_in,method = 'linear') \n",
    "subway_out = replace_negative(subway_out,method = 'linear')\n",
    "\n",
    "# Set forbidden dates :\n",
    "# Data from  23_03_2019 14:00:00 to 28_04_2019 12:00:00 included should not been taken into account \n",
    "invalid_dates = pd.date_range(datetime(2019,4,23,14),datetime(2019,4,28,14),freq = f'{60/time_step_per_hour}min')\n",
    "\n",
    "(dataset_in,U_in,Utarget_in) = load_normalized_dataset(subway_in,time_step_per_hour,train_prop,step_ahead,H,D,W,invalid_dates)\n",
    "(dataset_out,U_out,Utarget_out) = load_normalized_dataset(subway_out,time_step_per_hour,train_prop,step_ahead,H,D,W,invalid_dates)\n",
    "\n",
    "# colname2indx allow to keep track on the position of a station ('Ampere', 'Brotteaux', ...) within the Tensor\n",
    "colname2indx_in,indx2colname_in = dataset_in.bijection_name_indx()\n",
    "colname2indx_out,indx2colname_out = dataset_out.bijection_name_indx()\n",
    "\n",
    "# Adjacency Matrices : \n",
    "A_dist = load_adjacency_matrix(dataset_in, type = 'distance', df_locations = df_locations, treshold = 0.3)\n",
    "A_neighbor = load_adjacency_matrix(dataset_in, type = 'adjacent')\n",
    "A_corr = load_adjacency_matrix(dataset_in, type = 'correlation')\n",
    "\n",
    "print(f'U shape: {U_in.shape} and Target shape: {Utarget_in.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers qui posent problèmes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    dic_maxi = {}\n",
    "    for c in subway_in:\n",
    "        df_tmps = pd.DataFrame()\n",
    "        d = subway_in[[c]]\n",
    "        for k,(indx,flow) in enumerate(d.iterrows()):\n",
    "            if ((flow[c] > 1200) and not(indx in invalid_dates)) :\n",
    "                concat = d.iloc[k-3:k+3,:]\n",
    "                df_tmps = pd.concat([df_tmps,concat])\n",
    "\n",
    "        dic_maxi[c] = df_tmps\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader, train,valid,test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter\n",
    "valid_prop = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "dataset = dataset_in\n",
    "U,Utarget = U_in, Utarget_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataloader object, which propose different validation (classic, K-fold Cross ...). \n",
    "# data_loader is a dictionnary containing train, valid, and test dataset\n",
    "data_loader_obj = DictDataLoader(U,Utarget,train_prop,valid_prop,validation = 'classic', shuffle = True)\n",
    "data_loader = data_loader_obj.get_dictdataloader(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Train Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of the dataset : \n",
    "L = H+W+D \n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 10\n",
    "lr = 1e-5\n",
    "momentum = 0.99 # 0.9\n",
    "h_dim = 64\n",
    "\n",
    "# Model name: \n",
    "model_name = 'cnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = cnn_perso(c_in=1, h_dim=h_dim, c_out=1, kernel_size = (2,), L=L, padding = 0)\n",
    "\n",
    "# Optimizer, Loss, Scheduler\n",
    "optimizer = SGD(model.parameters(),lr=lr,momentum = momentum)\n",
    "loss_function = nn.MSELoss()  \n",
    "\n",
    "trainer = Trainer(model,data_loader,epochs,optimizer,loss_function,scheduler = None)\n",
    "trainer.train_and_valid()\n",
    "\n",
    "# Access to the list of Loss : \n",
    "# trainer.train_loss,trainer.valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mse': tensor(49133.3281), 'mae': tensor(100.6426)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_pred,Y_true) = trainer.test()  # Normalized Pred and Y_true\n",
    "#df_metrics = evaluate_metrics(norm_Pred,norm_Y_true,metrics= ['mse','mae'])\n",
    "#df_metrics    #{'mse': tensor(0.0119), 'mae': tensor(0.0778)}  avec les données non interpolées\n",
    "\n",
    "test_pred = dataset.unormalize_tensor(test_pred)\n",
    "Y_true = dataset.unormalize_tensor(Y_true)\n",
    "\n",
    "df_metrics = evaluate_metrics(Pred,Y_true,metrics= ['mse','mae'])\n",
    "df_metrics    # {'mse': tensor(31837.8555), 'mae': tensor(95.1038)}  avec les données non interpolées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameter Tuning\n",
    "- add checkpoint\n",
    "- K-fold cross validation \n",
    "- Tuning classique ou Ray Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add (simple) PI intervall "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basleine of PI intervall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray_tensorboard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
