{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from datetime import datetime,timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "\n",
    "# Personnal Import \n",
    "from utilities import DataSet, get_batch,get_mode_date2path, str2neg, str_xa2int\n",
    "from dl_models.graph_conv import graphconv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "- Validation individuelles, aggrégée 3min.\n",
    "- Metro 15 min (entrée/sortie)\n",
    "\n",
    "Idée : Identifier des coupure de métro longue :\n",
    "    - entrée et sortie inhabituelles, montre que le métro est suspendu et laisse les portes ouvertes\n",
    "    - Sortie plus faible (coupure > 15min)\n",
    "Evaluer les qualités de prédiction sur ces moments là.\n",
    "\n",
    "## Data Description\n",
    "- Un 'VAL_ARRET_CODE' peut être l'arrêt de plusieurs mêmes bus, voir d'un même bus et d'un même arrêt de métro. Où d'un même bus et d'un même arrêt de tram. \n",
    "    - Je dois donc nommer différement les VAL_ARRET_CODE de chacun des modes. Une proposition est de mettre le mode (B,S,T) devant les id. Comme ça on pourra regrouper sans soucis.\n",
    "- La moyenne des déplacement de la df_subway est de 5 trajet toute les 3 minutes, quelque soit la station et l'heure (d'ouverture) considéré. Max 88.\n",
    "\n",
    "#### Questionnement \n",
    "- Ok on a aggrégé 3 min, mais est-ce qu'on peut recouper les sorties 3min avec les validation + Sortie de métro 15 min? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Validation Individuelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'data/'\n",
    "valid_ind_path = folder_path + 'Sub_Tram_11_2019_03_2020'\n",
    "dates = ['11-2019','12-2019','1-2020','2-2020','3-2020']\n",
    "\n",
    "subway_paths, tramway_paths, bus_paths = sorted(glob.glob(os.path.join(valid_ind_path, \"*df_subway*.csv\"))),sorted(glob.glob(os.path.join(folder_path, \"*df_tramway*.csv\"))),sorted(glob.glob(os.path.join(folder_path, \"*df_bus*.csv\")))\n",
    "mode_month2path = get_mode_date2path([subway_paths,tramway_paths,bus_paths],['sub','tram','bus'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load 3 df : df_sub, df_tram, df_bus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in ['sub','tram','bus']:\n",
    "    globals()[f'df_{name}'] = pd.concat([pd.read_csv(mode_month2path[name][d],index_col = 0) for d in dates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Subway 15 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subway_path_2019_2020 = \"../../Data/keolis_data_2019-2020/Métro 15 minutes 2019 2020.txt\"\n",
    "df_metro_funi_2019_2020 = pd.read_csv(df_subway_path_2019_2020, delimiter =\"\\t\",low_memory=False).rename(columns = {' Date jour (CAS tr)':'date','Heure (CAS tr)':'hour','Nb entrées total (CAS tr)':'in','Nb sorties total (CAS tr)':'out'})\n",
    "\n",
    "# Tackle float values represented by str: \n",
    "df_metro_funi_2019_2020['out'] = df_metro_funi_2019_2020['out'].transform(lambda x : str_xa2int(x))\n",
    "df_metro_funi_2019_2020['in'] = df_metro_funi_2019_2020['in'].transform(lambda x : str_xa2int(x))\n",
    "\n",
    "# Tackle String Values ('A', 'B'):\n",
    "df_metro_funi_2019_2020['in'] = df_metro_funi_2019_2020['in'].transform(lambda x : str2neg(x))  # Transform each string values like 'A', 'B', '1A'  and so on to '-1'. Keep NaN values as NaN values.\n",
    "df_metro_funi_2019_2020['out'] = df_metro_funi_2019_2020['out'].transform(lambda x : str2neg(x))  \n",
    "\n",
    "# Add '20' like '01_01_2020' instead of '01_01_20'\n",
    "df_metro_funi_2019_2020['date'] = df_metro_funi_2019_2020['date'].transform(lambda d : d+'20' if len(d)<10 else d)\n",
    "T_2020 = pd.to_datetime(df_metro_funi_2019_2020['date'] + ' ' + df_metro_funi_2019_2020['hour'], format='%d/%m/%Y %H:%M', errors='coerce')\n",
    "df_metro_funi_2019_2020['datetime'] = pd.to_datetime(df_metro_funi_2019_2020['date'] + ' ' + df_metro_funi_2019_2020['hour'], format='%d/%m/%Y %H:%M:%S', errors='coerce').fillna(T_2020)\n",
    "\n",
    "# Keep usefull columns\n",
    "df_metro_funi_2019_2020 = df_metro_funi_2019_2020[['datetime','Station','Code ligne','in','out']]\n",
    "\n",
    "# Restrain to usefull 'dates' : \n",
    "start,end = datetime(int(dates[0].split('-')[1]),int(dates[0].split('-')[0]),1),datetime(int(dates[-1].split('-')[1]),int(dates[-1].split('-')[0])+1,1)\n",
    "df_metro_funi_2019_2020 = df_metro_funi_2019_2020[(df_metro_funi_2019_2020.datetime >= start) & (df_metro_funi_2019_2020.datetime < end)]\n",
    "\n",
    "df_metro_funi_2019_2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.axes_grid1 as axes_grid1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def plot_coverage_matshow(data, x_labels = None, y_labels = None, log = False, cmap =\"afmhot\", save = None, cbar_label =  \"Number of Data\"):\n",
    "    # Def function to plot a df with matshow\n",
    "    # Use : plot the coverage through week and days \n",
    "\n",
    "    if log : \n",
    "        data = np.log(data + 1)\n",
    "    \n",
    "    data[data == 0] = np.nan\n",
    "\n",
    "    fig = plt.figure(figsize=(40, 12))\n",
    "    cax = plt.matshow(data.values, cmap=cmap)  #\n",
    "\n",
    "    cmap_perso = plt.get_cmap(cmap)\n",
    "    cmap_perso.set_bad('gray', 1.0)  # Configurez la couleur grise pour les valeurs nulles\n",
    "\n",
    "    # Configurez la colormap pour gérer les valeurs NaN comme le gris\n",
    "    cax.set_cmap(cmap_perso)\n",
    "    cax.set_clim(vmin=0.001, vmax=data.max().max())  # Ajustez les limites pour exclure les NaN\n",
    "\n",
    "\n",
    "    #x labels\n",
    "    if x_labels is None:\n",
    "        x_labels = data.columns.values\n",
    "    plt.gca().set_xticks(range(len(x_labels)))\n",
    "    plt.gca().set_xticklabels(x_labels, rotation=85, fontsize=8)\n",
    "    plt.gca().xaxis.set_ticks_position('bottom')\n",
    "\n",
    "    #y labels\n",
    "    if y_labels is None: \n",
    "        y_labels = data.index.values\n",
    "    plt.gca().set_yticks(range(len(y_labels)))\n",
    "    plt.gca().set_yticklabels(y_labels, fontsize=8)\n",
    "\n",
    "    # Add a colorbar to the right of the figure\n",
    "    cbar = plt.colorbar(cax, aspect=10)\n",
    "    cbar.set_label(cbar_label)  # You can customize the label as needed\n",
    "\n",
    "    if save is not None: \n",
    "            plt.savefig(save, format=\"pdf\")\n",
    "            \n",
    "def coverage_day_month(df_metro,freq= '24h',index = 'month_year',columns = 'day_date',save = 'subway_id',folder_save = 'save/'):\n",
    "\n",
    "    df_agg = df_metro.groupby([pd.Grouper(key = 'datetime',freq = freq)]).agg('sum').reset_index()[['datetime','in','out']]\n",
    "    df_agg['date']= df_agg.datetime.dt.date\n",
    "    df_agg['day_date'] = df_agg.datetime.dt.day\n",
    "    df_agg['month_year']= df_agg.datetime.dt.month.transform(lambda x : str(x)) + ' ' + df_agg.datetime.dt.year.transform(lambda x : str(x))\n",
    "    df_agg['month_year']= pd.to_datetime(df_agg['month_year'],format = '%m %Y')\n",
    "    #df_agg['hour']= df_agg.datetime.dt.hour.transform(lambda x : str(x)) + ':' + df_agg.datetime.dt.minute.transform(lambda x : str(x))\n",
    "    df_agg['hour']= df_agg.datetime.dt.hour + df_agg.datetime.dt.minute*0.01\n",
    "    df_agg['tot'] = df_agg['in'] + df_agg['out']\n",
    "    # Pivot\n",
    "\n",
    "    df_agg_in = df_agg.pivot(index = index,columns = columns,values = 'in').fillna(0)\n",
    "    df_agg_out = df_agg.pivot(index = index,columns = columns,values = 'out').fillna(0)\n",
    "    df_agg_tot = df_agg.pivot(index = index,columns = columns,values = 'tot').fillna(0)\n",
    "    \n",
    "    if index == 'month_year':\n",
    "        df_agg_in.index = df_agg_in.index.strftime('%Y-%m')\n",
    "        df_agg_out.index = df_agg_out.index.strftime('%Y-%m')\n",
    "        df_agg_tot.index = df_agg_out.index.strftime('%Y-%m')\n",
    "\n",
    "\n",
    "    # Plot \n",
    "    plot_coverage_matshow(df_agg_in, log  = False, cmap = 'YlOrRd',save = f'{folder_save}in_{save}')   \n",
    "    plot_coverage_matshow(df_agg_out, log  = False, cmap = 'YlOrRd',save = f'{folder_save}out_{save}')  \n",
    "    plot_coverage_matshow(df_agg_tot, log  = False, cmap = 'YlOrRd',save = f'{folder_save}tot_{save}')  \n",
    "    return(df_agg_in,df_agg_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation des flux IN et OUT entre 6h et 24H. En virant les outliers type fête des lumières: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq  = '15min'\n",
    "columns = 'hour'\n",
    "index = 'date'\n",
    "quantile = 0.95\n",
    "è\n",
    "for station in df_metro_funi_2019_2020.Station.unique():\n",
    "    df_tmps = df_metro_funi_2019_2020[df_metro_funi_2019_2020.Station == station]\n",
    "    df_tmps = df_tmps[df_tmps.datetime.dt.hour > 5]\n",
    "    in99,out99 = df_tmps['in'].quantile(quantile),df_tmps['out'].quantile(quantile)\n",
    "    df_tmps.loc[df_tmps['in']>in99,'in'] = in99\n",
    "    df_tmps.loc[df_tmps['out']>out99,'out'] = out99\n",
    "    coverage_day_month(df_tmps, freq = freq,index = index,columns = columns,save = station,folder_save = 'save/profile flux 15min filtred outliers 95/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation des flux IN et OUT pour chacunes des stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in df_metro_funi_2019_2020.Station.unique():\n",
    "    df_tmps = df_metro_funi_2019_2020[df_metro_funi_2019_2020.Station == station]\n",
    "    coverage_day_month(df_tmps, freq = '60min',index = 'date',columns = 'hour',save = station,folder_save = 'save/profile flux')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1er Etape : Prédiction Métro\n",
    "- On va d'abord prédire la demande sur une ligne (disons A).  \n",
    "- On va comparer des modèle : LSTM, CNN, CNN-LSTM, GNN.\n",
    "    - A priori pas de \"raison\" que le GNN marche mieux. Si c'est le cas, c'est peut être simplement que le modèle est plus complexe, mais j'ai du mal à croire que si on donne les bonnes informations (historique -7d, -1d, -4,3,2,1t), on a des meilleurs résultats avec GNN. Sauf si il y a des relation asynchrone \"récurrentes\", mais sans causalité.\n",
    "- Identifier des moment interessant : anomalie sur entrée/sortie métro. Voir les prédictions sur ces moments là particulier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Vector\n",
    "A définir pour chacune des stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bla Bla : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bijecction entre un entier, et son label\n",
    "def int2lab(int,n_adj=2,B=3):\n",
    "    n = 1+(int-1)//B \n",
    "    b = int-(n-1)*B\n",
    "    return(f'n{n}_b{b}')\n",
    "\n",
    "def lab2int(lab,n_adj=2,B=3):\n",
    "    nb = lab.split('_')\n",
    "    n,b = int(nb[0][1:]),int(nb[1][1:])\n",
    "    return((n-1)*B+b)\n",
    "\n",
    "for k in range(1,15):\n",
    "    print(int2lab(k),lab2int(int2lab(k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentation des numéro des cellules du Tensor\n",
    "Permet de faire des affichages graphique, et de s'assurer que les \".reshape\" font ce que l'on souhaite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int2lab(integer, n_adj=2, B=3, C=4, N=5, L=6):\n",
    "    N_adj_L = N * L\n",
    "    N_adj_C_L = C * N_adj_L\n",
    "    N_adj_B_C_L = B * N_adj_C_L\n",
    "\n",
    "    l = 1 + ((integer - 1) % L)\n",
    "    remaining = (integer - 1) // L\n",
    "    n = 1 + (remaining % N)\n",
    "    remaining //= N\n",
    "    c = 1 + (remaining % C)\n",
    "    remaining //= C\n",
    "    b = 1 + (remaining % B)\n",
    "    remaining //= B\n",
    "    n_adj = 1 + remaining % n_adj\n",
    "\n",
    "    return f'adj{n_adj}_b{b}_c{c}_n{n}_l{l}'\n",
    "\n",
    "def lab2int(label, n_adj=2, B=3, C=4, N=5, L=6):\n",
    "    split_label = label.split('_')\n",
    "    n_adj = int(split_label[0][3:])\n",
    "    b = int(split_label[1][1:])\n",
    "    c = int(split_label[2][1:])\n",
    "    n = int(split_label[3][1:])\n",
    "    l = int(split_label[4][1:])\n",
    "\n",
    "    N_adj_L = N * L\n",
    "    N_adj_C_L = C * N_adj_L\n",
    "    N_adj_B_C_L = B * N_adj_C_L\n",
    "\n",
    "    integer = ((l - 1) + (n - 1) * L + (c - 1) * N * L + (b - 1) * C * N * L + (n_adj - 1) * B * C * N * L) + 1\n",
    "    return integer\n",
    "\n",
    "\n",
    "for k in range(1,400):\n",
    "    print(int2lab(k),lab2int(int2lab(k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = int2lab(k)\n",
    "label.split('_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_H = output_H.permute(0, 1, 3, 4, 2)\n",
    "print(permuted_H.shape)\n",
    "\n",
    "new_c_in = permuted_H.shape[-1]\n",
    "first_attn = permuted_H.reshape(K,-1,new_c_in)\n",
    "print(first_attn.shape)  # [K, B*L*N, C_in']\n",
    "\n",
    "attn_weight = nn.Linear(new_c_in,1)\n",
    "softmax = nn.Softmax(-1)  #SoftMax on the last dimension \n",
    "\n",
    "#Coefficient d'attention \n",
    "attn = attn_weight(first_attn)    # \"Embedding\" du spatial channel \n",
    "attn = attn.permute(1,2,0)  # Permute pour avoir la nombre d'adjacency matrix en dernière dimension\n",
    "attn = softmax(attn)  # Coefficient d'attention pour chaque Matrices d'adjacence et Embedding spatial assicié  (Ici 1 seule matrice d'adjacence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Preprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
