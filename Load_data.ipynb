{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from datetime import datetime,timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "\n",
    "# Personnal Import \n",
    "from utilities import DataSet, get_batch,get_mode_date2path, str2neg, str_xa2int\n",
    "from dl_models.graph_conv import graphconv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "- Validation individuelles, aggrégée 3min.\n",
    "- Metro 15 min (entrée/sortie)\n",
    "\n",
    "Idée : Identifier des coupure de métro longue :\n",
    "    - entrée et sortie inhabituelles, montre que le métro est suspendu et laisse les portes ouvertes\n",
    "    - Sortie plus faible (coupure > 15min)\n",
    "Evaluer les qualités de prédiction sur ces moments là.\n",
    "\n",
    "## Data Description\n",
    "- Un 'VAL_ARRET_CODE' peut être l'arrêt de plusieurs mêmes bus, voir d'un même bus et d'un même arrêt de métro. Où d'un même bus et d'un même arrêt de tram. \n",
    "    - Je dois donc nommer différement les VAL_ARRET_CODE de chacun des modes. Une proposition est de mettre le mode (B,S,T) devant les id. Comme ça on pourra regrouper sans soucis.\n",
    "- La moyenne des déplacement de la df_subway est de 5 trajet toute les 3 minutes, quelque soit la station et l'heure (d'ouverture) considéré. Max 88.\n",
    "\n",
    "#### Questionnement \n",
    "- Ok on a aggrégé 3 min, mais est-ce qu'on peut recouper les sorties 3min avec les validation + Sortie de métro 15 min? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Validation Individuelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'data/'\n",
    "valid_ind_path = folder_path + 'Sub_Tram_11_2019_03_2020'\n",
    "dates = ['11-2019','12-2019','1-2020','2-2020','3-2020']\n",
    "\n",
    "subway_paths, tramway_paths, bus_paths = sorted(glob.glob(os.path.join(valid_ind_path, \"*df_subway*.csv\"))),sorted(glob.glob(os.path.join(valid_ind_path, \"*df_tramway*.csv\"))),sorted(glob.glob(os.path.join(valid_ind_path, \"*df_bus*.csv\")))\n",
    "mode_month2path = get_mode_date2path([subway_paths,tramway_paths,bus_paths],['sub','tram','bus'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load 3 df : df_sub, df_tram, df_bus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in ['sub','tram','bus']:\n",
    "    globals()[f'df_{name}'] = pd.concat([pd.read_csv(mode_month2path[name][d],index_col = 0) for d in dates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Subway 15 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subway_path_2019_2020 = \"../../Data/keolis_data_2019-2020/Métro 15 minutes 2019 2020.txt\"\n",
    "df_metro_funi_2019_2020 = pd.read_csv(df_subway_path_2019_2020, delimiter =\"\\t\",low_memory=False).rename(columns = {' Date jour (CAS tr)':'date','Heure (CAS tr)':'hour','Nb entrées total (CAS tr)':'in','Nb sorties total (CAS tr)':'out'})\n",
    "\n",
    "# Tackle float values represented by str: \n",
    "df_metro_funi_2019_2020['out'] = df_metro_funi_2019_2020['out'].transform(lambda x : str_xa2int(x))\n",
    "df_metro_funi_2019_2020['in'] = df_metro_funi_2019_2020['in'].transform(lambda x : str_xa2int(x))\n",
    "\n",
    "# Tackle String Values ('A', 'B'):\n",
    "df_metro_funi_2019_2020['in'] = df_metro_funi_2019_2020['in'].transform(lambda x : str2neg(x))  # Transform each string values like 'A', 'B', '1A'  and so on to '-1'. Keep NaN values as NaN values.\n",
    "df_metro_funi_2019_2020['out'] = df_metro_funi_2019_2020['out'].transform(lambda x : str2neg(x))  \n",
    "\n",
    "# Add '20' like '01_01_2020' instead of '01_01_20'\n",
    "df_metro_funi_2019_2020['date'] = df_metro_funi_2019_2020['date'].transform(lambda d : d+'20' if len(d)<10 else d)\n",
    "T_2020 = pd.to_datetime(df_metro_funi_2019_2020['date'] + ' ' + df_metro_funi_2019_2020['hour'], format='%d/%m/%Y %H:%M', errors='coerce')\n",
    "df_metro_funi_2019_2020['datetime'] = pd.to_datetime(df_metro_funi_2019_2020['date'] + ' ' + df_metro_funi_2019_2020['hour'], format='%d/%m/%Y %H:%M:%S', errors='coerce').fillna(T_2020)\n",
    "\n",
    "# Keep usefull columns\n",
    "df_metro_funi_2019_2020 = df_metro_funi_2019_2020[['datetime','Station','Code ligne','in','out']]\n",
    "\n",
    "# Restrain to usefull 'dates' : \n",
    "start,end = datetime(int(dates[0].split('-')[1]),int(dates[0].split('-')[0]),1),datetime(int(dates[-1].split('-')[1]),int(dates[-1].split('-')[0])+1,1)\n",
    "df_metro_funi_2019_2020 = df_metro_funi_2019_2020[(df_metro_funi_2019_2020.datetime >= start) & (df_metro_funi_2019_2020.datetime < end)]\n",
    "\n",
    "df_metro_funi_2019_2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.axes_grid1 as axes_grid1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def plot_coverage_matshow(data, x_labels = None, y_labels = None, log = False, cmap =\"afmhot\", save = None, cbar_label =  \"Number of Data\"):\n",
    "    # Def function to plot a df with matshow\n",
    "    # Use : plot the coverage through week and days \n",
    "\n",
    "    if log : \n",
    "        data = np.log(data + 1)\n",
    "    \n",
    "    data[data == 0] = np.nan\n",
    "\n",
    "    fig = plt.figure(figsize=(40, 12))\n",
    "    cax = plt.matshow(data.values, cmap=cmap)  #\n",
    "\n",
    "    cmap_perso = plt.get_cmap(cmap)\n",
    "    cmap_perso.set_bad('gray', 1.0)  # Configurez la couleur grise pour les valeurs nulles\n",
    "\n",
    "    # Configurez la colormap pour gérer les valeurs NaN comme le gris\n",
    "    cax.set_cmap(cmap_perso)\n",
    "    cax.set_clim(vmin=0.001, vmax=data.max().max())  # Ajustez les limites pour exclure les NaN\n",
    "\n",
    "\n",
    "    #x labels\n",
    "    if x_labels is None:\n",
    "        x_labels = data.columns.values\n",
    "    plt.gca().set_xticks(range(len(x_labels)))\n",
    "    plt.gca().set_xticklabels(x_labels, rotation=85, fontsize=8)\n",
    "    plt.gca().xaxis.set_ticks_position('bottom')\n",
    "\n",
    "    #y labels\n",
    "    if y_labels is None: \n",
    "        y_labels = data.index.values\n",
    "    plt.gca().set_yticks(range(len(y_labels)))\n",
    "    plt.gca().set_yticklabels(y_labels, fontsize=8)\n",
    "\n",
    "    # Add a colorbar to the right of the figure\n",
    "    cbar = plt.colorbar(cax, aspect=10)\n",
    "    cbar.set_label(cbar_label)  # You can customize the label as needed\n",
    "\n",
    "    if save is not None: \n",
    "            plt.savefig(save, format=\"pdf\")\n",
    "            \n",
    "def coverage_day_month(df_metro,freq= '24h',index = 'month_year',columns = 'day_date',save = 'subway_id',folder_save = 'save/'):\n",
    "\n",
    "    df_agg = df_metro.groupby([pd.Grouper(key = 'datetime',freq = freq)]).agg('sum').reset_index()[['datetime','in','out']]\n",
    "    df_agg['date']= df_agg.datetime.dt.date\n",
    "    df_agg['day_date'] = df_agg.datetime.dt.day\n",
    "    df_agg['month_year']= df_agg.datetime.dt.month.transform(lambda x : str(x)) + ' ' + df_agg.datetime.dt.year.transform(lambda x : str(x))\n",
    "    df_agg['month_year']= pd.to_datetime(df_agg['month_year'],format = '%m %Y')\n",
    "    #df_agg['hour']= df_agg.datetime.dt.hour.transform(lambda x : str(x)) + ':' + df_agg.datetime.dt.minute.transform(lambda x : str(x))\n",
    "    df_agg['hour']= df_agg.datetime.dt.hour + df_agg.datetime.dt.minute*0.01\n",
    "    df_agg['tot'] = df_agg['in'] + df_agg['out']\n",
    "    # Pivot\n",
    "\n",
    "    df_agg_in = df_agg.pivot(index = index,columns = columns,values = 'in').fillna(0)\n",
    "    df_agg_out = df_agg.pivot(index = index,columns = columns,values = 'out').fillna(0)\n",
    "    df_agg_tot = df_agg.pivot(index = index,columns = columns,values = 'tot').fillna(0)\n",
    "    \n",
    "    if index == 'month_year':\n",
    "        df_agg_in.index = df_agg_in.index.strftime('%Y-%m')\n",
    "        df_agg_out.index = df_agg_out.index.strftime('%Y-%m')\n",
    "        df_agg_tot.index = df_agg_out.index.strftime('%Y-%m')\n",
    "\n",
    "\n",
    "    # Plot \n",
    "    plot_coverage_matshow(df_agg_in, log  = False, cmap = 'YlOrRd',save = f'{folder_save}in_{save}')   \n",
    "    plot_coverage_matshow(df_agg_out, log  = False, cmap = 'YlOrRd',save = f'{folder_save}out_{save}')  \n",
    "    plot_coverage_matshow(df_agg_tot, log  = False, cmap = 'YlOrRd',save = f'{folder_save}tot_{save}')  \n",
    "    return(df_agg_in,df_agg_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation des flux IN et OUT entre 6h et 24H. En virant les outliers type fête des lumières: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq  = '15min'\n",
    "columns = 'hour'\n",
    "index = 'date'\n",
    "quantile = 0.95\n",
    "è\n",
    "for station in df_metro_funi_2019_2020.Station.unique():\n",
    "    df_tmps = df_metro_funi_2019_2020[df_metro_funi_2019_2020.Station == station]\n",
    "    df_tmps = df_tmps[df_tmps.datetime.dt.hour > 5]\n",
    "    in99,out99 = df_tmps['in'].quantile(quantile),df_tmps['out'].quantile(quantile)\n",
    "    df_tmps.loc[df_tmps['in']>in99,'in'] = in99\n",
    "    df_tmps.loc[df_tmps['out']>out99,'out'] = out99\n",
    "    coverage_day_month(df_tmps, freq = freq,index = index,columns = columns,save = station,folder_save = 'save/profile flux 15min filtred outliers 95/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation des flux IN et OUT pour chacunes des stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in df_metro_funi_2019_2020.Station.unique():\n",
    "    df_tmps = df_metro_funi_2019_2020[df_metro_funi_2019_2020.Station == station]\n",
    "    coverage_day_month(df_tmps, freq = '60min',index = 'date',columns = 'hour',save = station,folder_save = 'save/profile flux')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1er Etape : Prédiction Métro\n",
    "- On va d'abord prédire la demande sur une ligne (disons A).  \n",
    "- On va comparer des modèle : LSTM, CNN, CNN-LSTM, GNN.\n",
    "    - A priori pas de \"raison\" que le GNN marche mieux. Si c'est le cas, c'est peut être simplement que le modèle est plus complexe, mais j'ai du mal à croire que si on donne les bonnes informations (historique -7d, -1d, -4,3,2,1t), on a des meilleurs résultats avec GNN. Sauf si il y a des relation asynchrone \"récurrentes\", mais sans causalité.\n",
    "- Identifier des moment interessant : anomalie sur entrée/sortie métro. Voir les prédictions sur ces moments là particulier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = sub_nov[sub_nov['lane'] =='A'][['COD_TRG','Flow','VAL_DATE']]\n",
    "sub_df = sub_df.groupby(['COD_TRG','VAL_DATE']).sum()\n",
    "sub_df = sub_df.reset_index()\n",
    "sub_df.VAL_DATE = pd.to_datetime(sub_df.VAL_DATE) \n",
    "\n",
    "# Reindex date\n",
    "start,end = sub_df.VAL_DATE.iloc[0],sub_df.VAL_DATE.iloc[-1]\n",
    "date_index = pd.date_range(start = start,end = end, freq = '3min')\n",
    "sub_df = sub_df.pivot(index = 'VAL_DATE',columns = 'COD_TRG',values = 'Flow')\n",
    "sub_df = sub_df.reindex(date_index).fillna(0)\n",
    "\n",
    "#Reindex columns :\n",
    "stations = ['PER','AMP','BEL','COR','HOT','FOC','MAS','CHA','REP','GRA','FLA','CUS','BON','SOI']\n",
    "sub_df = sub_df[stations]\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Vector\n",
    "A définir pour chacune des stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq  ='3min'\n",
    "freq_min = int(freq.split('min')[0])\n",
    "\n",
    "time_step_per_hour = int(60/freq_min) #3min agg\n",
    "\n",
    "historical_len = 7\n",
    "Days = 1\n",
    "Weeks = 1\n",
    "step_ahead = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ut = DataSet(sub_df,time_step_per_hour=time_step_per_hour)\n",
    "norm_Ut = Ut.normalize()    # Normalize before getting the \"Feature vector\"  (or \"Feature Tensor\")\n",
    "(X,Y,dates_verif) = norm_Ut.get_feature_vect(step_ahead,historical_len,Days,Weeks)\n",
    "print('Feature vector shape: ',X.shape)   # Nb Sample, Nb Nodes, Sequence Length\n",
    "dates_verif.head()\n",
    "\n",
    "# Ut.df\n",
    "# Ut.init_df\n",
    "# Ut.time_step_per_hour\n",
    "# Ut.mini\n",
    "# Ut.maxi\n",
    "# Ut.mean \n",
    "# norm_Ut.df.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load adjacency Matrix: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_indep = torch.diag(torch.ones(len(Ut.df.columns)))   # Matrice d'adjacence identité, personne n'est connecté avec personne\n",
    "A_Neighbors = torch.sum(torch.stack([torch.diag(torch.ones(len(Ut.df.columns)-abs(i)),i) for i in [-1,0,1]]),dim =0)   #Une seule ligne de métro, donc tri-diagonale\n",
    "A_learnable = torch.nn.Parameter(torch.randn(len(Ut.df.columns),len(Ut.df.columns)),requires_grad=True)   #Matrice d'adjacence apprentissable\n",
    "\n",
    "# Then convert into \"Laplacian Matrix\", or with \"random_walk Matrix\", or with another one ...\n",
    "A_indep = \n",
    "A_Neighbors =\n",
    "A_learnable = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class graphconv(nn.Module):\n",
    "    def __init__(self,c_in,c_out,graph_conv_act_func,K = 2,enable_bias =True):\n",
    "        super(graphconv,self).__init__()   # Demande a ce qu'on récupère les méthodes de la classe parent :  'nn.module'\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.enable_bias = enable_bias\n",
    "        self.graph_conv_act_func = graph_conv_act_func\n",
    "        self.K = K\n",
    "        if torch.cuda.is_available():\n",
    "            self.weight = nn.Parameter(torch.cuda.FloatTensor(K,c_in,c_out))   # Initialize with wierd weight like 0e-30 or 1e38. Might not be totally adaptated ...\n",
    "        else :\n",
    "            self.weight = nn.Parameter(torch.FloatTensor(K,c_in,c_out))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x,gcnconv_matrix):\n",
    "        B, C, L, N = x.shape\n",
    "        n_mat =  gcnconv_matrix.shape[0]\n",
    "\n",
    "        x = x.reshape(-1, self.c_in)  #[B, C_in, L, N] -> [BLN, C_in]\n",
    "        x = torch.einsum('ab, cbd->cad',x,self.weight)   # [BLN,C_in], [K,C_in,C_out] -> [K,BLN,C_out]\n",
    "        x = x.view(self.K, B*L,N,-1)  #[K,BLN,C_out] ->  [K,BL,N,C_out] \n",
    "        x = torch.einsum('ecab,ecbd->ecad',gcnconv_matrix,x)  #[n_adj,BL,N1,N2] ,[K,BL,N2,C_out]  -> [K,BL,N1,C_out] \n",
    "\n",
    "        if self.enable_bias:\n",
    "            x = x + self.bias\n",
    "        \n",
    "        x = x.view().permute()\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,1,3,4)\n",
    "A_indep = torch.ones(3,3)\n",
    "\n",
    "C_in = x.shape[1]\n",
    "C_out = 8\n",
    "weight = torch.FloatTensor(C_in,C_out)\n",
    "weight = torch.Tensor([[1,2,3,4,5,6,7,8]])\n",
    "\n",
    "adj_matrix =A_indep\n",
    "# x.shape = [B,C,N2,L]\n",
    "B,C_in,N2,L = x.shape\n",
    "# A.shape = [n_adj,N1,N2]\n",
    "N1,N2 = adj_matrix.shape\n",
    "#A.shape = [n_adj,B,N1,N2]\n",
    "stacked_adj = adj_matrix.repeat(1,B,1,1)\n",
    "\n",
    "\n",
    "embedding = torch.einsum('bcnl,ch -> bhnl',x,weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MGCN_conv(nn.Module):\n",
    "    def __init__(self,c_in,c_out,n_adj=2,enable_bias =True):\n",
    "        super(MGCN_conv,self).__init__()\n",
    "        self.n_adj = n_adj    \n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.weight = torch.FloatTensor(n_adj,c_in,c_out)  #[C_in,C_out]   ou pour Einsum: [l,h]\n",
    "        self.bias = torch.FloatTensor(c_out)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x,adj_matrix):\n",
    "        # x.shape = [B,N2,L]\n",
    "        B,N2,L = x.shape\n",
    "        # A.shape = [n_adj,N1,N2]\n",
    "        n_adj,N1,N2 = adj_matrix.shape\n",
    "\n",
    "        #adj_matrix = torch.unsqueeze(adj_matrix,0) #Créer un nouvel axe \n",
    "        stacked_adj = adj_matrix.repeat(B,1,1) # Concat tout les \"samples\" (B batch, L tmeporal dimension) long du nouvel axe\n",
    "        stacked_adj = stacked_adj.reshape(-1,N1,N2)  # Stack les n_adj adjacency matrix B*L fois     shape: [n_adj*B, N1,N2] et pour einsum : [b,p,n]\n",
    "\n",
    "        embedding = torch.einsum('bnl,klh -> bknh',x,self.weight)   # Embedding sur les feature de chaque Noeud\n",
    "        reshaped_embedding = embedding.reshape(-1,N2,self.c_out)    #Stack la dimension k sur l'axe 0  [n_adj*B,N2,C_out]  et pour einsum : [b,n,h]\n",
    "\n",
    "        convoluted = torch.einsum('bpn,bnh -> bph',stacked_adj,reshaped_embedding) \n",
    "        convoluted = convoluted.reshape(self.n_adj,B,N1,self.c_out)      #Unstack la dimension 0, pour séparer les n_adj matrices d'adjacences\n",
    "\n",
    "        convoluted = self.relu(convoluted + self.bias)\n",
    "\n",
    "        return(convoluted,embedding,reshaped_embedding)\n",
    "\n",
    "# Test : \n",
    "N1,N2 = 3,3\n",
    "adj_matrix = torch.stack([torch.diag(torch.ones(N1)),torch.randn(N1,N2)],dim = 0)  # Matrice d'adjacence identité, personne n'est connecté avec personne\n",
    "n_adj = adj_matrix.shape[0]\n",
    "\n",
    "B = 4\n",
    "L = 6\n",
    "x = torch.randn(B,N1,L)\n",
    "\n",
    "c_out = 16\n",
    "gcn_model = MGCN_conv(L,c_out,n_adj)\n",
    "AXW,embedding,reshaped_embedding = gcn_model(x,adj_matrix)\n",
    "\n",
    "print(\"L'embedding a d'abord été opéré sur la dernière dimension (X*W, temporelle), Puis la convolution (A*(XW)) a sommmé les embedding de chacun des voisins (ou Noeud en lien avec le noeud tagret).\")\n",
    "print(f'X.shape: {x.shape}, AXW.shape:{AXW.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.shape,reshaped_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bijecction entre un entier, et son label\n",
    "def int2lab(int,n_adj=2,B=3):\n",
    "    n = 1+(int-1)//B \n",
    "    b = int-(n-1)*B\n",
    "    return(f'n{n}_b{b}')\n",
    "\n",
    "def lab2int(lab,n_adj=2,B=3):\n",
    "    nb = lab.split('_')\n",
    "    n,b = int(nb[0][1:]),int(nb[1][1:])\n",
    "    return((n-1)*B+b)\n",
    "\n",
    "for k in range(1,15):\n",
    "    print(int2lab(k),lab2int(int2lab(k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentation des numéro des cellules du Tensor\n",
    "Permet de faire des affichages graphique, et de s'assurer que les \".reshape\" font ce que l'on souhaite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int2lab(integer, n_adj=2, B=3, C=4, N=5, L=6):\n",
    "    N_adj_L = N * L\n",
    "    N_adj_C_L = C * N_adj_L\n",
    "    N_adj_B_C_L = B * N_adj_C_L\n",
    "\n",
    "    l = 1 + ((integer - 1) % L)\n",
    "    remaining = (integer - 1) // L\n",
    "    n = 1 + (remaining % N)\n",
    "    remaining //= N\n",
    "    c = 1 + (remaining % C)\n",
    "    remaining //= C\n",
    "    b = 1 + (remaining % B)\n",
    "    remaining //= B\n",
    "    n_adj = 1 + remaining % n_adj\n",
    "\n",
    "    return f'adj{n_adj}_b{b}_c{c}_n{n}_l{l}'\n",
    "\n",
    "def lab2int(label, n_adj=2, B=3, C=4, N=5, L=6):\n",
    "    split_label = label.split('_')\n",
    "    n_adj = int(split_label[0][3:])\n",
    "    b = int(split_label[1][1:])\n",
    "    c = int(split_label[2][1:])\n",
    "    n = int(split_label[3][1:])\n",
    "    l = int(split_label[4][1:])\n",
    "\n",
    "    N_adj_L = N * L\n",
    "    N_adj_C_L = C * N_adj_L\n",
    "    N_adj_B_C_L = B * N_adj_C_L\n",
    "\n",
    "    integer = ((l - 1) + (n - 1) * L + (c - 1) * N * L + (b - 1) * C * N * L + (n_adj - 1) * B * C * N * L) + 1\n",
    "    return integer\n",
    "\n",
    "\n",
    "for k in range(1,400):\n",
    "    print(int2lab(k),lab2int(int2lab(k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = int2lab(k)\n",
    "label.split('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_conv(nn.Module):\n",
    "    def __init__(self,c_in,c_out,graph_conv_act_func,K=1,enable_bias =True):\n",
    "        super(GCN_conv,self).__init__()\n",
    "\n",
    "    \n",
    "    def forward(self,x,adj_matrix):\n",
    "        # x.shape = [B,C,N2,L]\n",
    "        B,C,N2,L = x.shape\n",
    "        # A.shape = [n_adj,N1,N2]\n",
    "        n_adj,N1,N2 = adj_matrix.shape\n",
    "        #A.shape = [n_adj,B,N1,N2]\n",
    "        adj_matrix = torch.unsqueeze(adj_matrix,0) #Créer un nouvel axe \n",
    "        stacked_adj = adj_matrix.repeat(B*L,1,1,1) # Concat tout les \"samples\" (B batch, L tmeporal dimension) long du nouvel axe\n",
    "        stacked_adj = stacked_adj.reshape(-1,N1,N2)  # Stack les n_adj adjacency matrix B*L fois\n",
    "\n",
    "        embedding = torch.einsum('bcnl,ch -> bhnl',x,self.weight)   # Embedding Spatial le long de C_in\n",
    "        embedding = embedding.permute(0,3,2,1)  # [B,L,N2,C_out]\n",
    "        embedding = embedding.reshape(-1,N2,C_out)  # [BL,N2,C_out]\n",
    "\n",
    "        convolution = torch.einsum('kbpn,bhnl -> kbpl',stacked_adj,embedding)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_adj_matrix = adj_matrix.repeat(2,1,1)\n",
    "n_adj,N1,N2 = n_adj_matrix.shape\n",
    "#A.shape = [n_adj,B,N1,N2]\n",
    "stacked_adj = n_adj_matrix.repeat(B*L,1,1) # [n_adj*B, N1,N2]  -> Concat tout les \"samples\" long de l'axe 0\n",
    "stacked_adj.shape,n_adj_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X.unsqueeze(1) #add the channel dimension (here, only \"flow')\n",
    "print(f'X shape: {x.shape}')\n",
    "x_b = x[:32]\n",
    "print(f'x_b shape: {x_b.shape}')\n",
    "\n",
    "GCN = graphconv(c_in = x.shape[1], c_out = 64, K=2, graph_conv_act_func = 'relu',enable_bias=True)  # K =2 dans MRGNN car considère Pattern et Adj matrix\n",
    "B, C, N, L = x_b.shape\n",
    "K = 1\n",
    "c_out = 64\n",
    "n_mat =  A_indep.shape[0]\n",
    "\n",
    "print(f'B,C,N,L : [{B},{C},{N},{L}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN détaillé "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = nn.Parameter(torch.FloatTensor(K,C,c_out))\n",
    "bias = nn.Parameter(torch.FloatTensor(c_out))\n",
    "print(f'x_b: {x_b.shape}, W: {weight.shape}, Cause there are {K} adjacency matrices (K), {C} C_in and {c_out} C_out')\n",
    "x_b = x_b.reshape(-1, C)  #[B, C_in, L, N] -> [BLN, C_in]\n",
    "print(f'reshaped x_b: {x_b.shape}, Cause we have to flatten x_b along the Channel axis, and then pass through a SPATIAL embedding (Linear layer, on C_in of each sample,nodes,historical element)')\n",
    "\n",
    "# Embedding on C_in:  X*W\n",
    "embedd_c_in = torch.einsum('ab, cbd->cad',x_b,weight)   # [BLN,C_in], [K,C_in,C_out] -> [K,BLN,C_out]n  Propose K embedding de C_in\n",
    "print('shape of reshaped_xb after K embedding on C_in: ',embedd_c_in.shape)\n",
    "reshaped_embedd_c_in = embedd_c_in.view(K, B*L,N,-1)  #[K,BLN,C_out] ->  [K,BL,N,C_out] \n",
    "print('Embedded feature vect reshaped: ',reshaped_embedd_c_in.shape, '\\n')\n",
    "\n",
    "# Concat Adj Matrix \n",
    "batched_adj_matrix = A_indep.repeat(1,B*L,1,1)\n",
    "print('Adjacency matrix A_indep: ',A_indep.shape,'batched Multi adj_matrix: ',batched_adj_matrix.shape)\n",
    "\n",
    "# Convolution A*(XW)\n",
    "convolutionned = torch.einsum('ecab,ecbd->ecad',batched_adj_matrix,reshaped_embedd_c_in)  #[K,BL,N1,N2] ,[K,BL,N2,C_out]  -> [K,BL,N1,C_out] \n",
    "print('Convolution A*(XW): ',convolutionned.shape)\n",
    "\n",
    "#Add bias: \n",
    "convolutionned_n_biased = convolutionned + bias\n",
    "\n",
    "# Reshape and Permute: \n",
    "H =convolutionned_n_biased.view(K,B,L,-1,c_out).permute(0,1,4,3,2)\n",
    "print(f'Reshaped convolution output: {H.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_H = output_H.permute(0, 1, 3, 4, 2)\n",
    "print(permuted_H.shape)\n",
    "\n",
    "new_c_in = permuted_H.shape[-1]\n",
    "first_attn = permuted_H.reshape(K,-1,new_c_in)\n",
    "print(first_attn.shape)  # [K, B*L*N, C_in']\n",
    "\n",
    "attn_weight = nn.Linear(new_c_in,1)\n",
    "softmax = nn.Softmax(-1)  #SoftMax on the last dimension \n",
    "\n",
    "#Coefficient d'attention \n",
    "attn = attn_weight(first_attn)    # \"Embedding\" du spatial channel \n",
    "attn = attn.permute(1,2,0)  # Permute pour avoir la nombre d'adjacency matrix en dernière dimension\n",
    "attn = softmax(attn)  # Coefficient d'attention pour chaque Matrices d'adjacence et Embedding spatial assicié  (Ici 1 seule matrice d'adjacence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(attn == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2,3,4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if self.enable_bias:\n",
    "    x = x + self.bias\n",
    "\n",
    "x = x.view().permute()\n",
    "x = self.relu(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Preprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
