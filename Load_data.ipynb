{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from datetime import datetime,timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "\n",
    "# Personnal Import \n",
    "from utilities import DataSet, get_batch,get_mode_date2path\n",
    "from load_data import load_subway_15_min\n",
    "from dl_models.graph_conv import graphconv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "- Validation individuelles, aggrégée 3min.\n",
    "- Metro 15 min (entrée/sortie)\n",
    "\n",
    "Idée : Identifier des coupure de métro longue :\n",
    "    - entrée et sortie inhabituelles, montre que le métro est suspendu et laisse les portes ouvertes\n",
    "    - Sortie plus faible (coupure > 15min)\n",
    "Evaluer les qualités de prédiction sur ces moments là.\n",
    "\n",
    "## Data Description\n",
    "- Un 'VAL_ARRET_CODE' peut être l'arrêt de plusieurs mêmes bus, voir d'un même bus et d'un même arrêt de métro. Où d'un même bus et d'un même arrêt de tram. \n",
    "    - Je dois donc nommer différement les VAL_ARRET_CODE de chacun des modes. Une proposition est de mettre le mode (B,S,T) devant les id. Comme ça on pourra regrouper sans soucis.\n",
    "- La moyenne des déplacement de la df_subway est de 5 trajet toute les 3 minutes, quelque soit la station et l'heure (d'ouverture) considéré. Max 88.\n",
    "\n",
    "#### Questionnement \n",
    "- Ok on a aggrégé 3 min, mais est-ce qu'on peut recouper les sorties 3min avec les validation + Sortie de métro 15 min? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Validation Individuelles\n",
    "Load 3 df : df_sub, df_tram, df_bus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\romai\\AppData\\Local\\Temp\\ipykernel_1708\\2127861133.py:9: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  globals()[f'df_{name}'] = pd.concat([pd.read_csv(mode_month2path[name][d],index_col = 0) for d in dates])\n",
      "C:\\Users\\romai\\AppData\\Local\\Temp\\ipykernel_1708\\2127861133.py:9: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  globals()[f'df_{name}'] = pd.concat([pd.read_csv(mode_month2path[name][d],index_col = 0) for d in dates])\n",
      "C:\\Users\\romai\\AppData\\Local\\Temp\\ipykernel_1708\\2127861133.py:9: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  globals()[f'df_{name}'] = pd.concat([pd.read_csv(mode_month2path[name][d],index_col = 0) for d in dates])\n",
      "C:\\Users\\romai\\AppData\\Local\\Temp\\ipykernel_1708\\2127861133.py:9: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  globals()[f'df_{name}'] = pd.concat([pd.read_csv(mode_month2path[name][d],index_col = 0) for d in dates])\n",
      "C:\\Users\\romai\\AppData\\Local\\Temp\\ipykernel_1708\\2127861133.py:9: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  globals()[f'df_{name}'] = pd.concat([pd.read_csv(mode_month2path[name][d],index_col = 0) for d in dates])\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'data/'\n",
    "valid_ind_path = folder_path + 'Sub_Tram_11_2019_03_2020'\n",
    "dates = ['11-2019','12-2019','1-2020','2-2020','3-2020']\n",
    "\n",
    "subway_paths, tramway_paths, bus_paths = sorted(glob.glob(os.path.join(valid_ind_path, \"*df_subway*.csv\"))),sorted(glob.glob(os.path.join(valid_ind_path, \"*df_tramway*.csv\"))),sorted(glob.glob(os.path.join(valid_ind_path, \"*df_bus*.csv\")))\n",
    "mode_month2path = get_mode_date2path([subway_paths,tramway_paths,bus_paths],['sub','tram','bus'])\n",
    "\n",
    "for name in ['sub','tram','bus']:\n",
    "    globals()[f'df_{name}'] = pd.concat([pd.read_csv(mode_month2path[name][d],index_col = 0) for d in dates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Subway 15 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>Station</th>\n",
       "      <th>Code ligne</th>\n",
       "      <th>in</th>\n",
       "      <th>out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>Ampère Victor Hugo</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01 00:15:00</td>\n",
       "      <td>Ampère Victor Hugo</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 00:30:00</td>\n",
       "      <td>Ampère Victor Hugo</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01 00:45:00</td>\n",
       "      <td>Ampère Victor Hugo</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01 01:00:00</td>\n",
       "      <td>Ampère Victor Hugo</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime             Station Code ligne  in  out\n",
       "0 2019-01-01 00:00:00  Ampère Victor Hugo          A   2  4.0\n",
       "1 2019-01-01 00:15:00  Ampère Victor Hugo          A   3  2.0\n",
       "2 2019-01-01 00:30:00  Ampère Victor Hugo          A   3  7.0\n",
       "3 2019-01-01 00:45:00  Ampère Victor Hugo          A   1  9.0\n",
       "4 2019-01-01 01:00:00  Ampère Victor Hugo          A   0  0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_path = \"Métro 15 minutes 2019 2020.txt\"\n",
    "\n",
    "df_metro_funi_2019_2020 = load_subway_15_min(folder_path+txt_path)\n",
    "df_metro_funi_2019_2020.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1er Etape : Prédiction Métro\n",
    "- On va d'abord prédire la demande sur une ligne (disons A).  \n",
    "- On va comparer des modèle : LSTM, CNN, CNN-LSTM, GNN.\n",
    "    - A priori pas de \"raison\" que le GNN marche mieux. Si c'est le cas, c'est peut être simplement que le modèle est plus complexe, mais j'ai du mal à croire que si on donne les bonnes informations (historique -7d, -1d, -4,3,2,1t), on a des meilleurs résultats avec GNN. Sauf si il y a des relation asynchrone \"récurrentes\", mais sans causalité. De la même manière que l'historique -7d sert de référence, mais ne témoigne pas d'un lien causal.\n",
    "- Identifier des moments interessants : anomalies sur entrée/sortie métro. Voir les prédictions sur ces moments là particulier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Coverage for each station, IN, OUT, and IN+OUT\n",
    "\n",
    "#### Visualisation des flux IN et OUT entre 6h et 24H. En virant les outliers (0.95) type fête des lumières:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from plotting import coverage_day_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init \n",
    "freq  = '15min'\n",
    "columns = 'hour'\n",
    "index = 'date'\n",
    "quantile = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for each station \n",
    "for station in df_metro_funi_2019_2020.Station.unique():\n",
    "    df_tmps = df_metro_funi_2019_2020[df_metro_funi_2019_2020.Station == station]\n",
    "    df_tmps = df_tmps[df_tmps.datetime.dt.hour > 5]\n",
    "    in99,out99 = df_tmps['in'].quantile(quantile),df_tmps['out'].quantile(quantile)\n",
    "    df_tmps.loc[df_tmps['in']>in99,'in'] = in99\n",
    "    df_tmps.loc[df_tmps['out']>out99,'out'] = out99\n",
    "    coverage_day_month(df_tmps, freq = freq,index = index,columns = columns,save = station,folder_save = 'save/profile flux 15min filtred outliers 95/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation des flux IN et OUT pour chacunes des stations, without filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in df_metro_funi_2019_2020.Station.unique():\n",
    "    df_tmps = df_metro_funi_2019_2020[df_metro_funi_2019_2020.Station == station]\n",
    "    coverage_day_month(df_tmps, freq = '60min',index = 'date',columns = 'hour',save = station,folder_save = 'save/profile flux')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = sub_nov[sub_nov['lane'] =='A'][['COD_TRG','Flow','VAL_DATE']]\n",
    "sub_df = sub_df.groupby(['COD_TRG','VAL_DATE']).sum()\n",
    "sub_df = sub_df.reset_index()\n",
    "sub_df.VAL_DATE = pd.to_datetime(sub_df.VAL_DATE) \n",
    "\n",
    "# Reindex date\n",
    "start,end = sub_df.VAL_DATE.iloc[0],sub_df.VAL_DATE.iloc[-1]\n",
    "date_index = pd.date_range(start = start,end = end, freq = '3min')\n",
    "sub_df = sub_df.pivot(index = 'VAL_DATE',columns = 'COD_TRG',values = 'Flow')\n",
    "sub_df = sub_df.reindex(date_index).fillna(0)\n",
    "\n",
    "#Reindex columns :\n",
    "stations = ['PER','AMP','BEL','COR','HOT','FOC','MAS','CHA','REP','GRA','FLA','CUS','BON','SOI']\n",
    "sub_df = sub_df[stations]\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Vector\n",
    "A définir pour chacune des stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq  ='3min'\n",
    "freq_min = int(freq.split('min')[0])\n",
    "\n",
    "time_step_per_hour = int(60/freq_min) #3min agg\n",
    "\n",
    "historical_len = 7\n",
    "Days = 1\n",
    "Weeks = 1\n",
    "step_ahead = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ut = DataSet(sub_df,time_step_per_hour=time_step_per_hour)\n",
    "norm_Ut = Ut.normalize()    # Normalize before getting the \"Feature vector\"  (or \"Feature Tensor\")\n",
    "(X,Y,dates_verif) = norm_Ut.get_feature_vect(step_ahead,historical_len,Days,Weeks)\n",
    "print('Feature vector shape: ',X.shape)   # Nb Sample, Nb Nodes, Sequence Length\n",
    "dates_verif.head()\n",
    "\n",
    "# Ut.df\n",
    "# Ut.init_df\n",
    "# Ut.time_step_per_hour\n",
    "# Ut.mini\n",
    "# Ut.maxi\n",
    "# Ut.mean \n",
    "# norm_Ut.df.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load adjacency Matrix: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_indep = torch.diag(torch.ones(len(Ut.df.columns)))   # Matrice d'adjacence identité, personne n'est connecté avec personne\n",
    "A_Neighbors = torch.sum(torch.stack([torch.diag(torch.ones(len(Ut.df.columns)-abs(i)),i) for i in [-1,0,1]]),dim =0)   #Une seule ligne de métro, donc tri-diagonale\n",
    "A_learnable = torch.nn.Parameter(torch.randn(len(Ut.df.columns),len(Ut.df.columns)),requires_grad=True)   #Matrice d'adjacence apprentissable\n",
    "\n",
    "# Then convert into \"Laplacian Matrix\", or with \"random_walk Matrix\", or with another one ...\n",
    "A_indep = \n",
    "A_Neighbors =\n",
    "A_learnable = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class graphconv(nn.Module):\n",
    "    def __init__(self,c_in,c_out,graph_conv_act_func,K = 2,enable_bias =True):\n",
    "        super(graphconv,self).__init__()   # Demande a ce qu'on récupère les méthodes de la classe parent :  'nn.module'\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.enable_bias = enable_bias\n",
    "        self.graph_conv_act_func = graph_conv_act_func\n",
    "        self.K = K\n",
    "        if torch.cuda.is_available():\n",
    "            self.weight = nn.Parameter(torch.cuda.FloatTensor(K,c_in,c_out))   # Initialize with wierd weight like 0e-30 or 1e38. Might not be totally adaptated ...\n",
    "        else :\n",
    "            self.weight = nn.Parameter(torch.FloatTensor(K,c_in,c_out))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x,gcnconv_matrix):\n",
    "        B, C, L, N = x.shape\n",
    "        n_mat =  gcnconv_matrix.shape[0]\n",
    "\n",
    "        x = x.reshape(-1, self.c_in)  #[B, C_in, L, N] -> [BLN, C_in]\n",
    "        x = torch.einsum('ab, cbd->cad',x,self.weight)   # [BLN,C_in], [K,C_in,C_out] -> [K,BLN,C_out]\n",
    "        x = x.view(self.K, B*L,N,-1)  #[K,BLN,C_out] ->  [K,BL,N,C_out] \n",
    "        x = torch.einsum('ecab,ecbd->ecad',gcnconv_matrix,x)  #[n_adj,BL,N1,N2] ,[K,BL,N2,C_out]  -> [K,BL,N1,C_out] \n",
    "\n",
    "        if self.enable_bias:\n",
    "            x = x + self.bias\n",
    "        \n",
    "        x = x.view().permute()\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,1,3,4)\n",
    "A_indep = torch.ones(3,3)\n",
    "\n",
    "C_in = x.shape[1]\n",
    "C_out = 8\n",
    "weight = torch.FloatTensor(C_in,C_out)\n",
    "weight = torch.Tensor([[1,2,3,4,5,6,7,8]])\n",
    "\n",
    "adj_matrix =A_indep\n",
    "# x.shape = [B,C,N2,L]\n",
    "B,C_in,N2,L = x.shape\n",
    "# A.shape = [n_adj,N1,N2]\n",
    "N1,N2 = adj_matrix.shape\n",
    "#A.shape = [n_adj,B,N1,N2]\n",
    "stacked_adj = adj_matrix.repeat(1,B,1,1)\n",
    "\n",
    "\n",
    "embedding = torch.einsum('bcnl,ch -> bhnl',x,weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MGCN_conv(nn.Module):\n",
    "    def __init__(self,c_in,c_out,n_adj=2,enable_bias =True):\n",
    "        super(MGCN_conv,self).__init__()\n",
    "        self.n_adj = n_adj    \n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.weight = torch.FloatTensor(n_adj,c_in,c_out)  #[C_in,C_out]   ou pour Einsum: [l,h]\n",
    "        self.bias = torch.FloatTensor(c_out)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x,adj_matrix):\n",
    "        # x.shape = [B,N2,L]\n",
    "        B,N2,L = x.shape\n",
    "        # A.shape = [n_adj,N1,N2]\n",
    "        n_adj,N1,N2 = adj_matrix.shape\n",
    "\n",
    "        #adj_matrix = torch.unsqueeze(adj_matrix,0) #Créer un nouvel axe \n",
    "        stacked_adj = adj_matrix.repeat(B,1,1) # Concat tout les \"samples\" (B batch, L tmeporal dimension) long du nouvel axe\n",
    "        stacked_adj = stacked_adj.reshape(-1,N1,N2)  # Stack les n_adj adjacency matrix B*L fois     shape: [n_adj*B, N1,N2] et pour einsum : [b,p,n]\n",
    "\n",
    "        embedding = torch.einsum('bnl,klh -> bknh',x,self.weight)   # Embedding sur les feature de chaque Noeud\n",
    "        reshaped_embedding = embedding.reshape(-1,N2,self.c_out)    #Stack la dimension k sur l'axe 0  [n_adj*B,N2,C_out]  et pour einsum : [b,n,h]\n",
    "\n",
    "        convoluted = torch.einsum('bpn,bnh -> bph',stacked_adj,reshaped_embedding) \n",
    "        convoluted = convoluted.reshape(self.n_adj,B,N1,self.c_out)      #Unstack la dimension 0, pour séparer les n_adj matrices d'adjacences\n",
    "\n",
    "        convoluted = self.relu(convoluted + self.bias)\n",
    "\n",
    "        return(convoluted,embedding,reshaped_embedding)\n",
    "\n",
    "# Test : \n",
    "N1,N2 = 3,3\n",
    "adj_matrix = torch.stack([torch.diag(torch.ones(N1)),torch.randn(N1,N2)],dim = 0)  # Matrice d'adjacence identité, personne n'est connecté avec personne\n",
    "n_adj = adj_matrix.shape[0]\n",
    "\n",
    "B = 4\n",
    "L = 6\n",
    "x = torch.randn(B,N1,L)\n",
    "\n",
    "c_out = 16\n",
    "gcn_model = MGCN_conv(L,c_out,n_adj)\n",
    "AXW,embedding,reshaped_embedding = gcn_model(x,adj_matrix)\n",
    "\n",
    "print(\"L'embedding a d'abord été opéré sur la dernière dimension (X*W, temporelle), Puis la convolution (A*(XW)) a sommmé les embedding de chacun des voisins (ou Noeud en lien avec le noeud tagret).\")\n",
    "print(f'X.shape: {x.shape}, AXW.shape:{AXW.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.shape,reshaped_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bijecction entre un entier, et son label\n",
    "def int2lab(int,n_adj=2,B=3):\n",
    "    n = 1+(int-1)//B \n",
    "    b = int-(n-1)*B\n",
    "    return(f'n{n}_b{b}')\n",
    "\n",
    "def lab2int(lab,n_adj=2,B=3):\n",
    "    nb = lab.split('_')\n",
    "    n,b = int(nb[0][1:]),int(nb[1][1:])\n",
    "    return((n-1)*B+b)\n",
    "\n",
    "for k in range(1,15):\n",
    "    print(int2lab(k),lab2int(int2lab(k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentation des numéro des cellules du Tensor\n",
    "Permet de faire des affichages graphique, et de s'assurer que les \".reshape\" font ce que l'on souhaite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int2lab(integer, n_adj=2, B=3, C=4, N=5, L=6):\n",
    "    N_adj_L = N * L\n",
    "    N_adj_C_L = C * N_adj_L\n",
    "    N_adj_B_C_L = B * N_adj_C_L\n",
    "\n",
    "    l = 1 + ((integer - 1) % L)\n",
    "    remaining = (integer - 1) // L\n",
    "    n = 1 + (remaining % N)\n",
    "    remaining //= N\n",
    "    c = 1 + (remaining % C)\n",
    "    remaining //= C\n",
    "    b = 1 + (remaining % B)\n",
    "    remaining //= B\n",
    "    n_adj = 1 + remaining % n_adj\n",
    "\n",
    "    return f'adj{n_adj}_b{b}_c{c}_n{n}_l{l}'\n",
    "\n",
    "def lab2int(label, n_adj=2, B=3, C=4, N=5, L=6):\n",
    "    split_label = label.split('_')\n",
    "    n_adj = int(split_label[0][3:])\n",
    "    b = int(split_label[1][1:])\n",
    "    c = int(split_label[2][1:])\n",
    "    n = int(split_label[3][1:])\n",
    "    l = int(split_label[4][1:])\n",
    "\n",
    "    N_adj_L = N * L\n",
    "    N_adj_C_L = C * N_adj_L\n",
    "    N_adj_B_C_L = B * N_adj_C_L\n",
    "\n",
    "    integer = ((l - 1) + (n - 1) * L + (c - 1) * N * L + (b - 1) * C * N * L + (n_adj - 1) * B * C * N * L) + 1\n",
    "    return integer\n",
    "\n",
    "\n",
    "for k in range(1,400):\n",
    "    print(int2lab(k),lab2int(int2lab(k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = int2lab(k)\n",
    "label.split('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_conv(nn.Module):\n",
    "    def __init__(self,c_in,c_out,graph_conv_act_func,K=1,enable_bias =True):\n",
    "        super(GCN_conv,self).__init__()\n",
    "\n",
    "    \n",
    "    def forward(self,x,adj_matrix):\n",
    "        # x.shape = [B,C,N2,L]\n",
    "        B,C,N2,L = x.shape\n",
    "        # A.shape = [n_adj,N1,N2]\n",
    "        n_adj,N1,N2 = adj_matrix.shape\n",
    "        #A.shape = [n_adj,B,N1,N2]\n",
    "        adj_matrix = torch.unsqueeze(adj_matrix,0) #Créer un nouvel axe \n",
    "        stacked_adj = adj_matrix.repeat(B*L,1,1,1) # Concat tout les \"samples\" (B batch, L tmeporal dimension) long du nouvel axe\n",
    "        stacked_adj = stacked_adj.reshape(-1,N1,N2)  # Stack les n_adj adjacency matrix B*L fois\n",
    "\n",
    "        embedding = torch.einsum('bcnl,ch -> bhnl',x,self.weight)   # Embedding Spatial le long de C_in\n",
    "        embedding = embedding.permute(0,3,2,1)  # [B,L,N2,C_out]\n",
    "        embedding = embedding.reshape(-1,N2,C_out)  # [BL,N2,C_out]\n",
    "\n",
    "        convolution = torch.einsum('kbpn,bhnl -> kbpl',stacked_adj,embedding)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_adj_matrix = adj_matrix.repeat(2,1,1)\n",
    "n_adj,N1,N2 = n_adj_matrix.shape\n",
    "#A.shape = [n_adj,B,N1,N2]\n",
    "stacked_adj = n_adj_matrix.repeat(B*L,1,1) # [n_adj*B, N1,N2]  -> Concat tout les \"samples\" long de l'axe 0\n",
    "stacked_adj.shape,n_adj_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X.unsqueeze(1) #add the channel dimension (here, only \"flow')\n",
    "print(f'X shape: {x.shape}')\n",
    "x_b = x[:32]\n",
    "print(f'x_b shape: {x_b.shape}')\n",
    "\n",
    "GCN = graphconv(c_in = x.shape[1], c_out = 64, K=2, graph_conv_act_func = 'relu',enable_bias=True)  # K =2 dans MRGNN car considère Pattern et Adj matrix\n",
    "B, C, N, L = x_b.shape\n",
    "K = 1\n",
    "c_out = 64\n",
    "n_mat =  A_indep.shape[0]\n",
    "\n",
    "print(f'B,C,N,L : [{B},{C},{N},{L}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN détaillé "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = nn.Parameter(torch.FloatTensor(K,C,c_out))\n",
    "bias = nn.Parameter(torch.FloatTensor(c_out))\n",
    "print(f'x_b: {x_b.shape}, W: {weight.shape}, Cause there are {K} adjacency matrices (K), {C} C_in and {c_out} C_out')\n",
    "x_b = x_b.reshape(-1, C)  #[B, C_in, L, N] -> [BLN, C_in]\n",
    "print(f'reshaped x_b: {x_b.shape}, Cause we have to flatten x_b along the Channel axis, and then pass through a SPATIAL embedding (Linear layer, on C_in of each sample,nodes,historical element)')\n",
    "\n",
    "# Embedding on C_in:  X*W\n",
    "embedd_c_in = torch.einsum('ab, cbd->cad',x_b,weight)   # [BLN,C_in], [K,C_in,C_out] -> [K,BLN,C_out]n  Propose K embedding de C_in\n",
    "print('shape of reshaped_xb after K embedding on C_in: ',embedd_c_in.shape)\n",
    "reshaped_embedd_c_in = embedd_c_in.view(K, B*L,N,-1)  #[K,BLN,C_out] ->  [K,BL,N,C_out] \n",
    "print('Embedded feature vect reshaped: ',reshaped_embedd_c_in.shape, '\\n')\n",
    "\n",
    "# Concat Adj Matrix \n",
    "batched_adj_matrix = A_indep.repeat(1,B*L,1,1)\n",
    "print('Adjacency matrix A_indep: ',A_indep.shape,'batched Multi adj_matrix: ',batched_adj_matrix.shape)\n",
    "\n",
    "# Convolution A*(XW)\n",
    "convolutionned = torch.einsum('ecab,ecbd->ecad',batched_adj_matrix,reshaped_embedd_c_in)  #[K,BL,N1,N2] ,[K,BL,N2,C_out]  -> [K,BL,N1,C_out] \n",
    "print('Convolution A*(XW): ',convolutionned.shape)\n",
    "\n",
    "#Add bias: \n",
    "convolutionned_n_biased = convolutionned + bias\n",
    "\n",
    "# Reshape and Permute: \n",
    "H =convolutionned_n_biased.view(K,B,L,-1,c_out).permute(0,1,4,3,2)\n",
    "print(f'Reshaped convolution output: {H.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_H = output_H.permute(0, 1, 3, 4, 2)\n",
    "print(permuted_H.shape)\n",
    "\n",
    "new_c_in = permuted_H.shape[-1]\n",
    "first_attn = permuted_H.reshape(K,-1,new_c_in)\n",
    "print(first_attn.shape)  # [K, B*L*N, C_in']\n",
    "\n",
    "attn_weight = nn.Linear(new_c_in,1)\n",
    "softmax = nn.Softmax(-1)  #SoftMax on the last dimension \n",
    "\n",
    "#Coefficient d'attention \n",
    "attn = attn_weight(first_attn)    # \"Embedding\" du spatial channel \n",
    "attn = attn.permute(1,2,0)  # Permute pour avoir la nombre d'adjacency matrix en dernière dimension\n",
    "attn = softmax(attn)  # Coefficient d'attention pour chaque Matrices d'adjacence et Embedding spatial assicié  (Ici 1 seule matrice d'adjacence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(attn == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2,3,4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if self.enable_bias:\n",
    "    x = x + self.bias\n",
    "\n",
    "x = x.view().permute()\n",
    "x = self.relu(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Preprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
