{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from datetime import datetime,timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "\n",
    "# Personnal Import \n",
    "from utilities import DataSet, get_batch\n",
    "from dl_models.graph_conv import graphconv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "- Un 'VAL_ARRET_CODE' peut être l'arrêt de plusieurs mêmes bus, voir d'un même bus et d'un même arrêt de métro. Où d'un même bus et d'un même arrêt de tram. \n",
    "    - Je dois donc nommer différement les VAL_ARRET_CODE de chacun des modes. Une proposition est de mettre le mode (B,S,T) devant les id. Comme ça on pourra regrouper sans soucis.\n",
    "- La moyenne des déplacement de la df_subway est de 5 trajet toute les 3 minutes, quelque soit la station et l'heure (d'ouverture) considéré. Max 88.\n",
    "\n",
    "- Ok on a aggrégé 3 min, mais est-ce qu'on peut recouper les sorties 3min avec les validation + Sortie de métro 15 min? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On commence par simple\n",
    "Restons sur de la prédiction.\n",
    "On va sortir un modèle qui fait de la prédiction court-terme.\n",
    "- Un model plutôt \"image\"\n",
    "- Un model plutôt \"graph\"\n",
    "Puis on les adaptera au fur et a mesure.\n",
    "## Quoi prédire, pourquoi ?\n",
    "- Prédiction Métro : Pour 'manager l'offre' (augmenter ou non la fréquence des transports), allouer des membre de staff, prévoir des évacuations d'urgence.  -> Toute la litérature cite ces 'scénarios', mais peu évaluent effectivement leur accuracy dessus. Ils donnent généralement des accuracy moyenne. \n",
    "## Focalisons sur un certains type d'événement.\n",
    "- Match de sport ou concert en général c'est pas intéressant.\n",
    "    - Parce que l'événement est prévu et récurrent, on sait le nombre de gens attendu. Donc l'offre de transport est adapté en amont, la fréquences des rames et TC alternatif sont prévu. Autre exemple, on pose des Vélo'v en masse dans une station fictive en sortie d'un match de foot. Alors c'est pas un événement intéressant. Biensur que si on entraine notre modèle avec ces événements assez récurrent et qu'on mets les bonnes informations, la prédiction court-terme pourra être améliorée. Mais à quoi bon ? \n",
    "- Match Sportif, Concert, ou Evenement de très grande ampleur : \n",
    "    - Concert de Ed Sheran, Finale d'Europa League : ce genre d'événement peut modifier fortement la demande en trafic sur tout le réseau, et il ne peut pas être correctement prévu. Les spectateur venant pour quelque jours, ils peuvent se donner rendez-vous a un endroit dans Lyon qui n'est pas forcément proche du Groupama. Exemple : des Holigans se donnent rendez-vous place des terreaux pour boire avant le match. Comment Anticiper l'augmentation de la demande en TC ? Pourquoi on a un intérêt à l'anticiper ? La seule raison que j'ai en tête c'est pour allouer des membre de staffs aux entrée /sorties pour la sécurité des usagers et maintenir la fréquence de l'offre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'data/Sub_Tram_11_2019_03_2020'\n",
    "subway_paths, tramway_paths = sorted(glob.glob(os.path.join(folder_path, \"*df_subway*.csv\"))),sorted(glob.glob(os.path.join(folder_path, \"*df_tramway*.csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_from_month_offset(n):\n",
    "    n = int(n)\n",
    "    reference_date = datetime(2019, 10, 1)\n",
    "    years, months = divmod(n, 12)\n",
    "    result_date = reference_date + timedelta(days=365 * years) + relativedelta(months=months)\n",
    "    return f'{result_date.date().month}-{result_date.date().year}'\n",
    "dic = {}\n",
    "for list_paths,mode in zip([subway_paths,tramway_paths],['sub','tram']):\n",
    "    dic[mode] = {}\n",
    "    for csv_path in list_paths:\n",
    "        month = csv_path.split('/')[2].split('_')[0]\n",
    "        dic[mode][date_from_month_offset(month)] = csv_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['11-2019','12-2019','1-2020','2-2020','3-2020']\n",
    "d = dates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIG_NUMERO_SAE</th>\n",
       "      <th>EMP_CEB</th>\n",
       "      <th>VAL_ARRET_CODE</th>\n",
       "      <th>CRS_SENS_TRAJET</th>\n",
       "      <th>VAL_DATE</th>\n",
       "      <th>Flow</th>\n",
       "      <th>COD_TRG</th>\n",
       "      <th>lane</th>\n",
       "      <th>LIB_STA_SIFO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>VMMA-BON---1--1--1-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-20 04:36:00</td>\n",
       "      <td>3</td>\n",
       "      <td>BON</td>\n",
       "      <td>A</td>\n",
       "      <td>LAURENT BONNEVAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>VMMA-BON---1--1--1-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-20 04:57:00</td>\n",
       "      <td>7</td>\n",
       "      <td>BON</td>\n",
       "      <td>A</td>\n",
       "      <td>LAURENT BONNEVAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>VMMA-BON---1--1--1-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-20 05:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>BON</td>\n",
       "      <td>A</td>\n",
       "      <td>LAURENT BONNEVAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>VMMA-BON---1--1--1-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-20 05:03:00</td>\n",
       "      <td>3</td>\n",
       "      <td>BON</td>\n",
       "      <td>A</td>\n",
       "      <td>LAURENT BONNEVAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>VMMA-BON---1--1--1-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-20 05:12:00</td>\n",
       "      <td>1</td>\n",
       "      <td>BON</td>\n",
       "      <td>A</td>\n",
       "      <td>LAURENT BONNEVAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938824</th>\n",
       "      <td>304</td>\n",
       "      <td>VMMD-VAI---4--31-99</td>\n",
       "      <td>503</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-11-29 16:03:00</td>\n",
       "      <td>1</td>\n",
       "      <td>VAI</td>\n",
       "      <td>D</td>\n",
       "      <td>GARE DE VAISE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938825</th>\n",
       "      <td>304</td>\n",
       "      <td>VMMD-VAI---4--31-99</td>\n",
       "      <td>503</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-11-29 16:06:00</td>\n",
       "      <td>1</td>\n",
       "      <td>VAI</td>\n",
       "      <td>D</td>\n",
       "      <td>GARE DE VAISE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938826</th>\n",
       "      <td>304</td>\n",
       "      <td>VMMD-VAI---4--31-99</td>\n",
       "      <td>503</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-11-29 16:48:00</td>\n",
       "      <td>1</td>\n",
       "      <td>VAI</td>\n",
       "      <td>D</td>\n",
       "      <td>GARE DE VAISE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938827</th>\n",
       "      <td>304</td>\n",
       "      <td>VMMD-VAI---4--31-99</td>\n",
       "      <td>503</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-11-29 17:54:00</td>\n",
       "      <td>1</td>\n",
       "      <td>VAI</td>\n",
       "      <td>D</td>\n",
       "      <td>GARE DE VAISE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938828</th>\n",
       "      <td>304</td>\n",
       "      <td>VMMD-VAI---4--31-99</td>\n",
       "      <td>503</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-11-29 23:33:00</td>\n",
       "      <td>1</td>\n",
       "      <td>VAI</td>\n",
       "      <td>D</td>\n",
       "      <td>GARE DE VAISE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2938829 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         LIG_NUMERO_SAE              EMP_CEB  VAL_ARRET_CODE  CRS_SENS_TRAJET  \\\n",
       "0                     0  VMMA-BON---1--1--1-               0                0   \n",
       "1                     0  VMMA-BON---1--1--1-               0                0   \n",
       "2                     0  VMMA-BON---1--1--1-               0                0   \n",
       "3                     0  VMMA-BON---1--1--1-               0                0   \n",
       "4                     0  VMMA-BON---1--1--1-               0                0   \n",
       "...                 ...                  ...             ...              ...   \n",
       "2938824             304  VMMD-VAI---4--31-99             503                2   \n",
       "2938825             304  VMMD-VAI---4--31-99             503                2   \n",
       "2938826             304  VMMD-VAI---4--31-99             503                2   \n",
       "2938827             304  VMMD-VAI---4--31-99             503                2   \n",
       "2938828             304  VMMD-VAI---4--31-99             503                2   \n",
       "\n",
       "                    VAL_DATE  Flow COD_TRG lane      LIB_STA_SIFO  \n",
       "0        2019-11-20 04:36:00     3     BON    A  LAURENT BONNEVAY  \n",
       "1        2019-11-20 04:57:00     7     BON    A  LAURENT BONNEVAY  \n",
       "2        2019-11-20 05:00:00     2     BON    A  LAURENT BONNEVAY  \n",
       "3        2019-11-20 05:03:00     3     BON    A  LAURENT BONNEVAY  \n",
       "4        2019-11-20 05:12:00     1     BON    A  LAURENT BONNEVAY  \n",
       "...                      ...   ...     ...  ...               ...  \n",
       "2938824  2019-11-29 16:03:00     1     VAI    D     GARE DE VAISE  \n",
       "2938825  2019-11-29 16:06:00     1     VAI    D     GARE DE VAISE  \n",
       "2938826  2019-11-29 16:48:00     1     VAI    D     GARE DE VAISE  \n",
       "2938827  2019-11-29 17:54:00     1     VAI    D     GARE DE VAISE  \n",
       "2938828  2019-11-29 23:33:00     1     VAI    D     GARE DE VAISE  \n",
       "\n",
       "[2938829 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_nov = pd.read_csv(dic['sub'][d],index_col = 0)\n",
    "sub_nov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIG_NUMERO_SAE</th>\n",
       "      <th>EMP_CEB</th>\n",
       "      <th>VAL_ARRET_CODE</th>\n",
       "      <th>CRS_SENS_TRAJET</th>\n",
       "      <th>VAL_DATE</th>\n",
       "      <th>Flow</th>\n",
       "      <th>NOM_PNT</th>\n",
       "      <th>COD_LIG_CLI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>530</td>\n",
       "      <td>VET--------832---10</td>\n",
       "      <td>32766</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-06 17:03:00</td>\n",
       "      <td>2</td>\n",
       "      <td>Passage machine à laver UTT</td>\n",
       "      <td>T2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>530</td>\n",
       "      <td>VET--------840---6-</td>\n",
       "      <td>32766</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-22 05:39:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Passage machine à laver UTT</td>\n",
       "      <td>T2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>530</td>\n",
       "      <td>VET--------840---7-</td>\n",
       "      <td>32766</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-22 05:39:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Passage machine à laver UTT</td>\n",
       "      <td>T2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>530</td>\n",
       "      <td>VET--------843---7-</td>\n",
       "      <td>32766</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-04 04:27:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Passage machine à laver UTT</td>\n",
       "      <td>T2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>530</td>\n",
       "      <td>VET--------847---9-</td>\n",
       "      <td>32766</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-16 09:27:00</td>\n",
       "      <td>2</td>\n",
       "      <td>Passage machine à laver UTT</td>\n",
       "      <td>T2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034012</th>\n",
       "      <td>580</td>\n",
       "      <td>VET--------884---8-</td>\n",
       "      <td>46605</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-16 19:27:00</td>\n",
       "      <td>4</td>\n",
       "      <td>Garage entrée Parc OL Ouest</td>\n",
       "      <td>TS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034013</th>\n",
       "      <td>580</td>\n",
       "      <td>VET--------884---8-</td>\n",
       "      <td>46605</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-16 19:33:00</td>\n",
       "      <td>2</td>\n",
       "      <td>Garage entrée Parc OL Ouest</td>\n",
       "      <td>TS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034014</th>\n",
       "      <td>580</td>\n",
       "      <td>VET--------884---9-</td>\n",
       "      <td>46605</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-16 19:30:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Garage entrée Parc OL Ouest</td>\n",
       "      <td>TS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034015</th>\n",
       "      <td>580</td>\n",
       "      <td>VET--------889---13</td>\n",
       "      <td>46605</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-05 22:54:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Garage entrée Parc OL Ouest</td>\n",
       "      <td>TS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034016</th>\n",
       "      <td>580</td>\n",
       "      <td>VET--------889---16</td>\n",
       "      <td>46605</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-05 22:54:00</td>\n",
       "      <td>2</td>\n",
       "      <td>Garage entrée Parc OL Ouest</td>\n",
       "      <td>TS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3034017 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         LIG_NUMERO_SAE              EMP_CEB  VAL_ARRET_CODE  CRS_SENS_TRAJET  \\\n",
       "0                   530  VET--------832---10           32766                0   \n",
       "1                   530  VET--------840---6-           32766                0   \n",
       "2                   530  VET--------840---7-           32766                0   \n",
       "3                   530  VET--------843---7-           32766                0   \n",
       "4                   530  VET--------847---9-           32766                0   \n",
       "...                 ...                  ...             ...              ...   \n",
       "3034012             580  VET--------884---8-           46605                0   \n",
       "3034013             580  VET--------884---8-           46605                0   \n",
       "3034014             580  VET--------884---9-           46605                0   \n",
       "3034015             580  VET--------889---13           46605                0   \n",
       "3034016             580  VET--------889---16           46605                0   \n",
       "\n",
       "                    VAL_DATE  Flow                      NOM_PNT COD_LIG_CLI  \n",
       "0        2019-11-06 17:03:00     2  Passage machine à laver UTT          T2  \n",
       "1        2019-11-22 05:39:00     1  Passage machine à laver UTT          T2  \n",
       "2        2019-11-22 05:39:00     1  Passage machine à laver UTT          T2  \n",
       "3        2019-11-04 04:27:00     1  Passage machine à laver UTT          T2  \n",
       "4        2019-11-16 09:27:00     2  Passage machine à laver UTT          T2  \n",
       "...                      ...   ...                          ...         ...  \n",
       "3034012  2019-11-16 19:27:00     4  Garage entrée Parc OL Ouest          TS  \n",
       "3034013  2019-11-16 19:33:00     2  Garage entrée Parc OL Ouest          TS  \n",
       "3034014  2019-11-16 19:30:00     3  Garage entrée Parc OL Ouest          TS  \n",
       "3034015  2019-11-05 22:54:00     1  Garage entrée Parc OL Ouest          TS  \n",
       "3034016  2019-11-05 22:54:00     2  Garage entrée Parc OL Ouest          TS  \n",
       "\n",
       "[3034017 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tram_nov = pd.read_csv(dic['tram'][d],index_col = 0)\n",
    "tram_nov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On va faire de la prédiction sur les validations Métro.  \n",
    "- On va d'abord prédire la demande sur une ligne (disons A).  \n",
    "- On va comparer des modèle : LSTM, CNN, CNN-LSTM, GNN.\n",
    "    - A priori pas de \"raison\" que le GNN marche mieux. Si c'est le cas, c'est peut être simplement que le modèle est plus complexe, mais j'ai du mal à croire que si on donne les bonnes informations (historique -7d, -1d, -4,3,2,1t), on a des meilleurs résultats avec GNN. Sauf si il y a des relation asynchrone \"récurrentes\", mais sans causalité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>COD_TRG</th>\n",
       "      <th>PER</th>\n",
       "      <th>AMP</th>\n",
       "      <th>BEL</th>\n",
       "      <th>COR</th>\n",
       "      <th>HOT</th>\n",
       "      <th>FOC</th>\n",
       "      <th>MAS</th>\n",
       "      <th>CHA</th>\n",
       "      <th>REP</th>\n",
       "      <th>GRA</th>\n",
       "      <th>FLA</th>\n",
       "      <th>CUS</th>\n",
       "      <th>BON</th>\n",
       "      <th>SOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-11-01 00:00:00</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01 00:03:00</th>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01 00:06:00</th>\n",
       "      <td>19.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01 00:09:00</th>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01 00:12:00</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "COD_TRG               PER   AMP   BEL   COR   HOT  FOC   MAS   CHA  REP   GRA  \\\n",
       "2019-11-01 00:00:00  10.0  10.0  29.0  14.0  43.0  2.0  13.0   4.0  6.0  19.0   \n",
       "2019-11-01 00:03:00  28.0   2.0  18.0  32.0  41.0  3.0   7.0  16.0  2.0   6.0   \n",
       "2019-11-01 00:06:00  19.0   5.0  23.0  16.0  41.0  5.0  10.0   7.0  1.0   9.0   \n",
       "2019-11-01 00:09:00  19.0   3.0  29.0   8.0  64.0  0.0  12.0   0.0  2.0   2.0   \n",
       "2019-11-01 00:12:00  13.0   0.0  25.0  18.0  34.0  3.0   3.0  15.0  1.0   1.0   \n",
       "\n",
       "COD_TRG              FLA  CUS  BON  SOI  \n",
       "2019-11-01 00:00:00  2.0  0.0  1.0  1.0  \n",
       "2019-11-01 00:03:00  0.0  2.0  2.0  1.0  \n",
       "2019-11-01 00:06:00  1.0  0.0  0.0  0.0  \n",
       "2019-11-01 00:09:00  0.0  0.0  0.0  0.0  \n",
       "2019-11-01 00:12:00  0.0  3.0  0.0  1.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = sub_nov[sub_nov['lane'] =='A'][['COD_TRG','Flow','VAL_DATE']]\n",
    "sub_df = sub_df.groupby(['COD_TRG','VAL_DATE']).sum()\n",
    "sub_df = sub_df.reset_index()\n",
    "sub_df.VAL_DATE = pd.to_datetime(sub_df.VAL_DATE) \n",
    "\n",
    "# Reindex date\n",
    "start,end = sub_df.VAL_DATE.iloc[0],sub_df.VAL_DATE.iloc[-1]\n",
    "date_index = pd.date_range(start = start,end = end, freq = '3min')\n",
    "sub_df = sub_df.pivot(index = 'VAL_DATE',columns = 'COD_TRG',values = 'Flow')\n",
    "sub_df = sub_df.reindex(date_index).fillna(0)\n",
    "\n",
    "#Reindex columns :\n",
    "stations = ['PER','AMP','BEL','COR','HOT','FOC','MAS','CHA','REP','GRA','FLA','CUS','BON','SOI']\n",
    "sub_df = sub_df[stations]\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Vector\n",
    "A définir pour chacune des stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq  ='3min'\n",
    "freq_min = int(freq.split('min')[0])\n",
    "\n",
    "time_step_per_hour = int(60/freq_min) #3min agg\n",
    "\n",
    "historical_len = 7\n",
    "Days = 1\n",
    "Weeks = 1\n",
    "step_ahead = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector shape:  torch.Size([10560, 14, 9])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t-3360</th>\n",
       "      <th>t-480</th>\n",
       "      <th>t-7</th>\n",
       "      <th>t-6</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3360</th>\n",
       "      <td>2019-11-01 00:00:00</td>\n",
       "      <td>2019-11-07 00:00:00</td>\n",
       "      <td>2019-11-07 23:12:00</td>\n",
       "      <td>2019-11-07 23:15:00</td>\n",
       "      <td>2019-11-07 23:18:00</td>\n",
       "      <td>2019-11-07 23:21:00</td>\n",
       "      <td>2019-11-07 23:24:00</td>\n",
       "      <td>2019-11-07 23:27:00</td>\n",
       "      <td>2019-11-07 23:30:00</td>\n",
       "      <td>2019-11-08 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3361</th>\n",
       "      <td>2019-11-01 00:03:00</td>\n",
       "      <td>2019-11-07 00:03:00</td>\n",
       "      <td>2019-11-07 23:15:00</td>\n",
       "      <td>2019-11-07 23:18:00</td>\n",
       "      <td>2019-11-07 23:21:00</td>\n",
       "      <td>2019-11-07 23:24:00</td>\n",
       "      <td>2019-11-07 23:27:00</td>\n",
       "      <td>2019-11-07 23:30:00</td>\n",
       "      <td>2019-11-07 23:33:00</td>\n",
       "      <td>2019-11-08 00:03:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>2019-11-01 00:06:00</td>\n",
       "      <td>2019-11-07 00:06:00</td>\n",
       "      <td>2019-11-07 23:18:00</td>\n",
       "      <td>2019-11-07 23:21:00</td>\n",
       "      <td>2019-11-07 23:24:00</td>\n",
       "      <td>2019-11-07 23:27:00</td>\n",
       "      <td>2019-11-07 23:30:00</td>\n",
       "      <td>2019-11-07 23:33:00</td>\n",
       "      <td>2019-11-07 23:36:00</td>\n",
       "      <td>2019-11-08 00:06:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>2019-11-01 00:09:00</td>\n",
       "      <td>2019-11-07 00:09:00</td>\n",
       "      <td>2019-11-07 23:21:00</td>\n",
       "      <td>2019-11-07 23:24:00</td>\n",
       "      <td>2019-11-07 23:27:00</td>\n",
       "      <td>2019-11-07 23:30:00</td>\n",
       "      <td>2019-11-07 23:33:00</td>\n",
       "      <td>2019-11-07 23:36:00</td>\n",
       "      <td>2019-11-07 23:39:00</td>\n",
       "      <td>2019-11-08 00:09:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3364</th>\n",
       "      <td>2019-11-01 00:12:00</td>\n",
       "      <td>2019-11-07 00:12:00</td>\n",
       "      <td>2019-11-07 23:24:00</td>\n",
       "      <td>2019-11-07 23:27:00</td>\n",
       "      <td>2019-11-07 23:30:00</td>\n",
       "      <td>2019-11-07 23:33:00</td>\n",
       "      <td>2019-11-07 23:36:00</td>\n",
       "      <td>2019-11-07 23:39:00</td>\n",
       "      <td>2019-11-07 23:42:00</td>\n",
       "      <td>2019-11-08 00:12:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  t-3360               t-480                 t-7  \\\n",
       "3360 2019-11-01 00:00:00 2019-11-07 00:00:00 2019-11-07 23:12:00   \n",
       "3361 2019-11-01 00:03:00 2019-11-07 00:03:00 2019-11-07 23:15:00   \n",
       "3362 2019-11-01 00:06:00 2019-11-07 00:06:00 2019-11-07 23:18:00   \n",
       "3363 2019-11-01 00:09:00 2019-11-07 00:09:00 2019-11-07 23:21:00   \n",
       "3364 2019-11-01 00:12:00 2019-11-07 00:12:00 2019-11-07 23:24:00   \n",
       "\n",
       "                     t-6                 t-5                 t-4  \\\n",
       "3360 2019-11-07 23:15:00 2019-11-07 23:18:00 2019-11-07 23:21:00   \n",
       "3361 2019-11-07 23:18:00 2019-11-07 23:21:00 2019-11-07 23:24:00   \n",
       "3362 2019-11-07 23:21:00 2019-11-07 23:24:00 2019-11-07 23:27:00   \n",
       "3363 2019-11-07 23:24:00 2019-11-07 23:27:00 2019-11-07 23:30:00   \n",
       "3364 2019-11-07 23:27:00 2019-11-07 23:30:00 2019-11-07 23:33:00   \n",
       "\n",
       "                     t-3                 t-2                 t-1  \\\n",
       "3360 2019-11-07 23:24:00 2019-11-07 23:27:00 2019-11-07 23:30:00   \n",
       "3361 2019-11-07 23:27:00 2019-11-07 23:30:00 2019-11-07 23:33:00   \n",
       "3362 2019-11-07 23:30:00 2019-11-07 23:33:00 2019-11-07 23:36:00   \n",
       "3363 2019-11-07 23:33:00 2019-11-07 23:36:00 2019-11-07 23:39:00   \n",
       "3364 2019-11-07 23:36:00 2019-11-07 23:39:00 2019-11-07 23:42:00   \n",
       "\n",
       "                       t  \n",
       "3360 2019-11-08 00:00:00  \n",
       "3361 2019-11-08 00:03:00  \n",
       "3362 2019-11-08 00:06:00  \n",
       "3363 2019-11-08 00:09:00  \n",
       "3364 2019-11-08 00:12:00  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ut = DataSet(sub_df,time_step_per_hour=time_step_per_hour)\n",
    "norm_Ut = Ut.normalize()    # Normalize before getting the \"Feature vector\"  (or \"Feature Tensor\")\n",
    "(X,Y,dates_verif) = norm_Ut.get_feature_vect(step_ahead,historical_len,Days,Weeks)\n",
    "print('Feature vector shape: ',X.shape)   # Nb Sample, Nb Nodes, Sequence Length\n",
    "dates_verif.head()\n",
    "\n",
    "# Ut.df\n",
    "# Ut.init_df\n",
    "# Ut.time_step_per_hour\n",
    "# Ut.mini\n",
    "# Ut.maxi\n",
    "# Ut.mean \n",
    "# norm_Ut.df.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load adjacency Matrix: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_indep = torch.diag(torch.ones(len(Ut.df.columns)))   # Matrice d'adjacence identité, personne n'est connecté avec personne\n",
    "A_Neighbors = torch.sum(torch.stack([torch.diag(torch.ones(len(Ut.df.columns)-abs(i)),i) for i in [-1,0,1]]),dim =0)   #Une seule ligne de métro, donc tri-diagonale\n",
    "A_learnable = torch.nn.Parameter(torch.randn(len(Ut.df.columns),len(Ut.df.columns)),requires_grad=True)   #Matrice d'adjacence apprentissable\n",
    "\n",
    "# Then convert into \"Laplacian Matrix\", or with \"random_walk Matrix\", or with another one ...\n",
    "A_indep = \n",
    "A_Neighbors =\n",
    "A_learnable = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class graphconv(nn.Module):\n",
    "    def __init__(self,c_in,c_out,graph_conv_act_func,K = 2,enable_bias =True):\n",
    "        super(graphconv,self).__init__()   # Demande a ce qu'on récupère les méthodes de la classe parent :  'nn.module'\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.enable_bias = enable_bias\n",
    "        self.graph_conv_act_func = graph_conv_act_func\n",
    "        self.K = K\n",
    "        if torch.cuda.is_available():\n",
    "            self.weight = nn.Parameter(torch.cuda.FloatTensor(K,c_in,c_out))   # Initialize with wierd weight like 0e-30 or 1e38. Might not be totally adaptated ...\n",
    "        else :\n",
    "            self.weight = nn.Parameter(torch.FloatTensor(K,c_in,c_out))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x,gcnconv_matrix):\n",
    "        B, C, L, N = x.shape\n",
    "        n_mat =  gcnconv_matrix.shape[0]\n",
    "\n",
    "        x = x.reshape(-1, self.c_in)  #[B, C_in, L, N] -> [BLN, C_in]\n",
    "        x = torch.einsum('ab, cbd->cad',x,self.weight)   # [BLN,C_in], [K,C_in,C_out] -> [K,BLN,C_out]\n",
    "        x = x.view(self.K, B*L,N,-1)  #[K,BLN,C_out] ->  [K,BL,N,C_out] \n",
    "        x = torch.einsum('ecab,ecbd->ecad',gcnconv_matrix,x)  #[n_adj,BL,N1,N2] ,[K,BL,N2,C_out]  -> [K,BL,N1,C_out] \n",
    "\n",
    "        if self.enable_bias:\n",
    "            x = x + self.bias\n",
    "        \n",
    "        x = x.view().permute()\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,1,3,4)\n",
    "A_indep = torch.ones(3,3)\n",
    "\n",
    "C_in = x.shape[1]\n",
    "C_out = 8\n",
    "weight = torch.FloatTensor(C_in,C_out)\n",
    "weight = torch.Tensor([[1,2,3,4,5,6,7,8]])\n",
    "\n",
    "adj_matrix =A_indep\n",
    "# x.shape = [B,C,N2,L]\n",
    "B,C_in,N2,L = x.shape\n",
    "# A.shape = [n_adj,N1,N2]\n",
    "N1,N2 = adj_matrix.shape\n",
    "#A.shape = [n_adj,B,N1,N2]\n",
    "stacked_adj = adj_matrix.repeat(1,B,1,1)\n",
    "\n",
    "\n",
    "embedding = torch.einsum('bcnl,ch -> bhnl',x,weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'embedding a d'abord été opéré sur la dernière dimension (X*W, temporelle), Puis la convolution (A*(XW)) a sommmé les embedding de chacun des voisins (ou Noeud en lien avec le noeud tagret).\n",
      "X.shape: torch.Size([4, 3, 6]), AXW.shape:torch.Size([2, 4, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "class MGCN_conv(nn.Module):\n",
    "    def __init__(self,c_in,c_out,n_adj=2,enable_bias =True):\n",
    "        super(MGCN_conv,self).__init__()\n",
    "        self.n_adj = n_adj    \n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.weight = torch.FloatTensor(n_adj,c_in,c_out)  #[C_in,C_out]   ou pour Einsum: [l,h]\n",
    "        self.bias = torch.FloatTensor(c_out)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x,adj_matrix):\n",
    "        # x.shape = [B,N2,L]\n",
    "        B,N2,L = x.shape\n",
    "        # A.shape = [n_adj,N1,N2]\n",
    "        n_adj,N1,N2 = adj_matrix.shape\n",
    "\n",
    "        #adj_matrix = torch.unsqueeze(adj_matrix,0) #Créer un nouvel axe \n",
    "        stacked_adj = adj_matrix.repeat(B,1,1) # Concat tout les \"samples\" (B batch, L tmeporal dimension) long du nouvel axe\n",
    "        stacked_adj = stacked_adj.reshape(-1,N1,N2)  # Stack les n_adj adjacency matrix B*L fois     shape: [n_adj*B, N1,N2] et pour einsum : [b,p,n]\n",
    "\n",
    "        embedding = torch.einsum('bnl,klh -> bknh',x,self.weight)   # Embedding sur les feature de chaque Noeud\n",
    "        reshaped_embedding = embedding.reshape(-1,N2,self.c_out)    #Stack la dimension k sur l'axe 0  [n_adj*B,N2,C_out]  et pour einsum : [b,n,h]\n",
    "\n",
    "        convoluted = torch.einsum('bpn,bnh -> bph',stacked_adj,reshaped_embedding) \n",
    "        convoluted = convoluted.reshape(self.n_adj,B,N1,self.c_out)      #Unstack la dimension 0, pour séparer les n_adj matrices d'adjacences\n",
    "\n",
    "        convoluted = self.relu(convoluted + self.bias)\n",
    "\n",
    "        return(convoluted,embedding,reshaped_embedding)\n",
    "\n",
    "# Test : \n",
    "N1,N2 = 3,3\n",
    "adj_matrix = torch.stack([torch.diag(torch.ones(N1)),torch.randn(N1,N2)],dim = 0)  # Matrice d'adjacence identité, personne n'est connecté avec personne\n",
    "n_adj = adj_matrix.shape[0]\n",
    "\n",
    "B = 4\n",
    "L = 6\n",
    "x = torch.randn(B,N1,L)\n",
    "\n",
    "c_out = 16\n",
    "gcn_model = MGCN_conv(L,c_out,n_adj)\n",
    "AXW,embedding,reshaped_embedding = gcn_model(x,adj_matrix)\n",
    "\n",
    "print(\"L'embedding a d'abord été opéré sur la dernière dimension (X*W, temporelle), Puis la convolution (A*(XW)) a sommmé les embedding de chacun des voisins (ou Noeud en lien avec le noeud tagret).\")\n",
    "print(f'X.shape: {x.shape}, AXW.shape:{AXW.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 2, 3, 16]), torch.Size([8, 3, 16]))"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape,reshaped_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n1_b1 1\n",
      "n1_b2 2\n",
      "n1_b3 3\n",
      "n2_b1 4\n",
      "n2_b2 5\n",
      "n2_b3 6\n",
      "n3_b1 7\n",
      "n3_b2 8\n",
      "n3_b3 9\n",
      "n4_b1 10\n",
      "n4_b2 11\n",
      "n4_b3 12\n",
      "n5_b1 13\n",
      "n5_b2 14\n"
     ]
    }
   ],
   "source": [
    "#Bijecction entre un entier, et son label\n",
    "def int2lab(int,n_adj=2,B=3):\n",
    "    n = 1+(int-1)//B \n",
    "    b = int-(n-1)*B\n",
    "    return(f'n{n}_b{b}')\n",
    "\n",
    "def lab2int(lab,n_adj=2,B=3):\n",
    "    nb = lab.split('_')\n",
    "    n,b = int(nb[0][1:]),int(nb[1][1:])\n",
    "    return((n-1)*B+b)\n",
    "\n",
    "for k in range(1,15):\n",
    "    print(int2lab(k),lab2int(int2lab(k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentation des numéro des cellules du Tensor\n",
    "Permet de faire des affichages graphique, et de s'assurer que les \".reshape\" font ce que l'on souhaite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj1_b1_c1_n1_l1 1\n",
      "adj1_b1_c1_n1_l2 2\n",
      "adj1_b1_c1_n1_l3 3\n",
      "adj1_b1_c1_n1_l4 4\n",
      "adj1_b1_c1_n1_l5 5\n",
      "adj1_b1_c1_n1_l6 6\n",
      "adj1_b1_c1_n2_l1 7\n",
      "adj1_b1_c1_n2_l2 8\n",
      "adj1_b1_c1_n2_l3 9\n",
      "adj1_b1_c1_n2_l4 10\n",
      "adj1_b1_c1_n2_l5 11\n",
      "adj1_b1_c1_n2_l6 12\n",
      "adj1_b1_c1_n3_l1 13\n",
      "adj1_b1_c1_n3_l2 14\n",
      "adj1_b1_c1_n3_l3 15\n",
      "adj1_b1_c1_n3_l4 16\n",
      "adj1_b1_c1_n3_l5 17\n",
      "adj1_b1_c1_n3_l6 18\n",
      "adj1_b1_c1_n4_l1 19\n",
      "adj1_b1_c1_n4_l2 20\n",
      "adj1_b1_c1_n4_l3 21\n",
      "adj1_b1_c1_n4_l4 22\n",
      "adj1_b1_c1_n4_l5 23\n",
      "adj1_b1_c1_n4_l6 24\n",
      "adj1_b1_c1_n5_l1 25\n",
      "adj1_b1_c1_n5_l2 26\n",
      "adj1_b1_c1_n5_l3 27\n",
      "adj1_b1_c1_n5_l4 28\n",
      "adj1_b1_c1_n5_l5 29\n",
      "adj1_b1_c1_n5_l6 30\n",
      "adj1_b1_c2_n1_l1 31\n",
      "adj1_b1_c2_n1_l2 32\n",
      "adj1_b1_c2_n1_l3 33\n",
      "adj1_b1_c2_n1_l4 34\n",
      "adj1_b1_c2_n1_l5 35\n",
      "adj1_b1_c2_n1_l6 36\n",
      "adj1_b1_c2_n2_l1 37\n",
      "adj1_b1_c2_n2_l2 38\n",
      "adj1_b1_c2_n2_l3 39\n",
      "adj1_b1_c2_n2_l4 40\n",
      "adj1_b1_c2_n2_l5 41\n",
      "adj1_b1_c2_n2_l6 42\n",
      "adj1_b1_c2_n3_l1 43\n",
      "adj1_b1_c2_n3_l2 44\n",
      "adj1_b1_c2_n3_l3 45\n",
      "adj1_b1_c2_n3_l4 46\n",
      "adj1_b1_c2_n3_l5 47\n",
      "adj1_b1_c2_n3_l6 48\n",
      "adj1_b1_c2_n4_l1 49\n",
      "adj1_b1_c2_n4_l2 50\n",
      "adj1_b1_c2_n4_l3 51\n",
      "adj1_b1_c2_n4_l4 52\n",
      "adj1_b1_c2_n4_l5 53\n",
      "adj1_b1_c2_n4_l6 54\n",
      "adj1_b1_c2_n5_l1 55\n",
      "adj1_b1_c2_n5_l2 56\n",
      "adj1_b1_c2_n5_l3 57\n",
      "adj1_b1_c2_n5_l4 58\n",
      "adj1_b1_c2_n5_l5 59\n",
      "adj1_b1_c2_n5_l6 60\n",
      "adj1_b1_c3_n1_l1 61\n",
      "adj1_b1_c3_n1_l2 62\n",
      "adj1_b1_c3_n1_l3 63\n",
      "adj1_b1_c3_n1_l4 64\n",
      "adj1_b1_c3_n1_l5 65\n",
      "adj1_b1_c3_n1_l6 66\n",
      "adj1_b1_c3_n2_l1 67\n",
      "adj1_b1_c3_n2_l2 68\n",
      "adj1_b1_c3_n2_l3 69\n",
      "adj1_b1_c3_n2_l4 70\n",
      "adj1_b1_c3_n2_l5 71\n",
      "adj1_b1_c3_n2_l6 72\n",
      "adj1_b1_c3_n3_l1 73\n",
      "adj1_b1_c3_n3_l2 74\n",
      "adj1_b1_c3_n3_l3 75\n",
      "adj1_b1_c3_n3_l4 76\n",
      "adj1_b1_c3_n3_l5 77\n",
      "adj1_b1_c3_n3_l6 78\n",
      "adj1_b1_c3_n4_l1 79\n",
      "adj1_b1_c3_n4_l2 80\n",
      "adj1_b1_c3_n4_l3 81\n",
      "adj1_b1_c3_n4_l4 82\n",
      "adj1_b1_c3_n4_l5 83\n",
      "adj1_b1_c3_n4_l6 84\n",
      "adj1_b1_c3_n5_l1 85\n",
      "adj1_b1_c3_n5_l2 86\n",
      "adj1_b1_c3_n5_l3 87\n",
      "adj1_b1_c3_n5_l4 88\n",
      "adj1_b1_c3_n5_l5 89\n",
      "adj1_b1_c3_n5_l6 90\n",
      "adj1_b1_c4_n1_l1 91\n",
      "adj1_b1_c4_n1_l2 92\n",
      "adj1_b1_c4_n1_l3 93\n",
      "adj1_b1_c4_n1_l4 94\n",
      "adj1_b1_c4_n1_l5 95\n",
      "adj1_b1_c4_n1_l6 96\n",
      "adj1_b1_c4_n2_l1 97\n",
      "adj1_b1_c4_n2_l2 98\n",
      "adj1_b1_c4_n2_l3 99\n",
      "adj1_b1_c4_n2_l4 100\n",
      "adj1_b1_c4_n2_l5 101\n",
      "adj1_b1_c4_n2_l6 102\n",
      "adj1_b1_c4_n3_l1 103\n",
      "adj1_b1_c4_n3_l2 104\n",
      "adj1_b1_c4_n3_l3 105\n",
      "adj1_b1_c4_n3_l4 106\n",
      "adj1_b1_c4_n3_l5 107\n",
      "adj1_b1_c4_n3_l6 108\n",
      "adj1_b1_c4_n4_l1 109\n",
      "adj1_b1_c4_n4_l2 110\n",
      "adj1_b1_c4_n4_l3 111\n",
      "adj1_b1_c4_n4_l4 112\n",
      "adj1_b1_c4_n4_l5 113\n",
      "adj1_b1_c4_n4_l6 114\n",
      "adj1_b1_c4_n5_l1 115\n",
      "adj1_b1_c4_n5_l2 116\n",
      "adj1_b1_c4_n5_l3 117\n",
      "adj1_b1_c4_n5_l4 118\n",
      "adj1_b1_c4_n5_l5 119\n",
      "adj1_b1_c4_n5_l6 120\n",
      "adj1_b2_c1_n1_l1 121\n",
      "adj1_b2_c1_n1_l2 122\n",
      "adj1_b2_c1_n1_l3 123\n",
      "adj1_b2_c1_n1_l4 124\n",
      "adj1_b2_c1_n1_l5 125\n",
      "adj1_b2_c1_n1_l6 126\n",
      "adj1_b2_c1_n2_l1 127\n",
      "adj1_b2_c1_n2_l2 128\n",
      "adj1_b2_c1_n2_l3 129\n",
      "adj1_b2_c1_n2_l4 130\n",
      "adj1_b2_c1_n2_l5 131\n",
      "adj1_b2_c1_n2_l6 132\n",
      "adj1_b2_c1_n3_l1 133\n",
      "adj1_b2_c1_n3_l2 134\n",
      "adj1_b2_c1_n3_l3 135\n",
      "adj1_b2_c1_n3_l4 136\n",
      "adj1_b2_c1_n3_l5 137\n",
      "adj1_b2_c1_n3_l6 138\n",
      "adj1_b2_c1_n4_l1 139\n",
      "adj1_b2_c1_n4_l2 140\n",
      "adj1_b2_c1_n4_l3 141\n",
      "adj1_b2_c1_n4_l4 142\n",
      "adj1_b2_c1_n4_l5 143\n",
      "adj1_b2_c1_n4_l6 144\n",
      "adj1_b2_c1_n5_l1 145\n",
      "adj1_b2_c1_n5_l2 146\n",
      "adj1_b2_c1_n5_l3 147\n",
      "adj1_b2_c1_n5_l4 148\n",
      "adj1_b2_c1_n5_l5 149\n",
      "adj1_b2_c1_n5_l6 150\n",
      "adj1_b2_c2_n1_l1 151\n",
      "adj1_b2_c2_n1_l2 152\n",
      "adj1_b2_c2_n1_l3 153\n",
      "adj1_b2_c2_n1_l4 154\n",
      "adj1_b2_c2_n1_l5 155\n",
      "adj1_b2_c2_n1_l6 156\n",
      "adj1_b2_c2_n2_l1 157\n",
      "adj1_b2_c2_n2_l2 158\n",
      "adj1_b2_c2_n2_l3 159\n",
      "adj1_b2_c2_n2_l4 160\n",
      "adj1_b2_c2_n2_l5 161\n",
      "adj1_b2_c2_n2_l6 162\n",
      "adj1_b2_c2_n3_l1 163\n",
      "adj1_b2_c2_n3_l2 164\n",
      "adj1_b2_c2_n3_l3 165\n",
      "adj1_b2_c2_n3_l4 166\n",
      "adj1_b2_c2_n3_l5 167\n",
      "adj1_b2_c2_n3_l6 168\n",
      "adj1_b2_c2_n4_l1 169\n",
      "adj1_b2_c2_n4_l2 170\n",
      "adj1_b2_c2_n4_l3 171\n",
      "adj1_b2_c2_n4_l4 172\n",
      "adj1_b2_c2_n4_l5 173\n",
      "adj1_b2_c2_n4_l6 174\n",
      "adj1_b2_c2_n5_l1 175\n",
      "adj1_b2_c2_n5_l2 176\n",
      "adj1_b2_c2_n5_l3 177\n",
      "adj1_b2_c2_n5_l4 178\n",
      "adj1_b2_c2_n5_l5 179\n",
      "adj1_b2_c2_n5_l6 180\n",
      "adj1_b2_c3_n1_l1 181\n",
      "adj1_b2_c3_n1_l2 182\n",
      "adj1_b2_c3_n1_l3 183\n",
      "adj1_b2_c3_n1_l4 184\n",
      "adj1_b2_c3_n1_l5 185\n",
      "adj1_b2_c3_n1_l6 186\n",
      "adj1_b2_c3_n2_l1 187\n",
      "adj1_b2_c3_n2_l2 188\n",
      "adj1_b2_c3_n2_l3 189\n",
      "adj1_b2_c3_n2_l4 190\n",
      "adj1_b2_c3_n2_l5 191\n",
      "adj1_b2_c3_n2_l6 192\n",
      "adj1_b2_c3_n3_l1 193\n",
      "adj1_b2_c3_n3_l2 194\n",
      "adj1_b2_c3_n3_l3 195\n",
      "adj1_b2_c3_n3_l4 196\n",
      "adj1_b2_c3_n3_l5 197\n",
      "adj1_b2_c3_n3_l6 198\n",
      "adj1_b2_c3_n4_l1 199\n",
      "adj1_b2_c3_n4_l2 200\n",
      "adj1_b2_c3_n4_l3 201\n",
      "adj1_b2_c3_n4_l4 202\n",
      "adj1_b2_c3_n4_l5 203\n",
      "adj1_b2_c3_n4_l6 204\n",
      "adj1_b2_c3_n5_l1 205\n",
      "adj1_b2_c3_n5_l2 206\n",
      "adj1_b2_c3_n5_l3 207\n",
      "adj1_b2_c3_n5_l4 208\n",
      "adj1_b2_c3_n5_l5 209\n",
      "adj1_b2_c3_n5_l6 210\n",
      "adj1_b2_c4_n1_l1 211\n",
      "adj1_b2_c4_n1_l2 212\n",
      "adj1_b2_c4_n1_l3 213\n",
      "adj1_b2_c4_n1_l4 214\n",
      "adj1_b2_c4_n1_l5 215\n",
      "adj1_b2_c4_n1_l6 216\n",
      "adj1_b2_c4_n2_l1 217\n",
      "adj1_b2_c4_n2_l2 218\n",
      "adj1_b2_c4_n2_l3 219\n",
      "adj1_b2_c4_n2_l4 220\n",
      "adj1_b2_c4_n2_l5 221\n",
      "adj1_b2_c4_n2_l6 222\n",
      "adj1_b2_c4_n3_l1 223\n",
      "adj1_b2_c4_n3_l2 224\n",
      "adj1_b2_c4_n3_l3 225\n",
      "adj1_b2_c4_n3_l4 226\n",
      "adj1_b2_c4_n3_l5 227\n",
      "adj1_b2_c4_n3_l6 228\n",
      "adj1_b2_c4_n4_l1 229\n",
      "adj1_b2_c4_n4_l2 230\n",
      "adj1_b2_c4_n4_l3 231\n",
      "adj1_b2_c4_n4_l4 232\n",
      "adj1_b2_c4_n4_l5 233\n",
      "adj1_b2_c4_n4_l6 234\n",
      "adj1_b2_c4_n5_l1 235\n",
      "adj1_b2_c4_n5_l2 236\n",
      "adj1_b2_c4_n5_l3 237\n",
      "adj1_b2_c4_n5_l4 238\n",
      "adj1_b2_c4_n5_l5 239\n",
      "adj1_b2_c4_n5_l6 240\n",
      "adj1_b3_c1_n1_l1 241\n",
      "adj1_b3_c1_n1_l2 242\n",
      "adj1_b3_c1_n1_l3 243\n",
      "adj1_b3_c1_n1_l4 244\n",
      "adj1_b3_c1_n1_l5 245\n",
      "adj1_b3_c1_n1_l6 246\n",
      "adj1_b3_c1_n2_l1 247\n",
      "adj1_b3_c1_n2_l2 248\n",
      "adj1_b3_c1_n2_l3 249\n",
      "adj1_b3_c1_n2_l4 250\n",
      "adj1_b3_c1_n2_l5 251\n",
      "adj1_b3_c1_n2_l6 252\n",
      "adj1_b3_c1_n3_l1 253\n",
      "adj1_b3_c1_n3_l2 254\n",
      "adj1_b3_c1_n3_l3 255\n",
      "adj1_b3_c1_n3_l4 256\n",
      "adj1_b3_c1_n3_l5 257\n",
      "adj1_b3_c1_n3_l6 258\n",
      "adj1_b3_c1_n4_l1 259\n",
      "adj1_b3_c1_n4_l2 260\n",
      "adj1_b3_c1_n4_l3 261\n",
      "adj1_b3_c1_n4_l4 262\n",
      "adj1_b3_c1_n4_l5 263\n",
      "adj1_b3_c1_n4_l6 264\n",
      "adj1_b3_c1_n5_l1 265\n",
      "adj1_b3_c1_n5_l2 266\n",
      "adj1_b3_c1_n5_l3 267\n",
      "adj1_b3_c1_n5_l4 268\n",
      "adj1_b3_c1_n5_l5 269\n",
      "adj1_b3_c1_n5_l6 270\n",
      "adj1_b3_c2_n1_l1 271\n",
      "adj1_b3_c2_n1_l2 272\n",
      "adj1_b3_c2_n1_l3 273\n",
      "adj1_b3_c2_n1_l4 274\n",
      "adj1_b3_c2_n1_l5 275\n",
      "adj1_b3_c2_n1_l6 276\n",
      "adj1_b3_c2_n2_l1 277\n",
      "adj1_b3_c2_n2_l2 278\n",
      "adj1_b3_c2_n2_l3 279\n",
      "adj1_b3_c2_n2_l4 280\n",
      "adj1_b3_c2_n2_l5 281\n",
      "adj1_b3_c2_n2_l6 282\n",
      "adj1_b3_c2_n3_l1 283\n",
      "adj1_b3_c2_n3_l2 284\n",
      "adj1_b3_c2_n3_l3 285\n",
      "adj1_b3_c2_n3_l4 286\n",
      "adj1_b3_c2_n3_l5 287\n",
      "adj1_b3_c2_n3_l6 288\n",
      "adj1_b3_c2_n4_l1 289\n",
      "adj1_b3_c2_n4_l2 290\n",
      "adj1_b3_c2_n4_l3 291\n",
      "adj1_b3_c2_n4_l4 292\n",
      "adj1_b3_c2_n4_l5 293\n",
      "adj1_b3_c2_n4_l6 294\n",
      "adj1_b3_c2_n5_l1 295\n",
      "adj1_b3_c2_n5_l2 296\n",
      "adj1_b3_c2_n5_l3 297\n",
      "adj1_b3_c2_n5_l4 298\n",
      "adj1_b3_c2_n5_l5 299\n",
      "adj1_b3_c2_n5_l6 300\n",
      "adj1_b3_c3_n1_l1 301\n",
      "adj1_b3_c3_n1_l2 302\n",
      "adj1_b3_c3_n1_l3 303\n",
      "adj1_b3_c3_n1_l4 304\n",
      "adj1_b3_c3_n1_l5 305\n",
      "adj1_b3_c3_n1_l6 306\n",
      "adj1_b3_c3_n2_l1 307\n",
      "adj1_b3_c3_n2_l2 308\n",
      "adj1_b3_c3_n2_l3 309\n",
      "adj1_b3_c3_n2_l4 310\n",
      "adj1_b3_c3_n2_l5 311\n",
      "adj1_b3_c3_n2_l6 312\n",
      "adj1_b3_c3_n3_l1 313\n",
      "adj1_b3_c3_n3_l2 314\n",
      "adj1_b3_c3_n3_l3 315\n",
      "adj1_b3_c3_n3_l4 316\n",
      "adj1_b3_c3_n3_l5 317\n",
      "adj1_b3_c3_n3_l6 318\n",
      "adj1_b3_c3_n4_l1 319\n",
      "adj1_b3_c3_n4_l2 320\n",
      "adj1_b3_c3_n4_l3 321\n",
      "adj1_b3_c3_n4_l4 322\n",
      "adj1_b3_c3_n4_l5 323\n",
      "adj1_b3_c3_n4_l6 324\n",
      "adj1_b3_c3_n5_l1 325\n",
      "adj1_b3_c3_n5_l2 326\n",
      "adj1_b3_c3_n5_l3 327\n",
      "adj1_b3_c3_n5_l4 328\n",
      "adj1_b3_c3_n5_l5 329\n",
      "adj1_b3_c3_n5_l6 330\n",
      "adj1_b3_c4_n1_l1 331\n",
      "adj1_b3_c4_n1_l2 332\n",
      "adj1_b3_c4_n1_l3 333\n",
      "adj1_b3_c4_n1_l4 334\n",
      "adj1_b3_c4_n1_l5 335\n",
      "adj1_b3_c4_n1_l6 336\n",
      "adj1_b3_c4_n2_l1 337\n",
      "adj1_b3_c4_n2_l2 338\n",
      "adj1_b3_c4_n2_l3 339\n",
      "adj1_b3_c4_n2_l4 340\n",
      "adj1_b3_c4_n2_l5 341\n",
      "adj1_b3_c4_n2_l6 342\n",
      "adj1_b3_c4_n3_l1 343\n",
      "adj1_b3_c4_n3_l2 344\n",
      "adj1_b3_c4_n3_l3 345\n",
      "adj1_b3_c4_n3_l4 346\n",
      "adj1_b3_c4_n3_l5 347\n",
      "adj1_b3_c4_n3_l6 348\n",
      "adj1_b3_c4_n4_l1 349\n",
      "adj1_b3_c4_n4_l2 350\n",
      "adj1_b3_c4_n4_l3 351\n",
      "adj1_b3_c4_n4_l4 352\n",
      "adj1_b3_c4_n4_l5 353\n",
      "adj1_b3_c4_n4_l6 354\n",
      "adj1_b3_c4_n5_l1 355\n",
      "adj1_b3_c4_n5_l2 356\n",
      "adj1_b3_c4_n5_l3 357\n",
      "adj1_b3_c4_n5_l4 358\n",
      "adj1_b3_c4_n5_l5 359\n",
      "adj1_b3_c4_n5_l6 360\n",
      "adj2_b1_c1_n1_l1 361\n",
      "adj2_b1_c1_n1_l2 362\n",
      "adj2_b1_c1_n1_l3 363\n",
      "adj2_b1_c1_n1_l4 364\n",
      "adj2_b1_c1_n1_l5 365\n",
      "adj2_b1_c1_n1_l6 366\n",
      "adj2_b1_c1_n2_l1 367\n",
      "adj2_b1_c1_n2_l2 368\n",
      "adj2_b1_c1_n2_l3 369\n",
      "adj2_b1_c1_n2_l4 370\n",
      "adj2_b1_c1_n2_l5 371\n",
      "adj2_b1_c1_n2_l6 372\n",
      "adj2_b1_c1_n3_l1 373\n",
      "adj2_b1_c1_n3_l2 374\n",
      "adj2_b1_c1_n3_l3 375\n",
      "adj2_b1_c1_n3_l4 376\n",
      "adj2_b1_c1_n3_l5 377\n",
      "adj2_b1_c1_n3_l6 378\n",
      "adj2_b1_c1_n4_l1 379\n",
      "adj2_b1_c1_n4_l2 380\n",
      "adj2_b1_c1_n4_l3 381\n",
      "adj2_b1_c1_n4_l4 382\n",
      "adj2_b1_c1_n4_l5 383\n",
      "adj2_b1_c1_n4_l6 384\n",
      "adj2_b1_c1_n5_l1 385\n",
      "adj2_b1_c1_n5_l2 386\n",
      "adj2_b1_c1_n5_l3 387\n",
      "adj2_b1_c1_n5_l4 388\n",
      "adj2_b1_c1_n5_l5 389\n",
      "adj2_b1_c1_n5_l6 390\n",
      "adj2_b1_c2_n1_l1 391\n",
      "adj2_b1_c2_n1_l2 392\n",
      "adj2_b1_c2_n1_l3 393\n",
      "adj2_b1_c2_n1_l4 394\n",
      "adj2_b1_c2_n1_l5 395\n",
      "adj2_b1_c2_n1_l6 396\n",
      "adj2_b1_c2_n2_l1 397\n",
      "adj2_b1_c2_n2_l2 398\n",
      "adj2_b1_c2_n2_l3 399\n"
     ]
    }
   ],
   "source": [
    "def int2lab(integer, n_adj=2, B=3, C=4, N=5, L=6):\n",
    "    N_adj_L = N * L\n",
    "    N_adj_C_L = C * N_adj_L\n",
    "    N_adj_B_C_L = B * N_adj_C_L\n",
    "\n",
    "    l = 1 + ((integer - 1) % L)\n",
    "    remaining = (integer - 1) // L\n",
    "    n = 1 + (remaining % N)\n",
    "    remaining //= N\n",
    "    c = 1 + (remaining % C)\n",
    "    remaining //= C\n",
    "    b = 1 + (remaining % B)\n",
    "    remaining //= B\n",
    "    n_adj = 1 + remaining % n_adj\n",
    "\n",
    "    return f'adj{n_adj}_b{b}_c{c}_n{n}_l{l}'\n",
    "\n",
    "def lab2int(label, n_adj=2, B=3, C=4, N=5, L=6):\n",
    "    split_label = label.split('_')\n",
    "    n_adj = int(split_label[0][3:])\n",
    "    b = int(split_label[1][1:])\n",
    "    c = int(split_label[2][1:])\n",
    "    n = int(split_label[3][1:])\n",
    "    l = int(split_label[4][1:])\n",
    "\n",
    "    N_adj_L = N * L\n",
    "    N_adj_C_L = C * N_adj_L\n",
    "    N_adj_B_C_L = B * N_adj_C_L\n",
    "\n",
    "    integer = ((l - 1) + (n - 1) * L + (c - 1) * N * L + (b - 1) * C * N * L + (n_adj - 1) * B * C * N * L) + 1\n",
    "    return integer\n",
    "\n",
    "\n",
    "for k in range(1,400):\n",
    "    print(int2lab(k),lab2int(int2lab(k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n1', 'b1', 'c1', 'adj1', 'l1']"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = int2lab(k)\n",
    "label.split('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_conv(nn.Module):\n",
    "    def __init__(self,c_in,c_out,graph_conv_act_func,K=1,enable_bias =True):\n",
    "        super(GCN_conv,self).__init__()\n",
    "\n",
    "    \n",
    "    def forward(self,x,adj_matrix):\n",
    "        # x.shape = [B,C,N2,L]\n",
    "        B,C,N2,L = x.shape\n",
    "        # A.shape = [n_adj,N1,N2]\n",
    "        n_adj,N1,N2 = adj_matrix.shape\n",
    "        #A.shape = [n_adj,B,N1,N2]\n",
    "        adj_matrix = torch.unsqueeze(adj_matrix,0) #Créer un nouvel axe \n",
    "        stacked_adj = adj_matrix.repeat(B*L,1,1,1) # Concat tout les \"samples\" (B batch, L tmeporal dimension) long du nouvel axe\n",
    "        stacked_adj = stacked_adj.reshape(-1,N1,N2)  # Stack les n_adj adjacency matrix B*L fois\n",
    "\n",
    "        embedding = torch.einsum('bcnl,ch -> bhnl',x,self.weight)   # Embedding Spatial le long de C_in\n",
    "        embedding = embedding.permute(0,3,2,1)  # [B,L,N2,C_out]\n",
    "        embedding = embedding.reshape(-1,N2,C_out)  # [BL,N2,C_out]\n",
    "\n",
    "        convolution = torch.einsum('kbpn,bhnl -> kbpl',stacked_adj,embedding)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 3]), torch.Size([2, 3, 3]))"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_adj_matrix = adj_matrix.repeat(2,1,1)\n",
    "n_adj,N1,N2 = n_adj_matrix.shape\n",
    "#A.shape = [n_adj,B,N1,N2]\n",
    "stacked_adj = n_adj_matrix.repeat(B*L,1,1) # [n_adj*B, N1,N2]  -> Concat tout les \"samples\" long de l'axe 0\n",
    "stacked_adj.shape,n_adj_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([10560, 1, 14, 9])\n",
      "x_b shape: torch.Size([32, 1, 14, 9])\n",
      "B,C,N,L : [32,1,14,9]\n"
     ]
    }
   ],
   "source": [
    "x = X.unsqueeze(1) #add the channel dimension (here, only \"flow')\n",
    "print(f'X shape: {x.shape}')\n",
    "x_b = x[:32]\n",
    "print(f'x_b shape: {x_b.shape}')\n",
    "\n",
    "GCN = graphconv(c_in = x.shape[1], c_out = 64, K=2, graph_conv_act_func = 'relu',enable_bias=True)  # K =2 dans MRGNN car considère Pattern et Adj matrix\n",
    "B, C, N, L = x_b.shape\n",
    "K = 1\n",
    "c_out = 64\n",
    "n_mat =  A_indep.shape[0]\n",
    "\n",
    "print(f'B,C,N,L : [{B},{C},{N},{L}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN détaillé "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_b: torch.Size([4032, 1]), W: torch.Size([1, 1, 64]), Cause there are 1 adjacency matrices (K), 1 C_in and 64 C_out\n",
      "reshaped x_b: torch.Size([4032, 1]), Cause we have to flatten x_b along the Channel axis, and then pass through a SPATIAL embedding (Linear layer, on C_in of each sample,nodes,historical element)\n",
      "shape of reshaped_xb after K embedding on C_in:  torch.Size([1, 4032, 64])\n",
      "Embedded feature vect reshaped:  torch.Size([1, 288, 14, 64]) \n",
      "\n",
      "Adjacency matrix A_indep:  torch.Size([14, 14]) batched Multi adj_matrix:  torch.Size([1, 288, 14, 14])\n",
      "Convolution A*(XW):  torch.Size([1, 288, 14, 64])\n",
      "Reshaped convolution output: torch.Size([1, 32, 64, 14, 9])\n"
     ]
    }
   ],
   "source": [
    "weight = nn.Parameter(torch.FloatTensor(K,C,c_out))\n",
    "bias = nn.Parameter(torch.FloatTensor(c_out))\n",
    "print(f'x_b: {x_b.shape}, W: {weight.shape}, Cause there are {K} adjacency matrices (K), {C} C_in and {c_out} C_out')\n",
    "x_b = x_b.reshape(-1, C)  #[B, C_in, L, N] -> [BLN, C_in]\n",
    "print(f'reshaped x_b: {x_b.shape}, Cause we have to flatten x_b along the Channel axis, and then pass through a SPATIAL embedding (Linear layer, on C_in of each sample,nodes,historical element)')\n",
    "\n",
    "# Embedding on C_in:  X*W\n",
    "embedd_c_in = torch.einsum('ab, cbd->cad',x_b,weight)   # [BLN,C_in], [K,C_in,C_out] -> [K,BLN,C_out]n  Propose K embedding de C_in\n",
    "print('shape of reshaped_xb after K embedding on C_in: ',embedd_c_in.shape)\n",
    "reshaped_embedd_c_in = embedd_c_in.view(K, B*L,N,-1)  #[K,BLN,C_out] ->  [K,BL,N,C_out] \n",
    "print('Embedded feature vect reshaped: ',reshaped_embedd_c_in.shape, '\\n')\n",
    "\n",
    "# Concat Adj Matrix \n",
    "batched_adj_matrix = A_indep.repeat(1,B*L,1,1)\n",
    "print('Adjacency matrix A_indep: ',A_indep.shape,'batched Multi adj_matrix: ',batched_adj_matrix.shape)\n",
    "\n",
    "# Convolution A*(XW)\n",
    "convolutionned = torch.einsum('ecab,ecbd->ecad',batched_adj_matrix,reshaped_embedd_c_in)  #[K,BL,N1,N2] ,[K,BL,N2,C_out]  -> [K,BL,N1,C_out] \n",
    "print('Convolution A*(XW): ',convolutionned.shape)\n",
    "\n",
    "#Add bias: \n",
    "convolutionned_n_biased = convolutionned + bias\n",
    "\n",
    "# Reshape and Permute: \n",
    "H =convolutionned_n_biased.view(K,B,L,-1,c_out).permute(0,1,4,3,2)\n",
    "print(f'Reshaped convolution output: {H.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 9, 14, 64])\n",
      "torch.Size([1, 4032, 64])\n"
     ]
    }
   ],
   "source": [
    "permuted_H = output_H.permute(0, 1, 3, 4, 2)\n",
    "print(permuted_H.shape)\n",
    "\n",
    "new_c_in = permuted_H.shape[-1]\n",
    "first_attn = permuted_H.reshape(K,-1,new_c_in)\n",
    "print(first_attn.shape)  # [K, B*L*N, C_in']\n",
    "\n",
    "attn_weight = nn.Linear(new_c_in,1)\n",
    "softmax = nn.Softmax(-1)  #SoftMax on the last dimension \n",
    "\n",
    "#Coefficient d'attention \n",
    "attn = attn_weight(first_attn)    # \"Embedding\" du spatial channel \n",
    "attn = attn.permute(1,2,0)  # Permute pour avoir la nombre d'adjacency matrix en dernière dimension\n",
    "attn = softmax(attn)  # Coefficient d'attention pour chaque Matrices d'adjacence et Embedding spatial assicié  (Ici 1 seule matrice d'adjacence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4032)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(attn == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1155,  0.2000, -0.2143, -0.2138],\n",
       "         [-0.6428, -1.0331, -0.2202, -1.8725],\n",
       "         [-0.9164,  0.0879, -1.4340,  0.1705]],\n",
       "\n",
       "        [[ 0.3603,  0.1547,  0.0509, -0.0657],\n",
       "         [ 0.7353,  1.4075, -1.5137,  1.6018],\n",
       "         [ 0.7461, -0.3099, -0.6415, -0.4405]]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2,3,4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if self.enable_bias:\n",
    "    x = x + self.bias\n",
    "\n",
    "x = x.view().permute()\n",
    "x = self.relu(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Preprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
