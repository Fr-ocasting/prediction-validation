{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utilities_DL import get_DataSet_and_invalid_dates,get_MultiModel_loss_args_emb_opts\n",
    "from DL_class import MultiModelTrainer, Trainer\n",
    "from config import get_args\n",
    "from plotting import plot_k_fold_split\n",
    "from save_results import build_results_df\n",
    "from paths import folder_path,file_name,get_save_directory\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'STGCN' #'CNN' \n",
    "args = get_args(model_name)\n",
    "\n",
    "# Necessary args :\n",
    "args.calendar_class = 3\n",
    "args.embedding_dim = 3\n",
    "args.type_calendar = 'tuple'\n",
    "args.time_embedding = True\n",
    "\n",
    "# Other args: \n",
    "args.H = 0\n",
    "args.W = 0\n",
    "args.D = 0\n",
    "args.L =args.H+args.W+args.D\n",
    "args.single_station = True\n",
    "\n",
    "args.specific_lr = False\n",
    "args.K_fold = 1\n",
    "\n",
    "# Save Directory:\n",
    "main_dir = get_save_directory(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset,invalid_dates = get_DataSet_and_invalid_dates(folder_path,file_name,args.W,args.D,args.H,args.step_ahead,single_station = args.single_station)\n",
    "(Datasets,DataLoader_list,time_slots_labels,dic_class2rpz,dic_rpz2class,nb_words_embedding) =  dataset.split_K_fold(args,invalid_dates)\n",
    "\n",
    "# Load associated K_folds Models: \n",
    "(loss_function,Model_list,Optimizer_list,args_embedding) = get_MultiModel_loss_args_emb_opts(args,nb_words_embedding,dic_class2rpz)\n",
    "\n",
    "model,optimizer,dataloader,dataset = Model_list[0], Optimizer_list[0],DataLoader_list[0],Datasets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning: \n",
    "Récupération de la sauvegarde contenant tout les poids qui nous intéressent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir  = 'save/STGCN/K_fold5/H0_D0_W0/graph_conv/sym_norm_lap/act_glu_Ks2/optadamw/train_valid_calib_0.60.20.5/E300_lr0.0001_B64/FC1_17_8_FC2_8_4/Emb_dim3/Specific_lr_False/CalendarClass3/position_input/fold0/'\n",
    "saved_checkpoint = torch.load(f'{model_dir}best_model.pkl')\n",
    "embedding_weights = {k: v for k, v in saved_checkpoint['state_dict'].items() if 'Tembedding' in k}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load du model en ne récupérant que les poids du TimeEmbedding Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(embedding_weights, strict=False)\n",
    "trainer = Trainer(dataset,model,dataloader,args,optimizer,loss_function,scheduler = None, ray = False,args_embedding  =args_embedding,dic_class2rpz = dic_class2rpz, save_dir = None, fold = 0)\n",
    "trainer.plot_bokeh_and_save_results(pd.DataFrame(),epoch=0,station=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load du module en récupérant l'ensemble des poids suavegardé:\n",
    "Ne fonctionne que si le model est exactement identique (car les noms de chaque couche et leur structure doivent correspondre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(saved_checkpoint['state_dict'])\n",
    "trainer = Trainer(dataset,model,dataloader,args,optimizer,loss_function,scheduler = None, ray = False,args_embedding  =args_embedding,dic_class2rpz = dic_class2rpz, save_dir = None, fold = 0)\n",
    "trainer.plot_bokeh_and_save_results(pd.DataFrame(),epoch=0,station=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faisons Maintenant un Transfer Learning sur un Model totalement différent (mais avec les poids du TimeEmbedding adapté: )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'STGCN' #'CNN' \n",
    "args = get_args(model_name)\n",
    "\n",
    "# Necessary args :\n",
    "args.calendar_class = 3\n",
    "args.embedding_dim = 3\n",
    "args.type_calendar = 'tuple'\n",
    "args.time_embedding = True\n",
    "\n",
    "# Other args: \n",
    "args.epochs = 100\n",
    "args.H = 6\n",
    "args.W = 1\n",
    "args.D = 1\n",
    "args.L =args.H+args.W+args.D\n",
    "args.single_station = True\n",
    "\n",
    "args.specific_lr = False\n",
    "args.K_fold = 1\n",
    "\n",
    "# Save Directory:\n",
    "main_dir = get_save_directory(args)\n",
    "\n",
    "# Load Model \n",
    "dataset,invalid_dates = get_DataSet_and_invalid_dates(folder_path,file_name,args.W,args.D,args.H,args.step_ahead,single_station = args.single_station)\n",
    "(Datasets,DataLoader_list,time_slots_labels,dic_class2rpz,dic_rpz2class,nb_words_embedding) =  dataset.split_K_fold(args,invalid_dates)\n",
    "(loss_function,Model_list,Optimizer_list,args_embedding) = get_MultiModel_loss_args_emb_opts(args,nb_words_embedding,dic_class2rpz)\n",
    "model,optimizer,dataloader,dataset = Model_list[0], Optimizer_list[0],DataLoader_list[0],Datasets[0]\n",
    "\n",
    "# TE Transfer Learning\n",
    "model.load_state_dict(embedding_weights, strict=False)\n",
    "trainer = Trainer(dataset,model,dataloader,args,optimizer,loss_function,scheduler = None, ray = False,args_embedding  =args_embedding,dic_class2rpz = dic_class2rpz, save_dir = None, fold = 0)\n",
    "\n",
    "# Training : \n",
    "trainer.train_and_valid(mod = 10000,mod_plot = 10, alpha = args.alpha,station = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparons maintenant avec un même model, mais sans le TransferLearning\n",
    "C'est un cas où, nous l'avons vu, le TimeEmbedding ne sert à rien. Il n'est pas entrainé correctement.\n",
    "- Après 1 epoch, l'avantage du Transfer Learning semble limité (Valid Loss 0.164 contre 0.170)\n",
    "- Après 10 epochs, Idem avantage très limité (Valid Loss 0.107 contre 0.109)\n",
    "- Après Seulement 20 epochs, le Transfer Learning n'a même plus d'intérêt (Valid Loss 0.099 contre 0.096). Mais on Voit aussi que l'espace d'embedding est déjà intéressant. \n",
    "- Après 100 epochs, le Transfer Learning n'a pas plus d'intérêt (Valid Loss 0.099 contre 0.084)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'save/STGCN/K_fold1/H6_D1_W1/graph_conv/sym_norm_lap/act_glu_Ks2/optadamw/train_valid_calib_0.60.20.5/E100_lr0.0001_B64/FC1_17_8_FC2_8_4/Emb_dim3/Specific_lr_False/CalendarClass3/position_input/'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model \n",
    "dataset,invalid_dates = get_DataSet_and_invalid_dates(folder_path,file_name,args.W,args.D,args.H,args.step_ahead,single_station = args.single_station)\n",
    "(Datasets,DataLoader_list,time_slots_labels,dic_class2rpz,dic_rpz2class,nb_words_embedding) =  dataset.split_K_fold(args,invalid_dates)\n",
    "(loss_function,Model_list,Optimizer_list,args_embedding) = get_MultiModel_loss_args_emb_opts(args,nb_words_embedding,dic_class2rpz)\n",
    "model,optimizer,dataloader,dataset = Model_list[0], Optimizer_list[0],DataLoader_list[0],Datasets[0]\n",
    "\n",
    "# TE Transfer Learning\n",
    "# model.load_state_dict(embedding_weights, strict=False)\n",
    "trainer = Trainer(dataset,model,dataloader,args,optimizer,loss_function,scheduler = None, ray = False,args_embedding  =args_embedding,dic_class2rpz = dic_class2rpz, save_dir = None, fold = 0)\n",
    "\n",
    "# Training : \n",
    "trainer.train_and_valid(mod = 10000,mod_plot = 10, alpha = args.alpha,station = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maintenant, essayons de Load un Multi-Station Model, mais en y mettant un time-embedding commun à toute stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.single_station = False\n",
    "# Load Model \n",
    "dataset,invalid_dates = get_DataSet_and_invalid_dates(folder_path,file_name,args.W,args.D,args.H,args.step_ahead,single_station = args.single_station)\n",
    "(Datasets,DataLoader_list,time_slots_labels,dic_class2rpz,dic_rpz2class,nb_words_embedding) =  dataset.split_K_fold(args,invalid_dates)\n",
    "(loss_function,Model_list,Optimizer_list,args_embedding) = get_MultiModel_loss_args_emb_opts(args,nb_words_embedding,dic_class2rpz)\n",
    "model,optimizer,dataloader,dataset = Model_list[0], Optimizer_list[0],DataLoader_list[0],Datasets[0]\n",
    "\n",
    "# TE Transfer Learning\n",
    "model.load_state_dict(embedding_weights, strict=False)\n",
    "trainer = Trainer(dataset,model,dataloader,args,optimizer,loss_function,scheduler = None, ray = False,args_embedding  =args_embedding,dic_class2rpz = dic_class2rpz, save_dir = None, fold = 0)\n",
    "\n",
    "# Training : \n",
    "trainer.train_and_valid(mod = 10000,mod_plot = 10, alpha = args.alpha,station = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "preprocessingclone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
