{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Hyper-parameter tuning with Ray is not possible\n",
      "----------------------------------------\n",
      "Loading the Complete Dataset for K-fold splitting\n",
      "Coverage Period: 5856 elts between 2019-11-01 00:00:00 and 2019-12-31 23:45:00\n",
      "Invalid dates within this fold: 5\n",
      "\n",
      ">>>Tackle Target dataset: subway_in\n",
      "   Load data from: /home/rrochas/prediction-validation/../../../../data/rrochas/prediction_validation/subway_in/subway_in.csv\n",
      "   Init Dataset: 'torch.Size([5856, 40]). 0 Nan values\n",
      "   TRAIN contextual_ds: torch.Size([2582, 40, 7])\n",
      "   VALID contextual_ds: torch.Size([861, 40, 7])\n",
      "   TEST contextual_ds: torch.Size([860, 40, 7])\n",
      "\n",
      ">>>Tackle Contextual dataset:  tramway_indiv\n",
      "   Load data from: /home/rrochas/prediction-validation/../../../../data/rrochas/prediction_validation/agg_data/validation_individuelle/tramway_indiv_15min/tramway_indiv_15min.csv\n",
      "   Filter station where average daily passenger < 240\n",
      "   Number of initial stations: 629. Number of stations after filtering: 164\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Jusque là les données sont filtrés et on selectionne un ensemble de stations explicative.                              par contre, il y a toujours plus de stations qu'il n'y a d'unité spatiale à prédire                              si on fait le choix de prédire le nombre de passeger des stations de metro.                              En tout cas il y a une incompatibilité entre la target station qui a N unité spatiale, et                              la donnée contextuelle qui a M unité spatiale.                              On doit intégrer une fcontion d'aggregation spatiale et des données statique géométrique.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 108\u001b[0m\n\u001b[1;32m    105\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain_and_valid(normalizer \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mnormalizer, mod \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m,mod_plot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m     (trainer,ds,ds_no_shuffle,args) \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_init\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mstation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstation\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mtraining_mode_to_visualise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_mode_to_visualise\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/prediction-validation/examples/train_and_visu_non_recurrent.py:54\u001b[0m, in \u001b[0;36mevaluate_config\u001b[0;34m(args_init, modification, fold_to_evaluate, training_mode_to_visualise, station, transfer_modes, type_POIs, spatial_units, apps, POI_or_stations, expanded, individual_poi, sum_ts_pois)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_config\u001b[39m(args_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m                     modification \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m     32\u001b[0m                     fold_to_evaluate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m                     sum_ts_pois \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     43\u001b[0m                     ):\n\u001b[1;32m     45\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    args: \u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    type_POIs : list of type of POIs.                         >>> ['stadium','nightclub']\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    expanded: '' if we look at the intensity of netmob consumption at the POI. '_expanded' if we look also one square around.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     trainer,ds,args,trial_id,df_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_the_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodification\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfold_to_evaluate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReload dataset without shuffling on train set, and remove data_augmentation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/prediction-validation/examples/train_and_visu_non_recurrent.py:76\u001b[0m, in \u001b[0;36mtrain_the_config\u001b[0;34m(args_init, modification, fold_to_evaluate)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_the_config\u001b[39m(args_init,modification,fold_to_evaluate):\n\u001b[0;32m---> 76\u001b[0m     ds,args,trial_id,save_folder,df_loss \u001b[38;5;241m=\u001b[39m \u001b[43mget_ds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodification\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfold_to_evaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_to_evaluate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     trainer,df_loss \u001b[38;5;241m=\u001b[39m train_on_ds(ds,args,trial_id,save_folder,df_loss)\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer,ds,args,trial_id,df_loss\n",
      "File \u001b[0;32m~/prediction-validation/examples/train_and_visu_non_recurrent.py:203\u001b[0m, in \u001b[0;36mget_ds\u001b[0;34m(model_name, dataset_names, dataset_for_coverage, modification, args_init, fold_to_evaluate)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_ds\u001b[39m(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,dataset_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,dataset_for_coverage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    199\u001b[0m            modification \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m    200\u001b[0m            args_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m    201\u001b[0m            fold_to_evaluate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    202\u001b[0m             ):\n\u001b[0;32m--> 203\u001b[0m     args_with_contextual,K_subway_ds \u001b[38;5;241m=\u001b[39m \u001b[43mget_multi_ds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs_init\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mdataset_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs_init\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mdataset_for_coverage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset_for_coverage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs_init\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_for_coverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mmodification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43margs_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mfold_to_evaluate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     ds \u001b[38;5;241m=\u001b[39m K_subway_ds[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    210\u001b[0m     trial_id \u001b[38;5;241m=\u001b[39m get_trial_id(args_with_contextual)\n",
      "File \u001b[0;32m~/prediction-validation/examples/train_and_visu_non_recurrent.py:251\u001b[0m, in \u001b[0;36mget_multi_ds\u001b[0;34m(model_name, dataset_names, dataset_for_coverage, modification, args_init, fold_to_evaluate)\u001b[0m\n\u001b[1;32m    246\u001b[0m     folds \u001b[38;5;241m=\u001b[39m fold_to_evaluate\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;66;03m#if args_init.hp_tuning_on_first_fold:\u001b[39;00m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m#    folds = [0] + folds\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m K_fold_splitter,K_subway_ds,args_with_contextual \u001b[38;5;241m=\u001b[39m \u001b[43mget_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (args_init\u001b[38;5;241m.\u001b[39mhp_tuning_on_first_fold) \u001b[38;5;129;01mand\u001b[39;00m (folds \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39marange(args_init\u001b[38;5;241m.\u001b[39mK_fold))):\n\u001b[1;32m    254\u001b[0m     K_subway_ds \u001b[38;5;241m=\u001b[39m K_subway_ds[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/prediction-validation/examples/benchmark.py:52\u001b[0m, in \u001b[0;36mget_inputs\u001b[0;34m(args, folds)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_inputs\u001b[39m(args,folds):\n\u001b[1;32m     51\u001b[0m     K_fold_splitter \u001b[38;5;241m=\u001b[39m KFoldSplitter(args,folds)\n\u001b[0;32m---> 52\u001b[0m     K_subway_ds,args \u001b[38;5;241m=\u001b[39m \u001b[43mK_fold_splitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_k_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m## Specific case if we want to validate on the init entiere dataset:\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (args\u001b[38;5;241m.\u001b[39mevaluate_complete_ds \u001b[38;5;129;01mand\u001b[39;00m args\u001b[38;5;241m.\u001b[39mvalidation_split_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_blocked_cv\u001b[39m\u001b[38;5;124m'\u001b[39m): \n",
      "File \u001b[0;32m~/prediction-validation/K_fold_validation/K_fold_validation.py:79\u001b[0m, in \u001b[0;36mKFoldSplitter.split_k_fold\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_blocked_cv()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_split_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward_chaining_cv\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_chaining_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "File \u001b[0;32m~/prediction-validation/K_fold_validation/K_fold_validation.py:116\u001b[0m, in \u001b[0;36mKFoldSplitter.forward_chaining_cv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----------------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading the Complete Dataset for K-fold splitting\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m target_ds_init,_,args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_init_ds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load 'U' and 'U_target'. # Define already feature vect for the K-th fold with proportion train/valid/test.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Get Init Coverage Period\u001b[39;00m\n\u001b[1;32m    118\u001b[0m df_verif_init \u001b[38;5;241m=\u001b[39m target_ds_init\u001b[38;5;241m.\u001b[39mtensor_limits_keeper\u001b[38;5;241m.\u001b[39mdf_verif \n",
      "File \u001b[0;32m~/prediction-validation/K_fold_validation/K_fold_validation.py:71\u001b[0m, in \u001b[0;36mKFoldSplitter.load_init_ds\u001b[0;34m(self, normalize)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_init_ds\u001b[39m(\u001b[38;5;28mself\u001b[39m,normalize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 71\u001b[0m     target_ds,contextual_ds,args \u001b[38;5;241m=\u001b[39m \u001b[43mload_complete_ds\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#,dic_class2rpz\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(target_ds,contextual_ds,args)\n",
      "File \u001b[0;32m~/prediction-validation/build_inputs/load_preprocessed_dataset.py:232\u001b[0m, in \u001b[0;36mload_complete_ds\u001b[0;34m(args, coverage_period, normalize)\u001b[0m\n\u001b[1;32m    229\u001b[0m args \u001b[38;5;241m=\u001b[39m get_args_embedding(args,dict_calendar_U_train)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# Contextual: \u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m args,contextual_ds \u001b[38;5;241m=\u001b[39m \u001b[43mtackle_contextual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43minvalid_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43munion_invalid_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mcoverage_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintersect_coverage_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Add Contextual Tensors and their positions: \u001b[39;00m\n\u001b[1;32m    239\u001b[0m target_ds,args \u001b[38;5;241m=\u001b[39m add_contextual_data(args,target_ds,contextual_ds,dict_calendar_U_train,dict_calendar_U_valid,dict_calendar_U_test)\n",
      "File \u001b[0;32m~/prediction-validation/build_inputs/load_contextual_data.py:141\u001b[0m, in \u001b[0;36mtackle_contextual\u001b[0;34m(target_ds, invalid_dates, coverage_period, args, normalize)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# USE CONTEXTUAL DATA\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args\u001b[38;5;241m.\u001b[39mcontextual_dataset_names) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[0;32m--> 141\u001b[0m     contextual_ds,args \u001b[38;5;241m=\u001b[39m \u001b[43mtackle_input_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43minvalid_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcoverage_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# TACKLE THE FEATURE EXTRACTOR MODULE \u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mvision_model_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \n",
      "File \u001b[0;32m~/prediction-validation/build_inputs/load_contextual_data.py:89\u001b[0m, in \u001b[0;36mtackle_input_data\u001b[0;34m(invalid_dates, coverage_period, args, normalize)\u001b[0m\n\u001b[1;32m     87\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(module_path)\n\u001b[1;32m     88\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(module)\n\u001b[0;32m---> 89\u001b[0m contextual_ds_i \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mFOLDER_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mcoverage_period\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcoverage_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43minvalid_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minvalid_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m contextual_ds_i\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m dataset_name\n\u001b[1;32m     96\u001b[0m contextual_ds[dataset_name] \u001b[38;5;241m=\u001b[39m contextual_ds_i\n",
      "File \u001b[0;32m~/prediction-validation/load_inputs/tramway_indiv.py:113\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(ROOT, FOLDER_PATH, invalid_dates, coverage_period, args, normalize, data_subfolder, file_base_name)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   ERROR while pre-processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJusque là les données sont filtrés et on selectionne un ensemble de stations explicative.\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124m                          par contre, il y a toujours plus de stations qu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mil n\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my a d\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munité spatiale à prédire\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124m                          si on fait le choix de prédire le nombre de passeger des stations de metro.\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124m                          En tout cas il y a une incompatibilité entre la target station qui a N unité spatiale, et\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124m                          la donnée contextuelle qui a M unité spatiale.\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124m                          On doit intégrer une fcontion d\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maggregation spatiale et des données statique géométrique.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# --- Création et Prétraitement avec PersonnalInput ---\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Utilisation de la fonction helper locale\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m#print(\"   Création et prétraitement de l'objet PersonnalInput...\")\u001b[39;00m\n\u001b[1;32m    122\u001b[0m dims \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# if [0] then Normalisation on temporal dim\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Jusque là les données sont filtrés et on selectionne un ensemble de stations explicative.                              par contre, il y a toujours plus de stations qu'il n'y a d'unité spatiale à prédire                              si on fait le choix de prédire le nombre de passeger des stations de metro.                              En tout cas il y a une incompatibilité entre la target station qui a N unité spatiale, et                              la donnée contextuelle qui a M unité spatiale.                              On doit intégrer une fcontion d'aggregation spatiale et des données statique géométrique."
     ]
    }
   ],
   "source": [
    "# GET PARAMETERS\n",
    "import os \n",
    "import sys\n",
    "import torch \n",
    "# Get Parent folder : \n",
    "\n",
    "current_path = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_path, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "from plotting.plotting import error_per_station_calendar_pattern  \n",
    "from examples.benchmark import local_get_args\n",
    "from examples.train_and_visu_non_recurrent import evaluate_config,train_the_config,get_ds\n",
    "from high_level_DL_method import load_optimizer_and_scheduler\n",
    "from dl_models.full_model import full_model\n",
    "from trainer import Trainer\n",
    "\n",
    "# Init:\n",
    "#['subway_indiv','tramway_indiv','bus_indiv','velov','criter']\n",
    "target_data = 'subway_in' #'subway_in'\n",
    "dataset_names = ['subway_in','subway_out'] #['subway_in'] ['subway_in','subway_indiv'] #[\"subway_in\",\"subway_out\"] # ['subway_in','netmob_POIs_per_station'],[\"subway_in\",\"subway_out\"],[\"subway_in\",\"calendar\"] # [\"subway_in\"] # ['data_bidon'] # ['METR_LA'] # ['PEMS_BAY']\n",
    "dataset_for_coverage = ['subway_in','netmob_image_per_station']#['subway_in','subway_indiv'] # ['subway_in','netmob_image_per_station'] #  ['data_bidon','netmob'] #  ['subway_in','netmob']  # ['METR_LA'] # ['PEMS_BAY']\n",
    "model_name = 'STGCN' # 'STGCN', 'ASTGCN'\n",
    "station = ['BEL','PAR','AMP','SAN','FLA']   # 'BON'  #'GER'\n",
    "# ...\n",
    "\n",
    "# Modif \n",
    "modification = {'target_data': target_data, \n",
    "                'freq': '15min',\n",
    "                'use_target_as_context': False,\n",
    "                'data_augmentation': False,\n",
    "        \n",
    "                'epochs' : 200, #100\n",
    "\n",
    "                'lr': 0.00105, # 5e-5,# 4e-4,\n",
    "                'weight_decay': 0.0188896655584368, # 0.05,\n",
    "                'dropout': 0.271795372610271, # 0.15,\n",
    "                'Kt': 2,\n",
    "                'stblock_num': 3,\n",
    "                'gso_type': 'sym_renorm_adj',\n",
    "                'temporal_h_dim': 256,\n",
    "                'spatial_h_dim': 32,\n",
    "                'output_h_dim': 16,\n",
    "                'scheduler': True,  # None\n",
    "                'torch_scheduler_milestone': 28.0, #5,\n",
    "                'torch_scheduler_gamma': 0.9958348861339396, # 0.997,\n",
    "                'torch_scheduler_lr_start_factor': 0.8809942312067847, # 1,\n",
    "\n",
    "\n",
    "                #'set_spatial_units':  station,   \n",
    "                'adj_type':'corr',\n",
    "                'threshold': 0.7,\n",
    "                'learnable_adj_matrix' : False, # True                              # EXIST ONLY IF MODEL = STGCN\n",
    "                'graph_conv_type': 'graph_conv', # 'cheb_graph_conv', 'graph_conv'  # EXIST ONLY IF MODEL = STGCN\n",
    "                'learnable_adj_top_k': 10,                                          # EXIST ONLY IF MODEL = STGCN\n",
    "                'learnable_adj_embd_dim': 16,                                       # EXIST ONLY IF MODEL = STGCN  \n",
    "\n",
    "                'stacked_contextual': True, # True # False\n",
    "                'temporal_graph_transformer_encoder': False,\n",
    "                'compute_node_attr_with_attn' : False, # True ??\n",
    "\n",
    "                #\n",
    "                #'graph_conv_type': 'graph_conv', # 'cheb_graph_conv', 'graph_conv'\n",
    "                #'learnable_adj_top_k': 10,\n",
    "                #'learnable_adj_embd_dim': 16, \n",
    "                ### ========\n",
    "\n",
    "                ### Temporal Graph Transfermer Encoder parametrs:\n",
    "                #'TGE_num_layers' : 4, #2\n",
    "                #'TGE_num_heads' :  1, #IMPOSSIBLE > 1 CAR DOIT DIVISER L = 7\n",
    "                #'TGE_FC_hdim' :  32, #32\n",
    "\n",
    "                ### Netmob Parametrs: \n",
    "                #'NetMob_only_epsilon': True,    # True # False\n",
    "                #'NetMob_selected_apps': ['Apple_iMessage','Web_Ads'],# ['Apple_iMessage','Web_Ads'], #,'Deezer','WhatsApp','Twitter'] #['Google_Maps']# ['Instagram','Google_Maps','Twitter']\n",
    "                #'NetMob_transfer_mode' :  ['DL'], #,'UL'] # ['DL'] # ['UL'] #['DL','UL']\n",
    "                #'NetMob_selected_tags' : ['station_epsilon100'],#['iris','stadium','station','university']#['park','stadium','university','station','shop','nightclub','parkings','theatre','iris','transit','public_transport']\n",
    "                #'NetMob_expanded' : '', # '' # '_expanded\n",
    "\n",
    "                ### Compute node with attention parameters: \n",
    "                #'vision_num_heads':6,\n",
    "                #\"vision_grn_out_dim\":48,\n",
    "                #'vision_model_name': 'VariableSelectionNetwork',\n",
    "                #'vision_concatenation_early':True,   \n",
    "                #'vision_concatenation_late':True,\n",
    "                           }\n",
    "# ...\n",
    "#1038945\n",
    "\n",
    "# Training and visu: \n",
    "args_init = local_get_args(model_name,\n",
    "                    args_init = None,\n",
    "                    dataset_names=dataset_names,\n",
    "                    dataset_for_coverage=dataset_for_coverage,\n",
    "                    modification = modification)\n",
    "\n",
    "training_mode_to_visualise = ['test'] # ['test','valid','train']\n",
    "\n",
    "if False:\n",
    "    ds,args,trial_id,save_folder,df_loss = get_ds(modification=modification,args_init=args_init)\n",
    "    model = full_model(ds, args).to(args.device)\n",
    "    optimizer,scheduler,loss_function = load_optimizer_and_scheduler(model,args)\n",
    "    trainer = Trainer(ds,model,args,optimizer,loss_function,scheduler = scheduler,show_figure = False,trial_id = trial_id, fold=0,save_folder = save_folder)\n",
    "    trainer.train_and_valid(normalizer = ds.normalizer, mod = 1000,mod_plot = None) \n",
    "\n",
    "else:\n",
    "    (trainer,ds,ds_no_shuffle,args) = evaluate_config(args_init = args_init,\n",
    "                                                    station=station,modification=modification,\n",
    "                                                    training_mode_to_visualise=training_mode_to_visualise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "DATE_COL = 'VAL_DATE'\n",
    "LOCATION_COL = 'VAL_ARRET_CODE'\n",
    "VALUE_COL = 'Flow'\n",
    "\n",
    "START = '2019-11-01' # Exemple basé sur head()\n",
    "END = '2020-04-30 23:30:00'\n",
    "MIN_AVG_DAILY_PASSENGER = 24*10\n",
    "\n",
    "dirname = '/home/rrochas/prediction-validation/../../../../data/rrochas/prediction_validation/agg_data/validation_individuelle/tramway_indiv_15min/tramway_indiv_15min.csv'\n",
    "df = pd.read_csv(dirname,index_col=0)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "df['station_lane_sens'] = df['VAL_ARRET_CODE'].astype(str) + '_' + df['LIG_NUMERO_SAE'].astype(str) + '_' + df['CRS_SENS_TRAJET'].astype(str)\n",
    "df = df.pivot_table(index=DATE_COL,columns='station_lane_sens',values=VALUE_COL)\n",
    "reindex = pd.date_range(start=START, end=END, freq='15min')[:-1]\n",
    "df = df.reindex(reindex).fillna(0)\n",
    "\n",
    "# Filter useless stations: \n",
    "mask_init = df.resample('1D').sum().mean()>MIN_AVG_DAILY_PASSENGER\n",
    "mask = mask_init[mask_init].index.to_list()\n",
    "df = df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>> Load best CONFIG\n",
      "----------------------------------------\n",
      "Loading the Complete Dataset for K-fold splitting\n",
      "Coverage Period: 7392 elts between 2019-03-16 00:00:00 and 2019-05-31 23:45:00\n",
      "Invalid dates within this fold: 776\n",
      "\n",
      ">>>Tackle Target dataset: subway_in\n",
      "   Load data from: /home/rrochas/prediction-validation/../../../../data/rrochas/prediction_validation/subway_in/subway_in.csv\n",
      "   Init Dataset: 'torch.Size([7392, 40]). 0 Nan values\n",
      "   TRAIN contextual_ds: torch.Size([2821, 40, 7])\n",
      "   VALID contextual_ds: torch.Size([940, 40, 7])\n",
      "   TEST contextual_ds: torch.Size([940, 40, 7])\n",
      "\n",
      ">>>Tackle Contextual dataset:  subway_out\n",
      "   Load data from: /home/rrochas/prediction-validation/../../../../data/rrochas/prediction_validation/subway_out/subway_out.csv\n",
      "   Init Dataset: '[torch.Size([7392, 40])]. [tensor(0)] Nan values\n",
      "   TRAIN contextual_ds: [torch.Size([2821, 40, 7])]\n",
      "   VALID contextual_ds: [torch.Size([940, 40, 7])]\n",
      "   TEST contextual_ds: [torch.Size([940, 40, 7])]\n",
      "Init U/Utarget size: torch.Size([4702, 40, 7])/torch.Size([4702, 40, 1]) Train/Valid/Test 2821 940 940\n",
      "\n",
      " ===== ERROR WITH prefetch_factor====  \n",
      "ValueError: prefetch_factor option could only be specified in multiprocessing.let num_workers > 0 to enable multiprocessing\n",
      "\n",
      "----------------------------------------\n",
      "Loading the dataset for fold n°1\n",
      "Coverage Period: 5069 elts between 2019-03-16 00:00:00 and 2019-05-07 19:00:00\n",
      "Invalid dates within this fold: 481\n",
      "\n",
      ">>>Tackle Target dataset: subway_in\n",
      "   Load data from: /home/rrochas/prediction-validation/../../../../data/rrochas/prediction_validation/subway_in/subway_in.csv\n",
      "   Number of Considered Spatial-Unit:  40\n",
      "   Init Dataset: 'torch.Size([5069, 40]). 0 Nan values\n",
      "   TRAIN contextual_ds: torch.Size([1412, 40, 7])\n",
      "   VALID contextual_ds: torch.Size([940, 40, 7])\n",
      "   TEST contextual_ds: torch.Size([939, 40, 7])\n",
      "\n",
      ">>>Tackle Contextual dataset:  subway_out\n",
      "   Load data from: /home/rrochas/prediction-validation/../../../../data/rrochas/prediction_validation/subway_out/subway_out.csv\n",
      "   Number of Considered Spatial-Unit:  40\n",
      "   Init Dataset: '[torch.Size([5069, 40])]. [tensor(0)] Nan values\n",
      "   TRAIN contextual_ds: [torch.Size([1412, 40, 7])]\n",
      "   VALID contextual_ds: [torch.Size([940, 40, 7])]\n",
      "   TEST contextual_ds: [torch.Size([939, 40, 7])]\n",
      "Init U/Utarget size: torch.Size([3292, 40, 7])/torch.Size([3292, 40, 1]) Train/Valid/Test 1412 940 939\n",
      "\n",
      " ===== ERROR WITH prefetch_factor====  \n",
      "ValueError: prefetch_factor option could only be specified in multiprocessing.let num_workers > 0 to enable multiprocessing\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain_and_valid(normalizer \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mnormalizer, mod \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m,mod_plot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     (trainer,ds,ds_no_shuffle,args) \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_init\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mstation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstation\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mtraining_mode_to_visualise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_mode_to_visualise\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/prediction-validation/examples/train_and_visu_non_recurrent.py:54\u001b[0m, in \u001b[0;36mevaluate_config\u001b[0;34m(args_init, modification, fold_to_evaluate, training_mode_to_visualise, station, transfer_modes, type_POIs, spatial_units, apps, POI_or_stations, expanded, individual_poi, sum_ts_pois)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_config\u001b[39m(args_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m                     modification \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m     32\u001b[0m                     fold_to_evaluate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m                     sum_ts_pois \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     43\u001b[0m                     ):\n\u001b[1;32m     45\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    args: \u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    type_POIs : list of type of POIs.                         >>> ['stadium','nightclub']\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    expanded: '' if we look at the intensity of netmob consumption at the POI. '_expanded' if we look also one square around.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     trainer,ds,args,trial_id,df_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_the_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodification\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfold_to_evaluate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReload dataset without shuffling on train set, and remove data_augmentation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/prediction-validation/examples/train_and_visu_non_recurrent.py:76\u001b[0m, in \u001b[0;36mtrain_the_config\u001b[0;34m(args_init, modification, fold_to_evaluate)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_the_config\u001b[39m(args_init,modification,fold_to_evaluate):\n\u001b[0;32m---> 76\u001b[0m     ds,args,trial_id,save_folder,df_loss \u001b[38;5;241m=\u001b[39m \u001b[43mget_ds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodification\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfold_to_evaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_to_evaluate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     trainer,df_loss \u001b[38;5;241m=\u001b[39m train_on_ds(ds,args,trial_id,save_folder,df_loss)\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer,ds,args,trial_id,df_loss\n",
      "File \u001b[0;32m~/prediction-validation/examples/train_and_visu_non_recurrent.py:209\u001b[0m, in \u001b[0;36mget_ds\u001b[0;34m(model_name, dataset_names, dataset_for_coverage, modification, args_init, fold_to_evaluate)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_ds\u001b[39m(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,dataset_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,dataset_for_coverage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    199\u001b[0m            modification \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m    200\u001b[0m            args_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m    201\u001b[0m            fold_to_evaluate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    202\u001b[0m             ):\n\u001b[1;32m    203\u001b[0m     args_with_contextual,K_subway_ds \u001b[38;5;241m=\u001b[39m get_multi_ds(model_name \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m args_init\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[1;32m    204\u001b[0m                                                     dataset_names \u001b[38;5;28;01mif\u001b[39;00m dataset_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m args_init\u001b[38;5;241m.\u001b[39mdataset_names,\n\u001b[1;32m    205\u001b[0m                                                     dataset_for_coverage \u001b[38;5;28;01mif\u001b[39;00m dataset_for_coverage \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m args_init\u001b[38;5;241m.\u001b[39mdataset_for_coverage,\n\u001b[1;32m    206\u001b[0m                                                     modification,\n\u001b[1;32m    207\u001b[0m                                                     args_init,\n\u001b[1;32m    208\u001b[0m                                                     fold_to_evaluate)\n\u001b[0;32m--> 209\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mK_subway_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    210\u001b[0m     trial_id \u001b[38;5;241m=\u001b[39m get_trial_id(args_with_contextual)\n\u001b[1;32m    211\u001b[0m     save_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# GET PARAMETERS\n",
    "import os \n",
    "import sys\n",
    "import torch \n",
    "# Get Parent folder : \n",
    "\n",
    "current_path = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_path, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "from plotting.plotting import error_per_station_calendar_pattern  \n",
    "from examples.benchmark import local_get_args\n",
    "from examples.train_and_visu_non_recurrent import evaluate_config,train_the_config,get_ds\n",
    "from high_level_DL_method import load_optimizer_and_scheduler\n",
    "from dl_models.full_model import full_model\n",
    "from trainer import Trainer\n",
    "\n",
    "from examples.train_model_on_k_fold_validation import train_model_on_k_fold_validation,load_configuration,train_valid_1_model\n",
    "import torch\n",
    "save_folder = 'K_fold_validation/training_with_HP_tuning/subway_in_subway_out'\n",
    "trial_id = 'subway_in_subway_out_STGCN_MSELoss_2025_02_19_00_05_19271'\n",
    "args_init,folds = load_configuration(trial_id,True)\n",
    "\n",
    "training_mode_to_visualise = ['test'] # ['test','valid','train']\n",
    "station = ['BEL','PAR','AMP','SAN','FLA']   # 'BON'  #'GER'\n",
    "\n",
    "modification ={'keep_best_weights':True,\n",
    "                'epochs':100,\n",
    "                'device':torch.device(\"cuda:0\"),\n",
    "                'target_data':'subway_in',\n",
    "                \n",
    "                'use_target_as_context': False,\n",
    "                'freq':'15min',\n",
    "                'minmaxnorm':True,\n",
    "                'standardize': False,\n",
    "                'learnable_adj_matrix' : False,\n",
    "                'data_augmentation': False,\n",
    "                }\n",
    "\n",
    "config_diffs = {}\n",
    "config_diffs.update({'subway_in_subway_out':{'dataset_names':['subway_in','subway_out'],\n",
    "                                        'data_augmentation': True,\n",
    "                                        'DA_method': 'rich_interpolation',\n",
    "                                        'stacked_contextual': True,\n",
    "                                        'temporal_graph_transformer_encoder': False,\n",
    "                                        'compute_node_attr_with_attn': False,\n",
    "                                        }\n",
    "                            })\n",
    "for add_name_id,config_diff in config_diffs.items():\n",
    "    config_diff.update(modification)\n",
    "\n",
    "if False: \n",
    "    ds,args_ds,trial_id,save_folder,df_loss = get_ds(modification=modification,args_init=args_init)\n",
    "    model = full_model(ds, args_ds).to(args_ds.device)\n",
    "    optimizer,scheduler,loss_function = load_optimizer_and_scheduler(model,args_ds)\n",
    "    trainer = Trainer(ds,model,args_ds,optimizer,loss_function,scheduler = scheduler,show_figure = False,trial_id = trial_id, fold=0,save_folder = save_folder)\n",
    "    trainer.train_and_valid(normalizer = ds.normalizer, mod = 1000,mod_plot = None) \n",
    "else:\n",
    "    (trainer,ds,ds_no_shuffle,args) = evaluate_config(args_init = args_init,\n",
    "                                                    station=station,modification=modification,\n",
    "                                                    training_mode_to_visualise=training_mode_to_visualise)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'STGCN',\n",
       " 'dataset_names': ['subway_in', 'subway_out'],\n",
       " 'dataset_for_coverage': ['subway_in', 'netmob_POIs'],\n",
       " 'device': device(type='cuda', index=0),\n",
       " 'optimizer': 'adamw',\n",
       " 'single_station': False,\n",
       " 'loss_function_type': 'MSE',\n",
       " 'epsilon_clustering': 0.05,\n",
       " 'freq': '15min',\n",
       " 'contextual_positions': {'calendar': [0, 1], 'subway_out': 2},\n",
       " 'quick_vision': False,\n",
       " 'netmob_transfer_mode': 'DL',\n",
       " 'evaluate_complete_ds': True,\n",
       " 'train_valid_test_split_method': 'similar_length_method',\n",
       " 'set_spatial_units': None,\n",
       " 'hp_tuning_on_first_fold': True,\n",
       " 'keep_best_weights': True,\n",
       " 'num_workers': 0,\n",
       " 'persistent_workers': False,\n",
       " 'pin_memory': True,\n",
       " 'prefetch_factor': 2,\n",
       " 'drop_last': False,\n",
       " 'mixed_precision': False,\n",
       " 'non_blocking': True,\n",
       " 'torch_compile': False,\n",
       " 'backend': 'inductor',\n",
       " 'prefetch_all': False,\n",
       " 'NetMob_selected_apps': ['Google_Maps', 'Deezer', 'Instagram'],\n",
       " 'NetMob_transfer_mode': ['DL'],\n",
       " 'NetMob_selected_tags': ['iris'],\n",
       " 'NetMob_expanded': '',\n",
       " 'ray': False,\n",
       " 'ray_scheduler': 'ASHA',\n",
       " 'ray_search_alg': None,\n",
       " 'grace_period': 20,\n",
       " 'HP_max_epochs': 100,\n",
       " 'alpha': None,\n",
       " 'conformity_scores_type': 'max_residual',\n",
       " 'quantile_method': 'compute_quantile_by_class',\n",
       " 'calibration_calendar_class': 0,\n",
       " 'type_calib': 'classic',\n",
       " 'data_augmentation': True,\n",
       " 'DA_moment_to_focus': None,\n",
       " 'DA_min_count': 5,\n",
       " 'DA_method': 'rich_interpolation',\n",
       " 'DA_alpha': 1,\n",
       " 'DA_prop': 1,\n",
       " 'DA_noise_from': 'MSTL',\n",
       " 'H': 6,\n",
       " 'W': 0,\n",
       " 'D': 1,\n",
       " 'step_ahead': 1,\n",
       " 'L': 7,\n",
       " 'shuffle': True,\n",
       " 'train_prop': 0.6,\n",
       " 'calib_prop': None,\n",
       " 'valid_prop': 0.2,\n",
       " 'test_prop': 0.19999999999999996,\n",
       " 'track_pi': False,\n",
       " 'validation_split_method': 'forward_chaining_cv',\n",
       " 'min_fold_size_proportion': 0.75,\n",
       " 'no_common_dates_between_set': False,\n",
       " 'K_fold': 6,\n",
       " 'current_fold': 0,\n",
       " 'abs_path': '/home/rrochas/prediction-validation/',\n",
       " 'out_dim': 1,\n",
       " 'vision_model_name': None,\n",
       " 'vision_input_type': 'POIs',\n",
       " 'stacked_contextual': True,\n",
       " 'temporal_graph_transformer_encoder': False,\n",
       " 'compute_node_attr_with_attn': False,\n",
       " 'Kt': 2,\n",
       " 'stblock_num': 3,\n",
       " 'Ks': 2,\n",
       " 'graph_conv_type': 'graph_conv',\n",
       " 'gso_type': 'sym_renorm_adj',\n",
       " 'enable_bias': True,\n",
       " 'adj_type': 'corr',\n",
       " 'enable_padding': True,\n",
       " 'threshold': 0.3,\n",
       " 'act_func': 'glu',\n",
       " 'temporal_h_dim': 256,\n",
       " 'spatial_h_dim': 32,\n",
       " 'output_h_dim': 16,\n",
       " 'TGE_num_layers': 2,\n",
       " 'TGE_num_heads': 2,\n",
       " 'TGE_FC_hdim': 32,\n",
       " 'blocks': [[2], [32, 32, 32], [32, 32, 32], [64]],\n",
       " 'weight_decay': 0.0188896655584368,\n",
       " 'batch_size': 32,\n",
       " 'lr': 0.00105,\n",
       " 'dropout': 0.271795372610271,\n",
       " 'epochs': 100,\n",
       " 'scheduler': True,\n",
       " 'n_vertex': 40,\n",
       " 'C': 2,\n",
       " 'args_embedding': Namespace(),\n",
       " 'args_vision': Namespace(),\n",
       " 'n_units_subway_out': 40,\n",
       " 'input_dim_subway_out': 7,\n",
       " 'ds_which_need_spatial_attn': [],\n",
       " 'pos_node_attributes': [2],\n",
       " 'dict_node_attr2dataset': {2: 'subway_out'},\n",
       " 'node_attr_which_need_attn': [],\n",
       " 'torch_scheduler_milestone': 28.0,\n",
       " 'torch_scheduler_gamma': 0.9958348861339396,\n",
       " 'torch_scheduler_lr_start_factor': 0.8809942312067847,\n",
       " 'target_data': 'subway_in',\n",
       " 'use_target_as_context': False,\n",
       " 'minmaxnorm': True,\n",
       " 'standardize': False,\n",
       " 'learnable_adj_matrix': False,\n",
       " 'contextual_dataset_names': ['subway_out']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(args_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer,scheduler,loss_function = load_optimizer_and_scheduler(model,args_ds)\n",
    "trainer = Trainer(ds,model,args,optimizer,loss_function,scheduler = scheduler,show_figure = False,trial_id = trial_id, fold=0,save_folder = save_folder)\n",
    "trainer.train_and_valid(normalizer = ds.normalizer, mod = 1000,mod_plot = None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'blocks': [[1], [32, 32, 32], [32, 32, 32], [64]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'STGCN',\n",
       " 'dataset_names': ['subway_in', 'subway_out'],\n",
       " 'dataset_for_coverage': ['subway_in', 'netmob_POIs'],\n",
       " 'device': device(type='cuda'),\n",
       " 'optimizer': 'adamw',\n",
       " 'single_station': False,\n",
       " 'loss_function_type': 'MSE',\n",
       " 'epsilon_clustering': 0.05,\n",
       " 'freq': '15min',\n",
       " 'contextual_positions': {'subway_out': 2},\n",
       " 'quick_vision': False,\n",
       " 'netmob_transfer_mode': 'DL',\n",
       " 'evaluate_complete_ds': True,\n",
       " 'train_valid_test_split_method': 'similar_length_method',\n",
       " 'set_spatial_units': None,\n",
       " 'hp_tuning_on_first_fold': True,\n",
       " 'keep_best_weights': False,\n",
       " 'num_workers': 0,\n",
       " 'persistent_workers': False,\n",
       " 'pin_memory': True,\n",
       " 'prefetch_factor': 2,\n",
       " 'drop_last': False,\n",
       " 'mixed_precision': False,\n",
       " 'non_blocking': True,\n",
       " 'torch_compile': False,\n",
       " 'backend': 'inductor',\n",
       " 'prefetch_all': False,\n",
       " 'NetMob_selected_apps': ['Google_Maps', 'Deezer', 'Instagram'],\n",
       " 'NetMob_transfer_mode': ['DL'],\n",
       " 'NetMob_selected_tags': ['iris'],\n",
       " 'NetMob_expanded': '',\n",
       " 'ray': False,\n",
       " 'ray_scheduler': 'ASHA',\n",
       " 'ray_search_alg': None,\n",
       " 'grace_period': 20,\n",
       " 'HP_max_epochs': 100,\n",
       " 'alpha': None,\n",
       " 'conformity_scores_type': 'max_residual',\n",
       " 'quantile_method': 'compute_quantile_by_class',\n",
       " 'calibration_calendar_class': 0,\n",
       " 'type_calib': 'classic',\n",
       " 'data_augmentation': True,\n",
       " 'DA_moment_to_focus': None,\n",
       " 'DA_min_count': 5,\n",
       " 'DA_method': 'rich_interpolation',\n",
       " 'DA_alpha': 1,\n",
       " 'DA_prop': 1,\n",
       " 'DA_noise_from': 'MSTL',\n",
       " 'H': 6,\n",
       " 'W': 0,\n",
       " 'D': 1,\n",
       " 'step_ahead': 1,\n",
       " 'L': 7,\n",
       " 'shuffle': True,\n",
       " 'train_prop': 0.6,\n",
       " 'calib_prop': None,\n",
       " 'valid_prop': 0.2,\n",
       " 'test_prop': 0.19999999999999996,\n",
       " 'track_pi': False,\n",
       " 'validation_split_method': 'forward_chaining_cv',\n",
       " 'min_fold_size_proportion': 0.75,\n",
       " 'no_common_dates_between_set': False,\n",
       " 'K_fold': 6,\n",
       " 'current_fold': 0,\n",
       " 'abs_path': '/home/rrochas/prediction-validation/',\n",
       " 'out_dim': 1,\n",
       " 'vision_model_name': None,\n",
       " 'vision_input_type': 'POIs',\n",
       " 'stacked_contextual': True,\n",
       " 'temporal_graph_transformer_encoder': False,\n",
       " 'compute_node_attr_with_attn': False,\n",
       " 'Kt': 2,\n",
       " 'stblock_num': 3,\n",
       " 'Ks': 2,\n",
       " 'graph_conv_type': 'graph_conv',\n",
       " 'gso_type': 'sym_renorm_adj',\n",
       " 'enable_bias': True,\n",
       " 'adj_type': 'corr',\n",
       " 'enable_padding': True,\n",
       " 'threshold': 0.3,\n",
       " 'act_func': 'glu',\n",
       " 'temporal_h_dim': 256,\n",
       " 'spatial_h_dim': 32,\n",
       " 'output_h_dim': 16,\n",
       " 'TGE_num_layers': 2,\n",
       " 'TGE_num_heads': 2,\n",
       " 'TGE_FC_hdim': 32,\n",
       " 'blocks': [[1], [32, 32, 32], [32, 32, 32], [64]],\n",
       " 'weight_decay': 0.0188896655584368,\n",
       " 'batch_size': 32,\n",
       " 'lr': 0.00105,\n",
       " 'dropout': 0.271795372610271,\n",
       " 'epochs': 100,\n",
       " 'scheduler': True,\n",
       " 'n_vertex': 40,\n",
       " 'C': 2,\n",
       " 'args_embedding': Namespace(),\n",
       " 'args_vision': Namespace(),\n",
       " 'n_units_subway_out': 40,\n",
       " 'input_dim_subway_out': 7,\n",
       " 'ds_which_need_spatial_attn': [],\n",
       " 'pos_node_attributes': [2],\n",
       " 'dict_node_attr2dataset': {2: 'subway_out'},\n",
       " 'node_attr_which_need_attn': [],\n",
       " 'torch_scheduler_milestone': 28.0,\n",
       " 'torch_scheduler_gamma': 0.9958348861339396,\n",
       " 'torch_scheduler_lr_start_factor': 0.8809942312067847}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['METR_LA',\n",
       " 'PEMS_BAY',\n",
       " 'PEMS_d3',\n",
       " 'PEMS_d4',\n",
       " 'PEMS_d7',\n",
       " 'data_bidon',\n",
       " 'generate_data.ipynb',\n",
       " 'netmob_bidon',\n",
       " 'ref_subway.csv',\n",
       " 'subway_in',\n",
       " 'netmob_video_lyon',\n",
       " 'netmob_image_per_station',\n",
       " 'NetMob_lyon.geojson',\n",
       " 'lyon_iris_shapefile',\n",
       " 'POIs',\n",
       " 'subway_out',\n",
       " 'CRITER_3lanes',\n",
       " 'agg_data']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calendar_dayofweek :  torch.Size([971, 7])\n",
      "calendar_hour :  torch.Size([971, 18])\n",
      "subway_indiv :  torch.Size([834, 40, 7])\n"
     ]
    }
   ],
   "source": [
    "contextual_train = {name_i: contextual_i['train'] for name_i,contextual_i in ds.contextual_tensors.items()}  \n",
    "contextual_valid ={name_i: contextual_i['valid'] for name_i,contextual_i in ds.contextual_tensors.items() if 'valid' in contextual_i.keys()}  \n",
    "contextual_test  = {name_i: contextual_i['test'] for name_i,contextual_i in ds.contextual_tensors.items() if 'test' in contextual_i.keys()}  \n",
    "\n",
    "for name_i in contextual_train.keys():\n",
    "    print(name_i,': ',contextual_train[name_i].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontextual_positions: \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mcontextual_positions)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m contextual_i \u001b[38;5;129;01min\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mcontextual_tensors\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "print('contextual_positions: ',ds.contextual_positions)\n",
    "for contextual_i in ds.contextual_tensors.keys():\n",
    "    print()\n",
    "    for training_mode in ['train','valid','test']:\n",
    "        print(f\"contextual_tensors[{contextual_i}][{training_mode}]: \",ds.contextual_tensors[contextual_i][training_mode].size())\n",
    "\n",
    "print('\\nFirst/Last predicted train: ',ds.tensor_limits_keeper.first_predicted_train_date,ds.tensor_limits_keeper.last_predicted_train_date,'\\n',\n",
    "'First/Last predicted valid: ',ds.tensor_limits_keeper.first_predicted_valid_date,ds.tensor_limits_keeper.last_predicted_valid_date,'\\n',\n",
    "'First/Last predicted test: ',ds.tensor_limits_keeper.first_predicted_test_date,ds.tensor_limits_keeper.last_predicted_test_date\n",
    ")\n",
    "print('\\ndf dates: ')\n",
    "display(ds.tensor_limits_keeper.df_dates.sort_values(by='date'))\n",
    "print('\\ndf verif: ')\n",
    "ds.tensor_limits_keeper.df_verif\n",
    "\n",
    "output = next(iter(ds.dataloader['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2020-02-01 00:00:00', '2020-02-01 00:15:00',\n",
       "               '2020-02-01 00:30:00', '2020-02-01 00:45:00',\n",
       "               '2020-02-01 01:00:00', '2020-02-01 01:15:00',\n",
       "               '2020-02-01 01:30:00', '2020-02-01 01:45:00',\n",
       "               '2020-02-01 02:00:00', '2020-02-01 02:15:00',\n",
       "               ...\n",
       "               '2020-02-27 21:30:00', '2020-02-27 21:45:00',\n",
       "               '2020-02-27 22:00:00', '2020-02-27 22:15:00',\n",
       "               '2020-02-27 22:30:00', '2020-02-27 22:45:00',\n",
       "               '2020-02-27 23:00:00', '2020-02-27 23:15:00',\n",
       "               '2020-02-27 23:30:00', '2020-02-27 23:45:00'],\n",
       "              dtype='datetime64[ns]', name='VAL_DATE', length=2262, freq=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dirname = '/home/rrochas/prediction-validation/../../../../data/rrochas/prediction_validation/agg_data/validation_individuelle/subway_indiv_15min/'\n",
    "os.listdir(dirname)\n",
    "os.path.exists(f\"{dirname}/subway_indiv_15min.csv\")\n",
    "\n",
    "FILE_BASE_NAME = 'subway_indiv'\n",
    "DATA_SUBFOLDER = 'agg_data/validation_individuelle' # Sous-dossier dans FOLDER_PATH\n",
    "NATIVE_FREQ = '3min'\n",
    "START = '2019-10-01' # Exemple basé sur head()\n",
    "END = '2020-04-01'\n",
    "list_of_invalid_period = []\n",
    "C = 1\n",
    "\n",
    "DATE_COL = 'VAL_DATE'\n",
    "LOCATION_COL = 'COD_TRG'\n",
    "VALUE_COL = 'Flow'\n",
    "\n",
    "target_freq = '15min'\n",
    "file_name = f\"{FILE_BASE_NAME}_{target_freq}\"\n",
    "data_file_path = f\"{dirname}/{file_name}.csv\"\n",
    "\n",
    "df = pd.read_csv(data_file_path)\n",
    "df['VAL_DATE'] = pd.to_datetime(df['VAL_DATE'])\n",
    "df=df.set_index('VAL_DATE')\n",
    "df.index.unique().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start training\n",
      "\n",
      "Training Throughput:1063.21 sequences per seconds\n",
      ">>> Training complete in: 0:04:49.253988\n",
      ">>> Training performance time: min 0.016025543212890625 avg 0.028232812881469727 seconds (+/- 0.007409689556679389)\n",
      ">>> Loading performance time: min 0.00011897087097167969 avg 0.021025142456797832 seconds (+/- 0.0533871588248097)\n",
      ">>> Forward performance time: 0.014497923700226319 seconds (+/- 0.006785826679855182)\n",
      ">>> Backward performance time: 0.01471411531366729 seconds (+/- 0.0026613273244625673)\n",
      ">>> Plotting performance time: 2.2859429594260365e-06 seconds (+/- 7.815177432756738e-07)\n",
      ">>> Saving performance time: 0.6629389921824137 seconds (+/- 0.29361714449532417)\n",
      ">>> PI-tracking performance time: 5.977237643908016e-06 seconds (+/- 7.975377572890917e-06)\n",
      ">>> Scheduler-update performance time: 4.9972054946362674e-06 seconds (+/- 9.795780319996475e-06)\n",
      ">>> Validation time: 0:00:00.356755\n",
      "Proportion of time consumed for Loading: 41.6%\n",
      "Proportion of time consumed for Forward: 28.3%\n",
      "Proportion of time consumed for Backward: 28.7%\n",
      "Proportion of time consumed for Plotting: 0.0%\n",
      "Proportion of time consumed for CheckPoint Saving: 1.4%\n",
      "Proportion of time consumed for Tracking PI: 0.0%\n",
      "Proportion of time consumed for Update Scheduler: 0.0%\n",
      "Proportion of time consumed for Read all data on GPU: 0.0%\n",
      "\n",
      "Max GPU memory allocated: 0.18022966384887695 GB\n",
      "Max GPU memory cached: 0.2890625 GB\n",
      "Max CPU memory allocated: 3.2993011474609375 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "trainer.train_and_valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import eigs\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "L = (pd.DataFrame(np.random.rand(10,10))*10)\n",
    "\n",
    "\n",
    "lambda_max = eigs(L.values , k=1, which='LR')[0].real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'match_period_coverage_with_netmob' from 'utils.utilities_DL' (/home/rrochas/prediction-validation/utils/utilities_DL.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, working_dir)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Personnal import \u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities_DL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m match_period_coverage_with_netmob\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_args,update_modif\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpaths\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FOLDER_PATH,FILE_NAME,SAVE_DIRECTORY\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'match_period_coverage_with_netmob' from 'utils.utilities_DL' (/home/rrochas/prediction-validation/utils/utilities_DL.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Obtenir le chemin du dossier parent\n",
    "current_path = notebook_dir = os.getcwd()\n",
    "# current_path = os.path.dirname()\n",
    "working_dir = os.path.abspath(os.path.join(current_path, '..'))\n",
    "\n",
    "# Ajouter le dossier parent au chemin de recherche des modules\n",
    "if working_dir not in sys.path:\n",
    "    sys.path.insert(0, working_dir)\n",
    "\n",
    "# Personnal import \n",
    "from utils.utilities_DL import match_period_coverage_with_netmob\n",
    "from constants.config import get_args,update_modif\n",
    "from constants.paths import FOLDER_PATH,FILE_NAME,SAVE_DIRECTORY\n",
    "from K_fold_validation.K_fold_validation import KFoldSplitter\n",
    "from trainer import Trainer\n",
    "from high_level_DL_method import load_optimizer_and_scheduler\n",
    "from dl_models.full_model import full_model\n",
    "\n",
    "from plotting.plotting_bokeh import plot_bokeh\n",
    "\n",
    "\n",
    "# Load config\n",
    "model_name = 'STGCN' #'CNN'\n",
    "dataset_names = ['subway_in','netmob']\n",
    "args = get_args(model_name,dataset_names)\n",
    "\n",
    "# Modification : \n",
    "args.K_fold = 5\n",
    "\n",
    "args.ray = False\n",
    "args.W = 0  # IMPORTANT AVEC NETMOB\n",
    "\n",
    "args.epochs = 100\n",
    "args.loss_function_type = 'MSE' # 'quantile'\n",
    "\n",
    "# optimization:\n",
    "args.mixed_precision = True\n",
    "\n",
    "args = update_modif(args)\n",
    "\n",
    "# Coverage Period : \n",
    "small_ds = False\n",
    "coverage = match_period_coverage_with_netmob(FILE_NAME,dataset_names=['subway_in','netmob'])\n",
    "\n",
    "# Choose DataSet and VisionModel if needed: \n",
    "dataset_names = ['netmob','subway_in'] # ['calendar','netmob'] #['subway_in','netmob','calendar']\n",
    "vision_model_name = 'FeatureExtractor_ResNetInspired'  # 'ImageAvgPooling'  #'FeatureExtractor_ResNetInspired' #'MinimalFeatureExtractor',\n",
    "\n",
    "# Train and Evaluate Model: \n",
    "mod_plot = 1 # bokeh plotting every epoch \n",
    "\n",
    "# Load K-fold subway-ds \n",
    "folds = [0] # Here we use the first fold for HP-tuning. \n",
    "\n",
    "# In case we need to compute the Sliding K-fold validation:\n",
    "# folds = np.arange(1,args.K_fold)\n",
    "\n",
    "K_fold_splitter = KFoldSplitter(args,folds)\n",
    "K_subway_ds,args = K_fold_splitter.split_k_fold()\n",
    "subway_ds = K_subway_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model, trainer, and train it:\n",
    "model = load_model(args,dic_class2rpz)\n",
    "optimizer,scheduler,loss_function = load_optimizer_and_scheduler(model,args)\n",
    "trainer = Trainer(subway_ds,model,args,optimizer,loss_function,scheduler = scheduler,dic_class2rpz = dic_class2rpz,show_figure = True)# Ajoute dans trainer, if calibration_prop is not None .... et on modifie le dataloader en ajoutant un clabration set\n",
    "trainer.train_and_valid(mod = 1000,mod_plot = None)  # Récupère les conformity scores sur I1, avec les estimations faites precedemment \n",
    "\n",
    "# Plotting: \n",
    "pi,pi_cqr = plot_bokeh(trainer,subway_ds.normalizer,subway_ds.tensor_limits_keeper.df_verif_test,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model, trainer, and train it:\n",
    "model = load_model(args,dic_class2rpz)\n",
    "optimizer,scheduler,loss_function = load_optimizer_and_scheduler(model,args)\n",
    "trainer = Trainer(subway_ds,model,args,optimizer,loss_function,scheduler = scheduler,dic_class2rpz = dic_class2rpz,show_figure = True)# Ajoute dans trainer, if calibration_prop is not None .... et on modifie le dataloader en ajoutant un clabration set\n",
    "trainer.train_and_valid(mod = 1000,mod_plot = None)  # Récupère les conformity scores sur I1, avec les estimations faites precedemment \n",
    "\n",
    "# Plotting: \n",
    "pi,pi_cqr = plot_bokeh(trainer,subway_ds.normalizer,subway_ds.tensor_limits_keeper.df_verif_test,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init tensor:  tensor([2.2318e+09])\n",
      "Init feature vect:  tensor([7.0868e+09])\n",
      "\n",
      "block1:\n",
      "s_conv:  tensor(3.0692e+08) tconv:  tensor(1.2277e+09)\n",
      "\n",
      "block2:\n",
      "s_conv:  tensor(1.5548e+09) tconv:  tensor(1.2098e+09)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "netmob_init = torch.Tensor([7392*4*263*287])\n",
    "netmob_tensor = torch.Tensor([2934*4*263*287*8])\n",
    "\n",
    "print('Init tensor: ',netmob_init)\n",
    "print('Init feature vect: ',netmob_tensor)\n",
    "\n",
    "print('\\nblock1:')\n",
    "sconv = torch.prod(torch.Tensor([64, 32, 131, 143, 8]))\n",
    "tconv = torch.prod(torch.Tensor([64, 128, 131, 143, 8]))\n",
    "print('s_conv: ',sconv, 'tconv: ',tconv)\n",
    "\n",
    "print('\\nblock2:')\n",
    "sconv = torch.prod(torch.Tensor([64, 658, 65, 71, 8]))\n",
    "tconv = torch.prod(torch.Tensor([64, 512, 65, 71, 8]))\n",
    "print('s_conv: ',sconv, 'tconv: ',tconv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.0.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
