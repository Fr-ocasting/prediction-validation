{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Hyper-parameter tuning with Ray is not possible\n",
      "----------------------------------------\n",
      "Loading the Complete Dataset for K-fold splitting\n",
      "Coverage Period: 28224 elts between 2017-05-01 00:00:00 and 2017-08-06 23:55:00\n",
      "Invalid dates within this fold: 0\n",
      "\n",
      ">>>Tackle Target dataset: PeMS07\n",
      "   Load data from: /home/rrochas/prediction-validation/../../../../data/rrochas/prediction_validation/PEMS07\n",
      "   Data loaded with shape: (28224, 883)\n",
      "   len(predicted_serie): 27936\n",
      "   nb of nan values:  0\n",
      "   split_train_valid: 16761\n",
      "   split_valid_test: 22348\n",
      "   Access predicted_serie.iat[16761]: 2017-06-29 04:45:00\n",
      "   Access predicted_serie.iat[22348]: 2017-07-18 14:20:00\n",
      "   first_train_date: 2017-05-02 00:00:00, last_train_date: 2017-06-29 04:45:00\n",
      "   first_valid_date: 2017-06-29 04:45:00, last_valid_date: 2017-07-18 14:20:00\n",
      "   first_test_date: 2017-07-18 14:20:00, last_test_date: 2017-08-06 23:55:00\n",
      "   Init Dataset: 'torch.Size([28224, 883]). 0 Nan values\n",
      "   TRAIN contextual_ds: torch.Size([16761, 883, 7])\n",
      "   VALID contextual_ds: torch.Size([5587, 883, 7])\n",
      "   TEST contextual_ds: torch.Size([5587, 883, 7])\n",
      "Init U/Utarget size: torch.Size([27936, 883, 7])/torch.Size([27936, 883, 1]) Train/Valid/Test 16761 5587 5587\n",
      "\n",
      "----------------------------------------\n",
      "Loading the dataset for fold n°0\n",
      "Coverage Period: 17751 elts between 2017-05-01 00:00:00 and 2017-07-01 15:10:00\n",
      "Invalid dates within this fold: 0\n",
      "\n",
      ">>>Tackle Target dataset: PeMS07\n",
      "   Load data from: /home/rrochas/prediction-validation/../../../../data/rrochas/prediction_validation/PEMS07\n",
      "   Data loaded with shape: (17751, 883)\n",
      "   len(predicted_serie): 17463\n",
      "   nb of nan values:  0\n",
      "   split_train_valid: 6289\n",
      "   split_valid_test: 11876\n",
      "   Access predicted_serie.iat[6289]: 2017-05-23 20:05:00\n",
      "   Access predicted_serie.iat[11876]: 2017-06-12 05:40:00\n",
      "   first_train_date: 2017-05-02 00:00:00, last_train_date: 2017-05-23 20:05:00\n",
      "   first_valid_date: 2017-05-23 20:05:00, last_valid_date: 2017-06-12 05:40:00\n",
      "   first_test_date: 2017-06-12 05:40:00, last_test_date: 2017-07-01 15:10:00\n",
      "Values with issues:  0.113%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "Values with issues:  0.113%\n",
      "Regular Values that we have to set to 0:  0.020%\n",
      "Values with issues:  0.113%\n",
      "Regular Values that we have to set to 0:  0.001%\n",
      "Values with issues:  0.113%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "Values with issues:  0.113%\n",
      "Regular Values that we have to set to 0:  0.020%\n",
      "Values with issues:  0.113%\n",
      "Regular Values that we have to set to 0:  0.001%\n",
      "   Init Dataset: 'torch.Size([17751, 883]). 0 Nan values\n",
      "   TRAIN contextual_ds: torch.Size([6289, 883, 7])\n",
      "   VALID contextual_ds: torch.Size([5587, 883, 7])\n",
      "   TEST contextual_ds: torch.Size([5586, 883, 7])\n",
      "Init U/Utarget size: torch.Size([17463, 883, 7])/torch.Size([17463, 883, 1]) Train/Valid/Test 6289 5587 5586\n",
      ">>>>Model: STGCN; K_fold = 6; Loss function: MSE \n",
      "\n",
      "NetMob Vision is NONE\n",
      "Model size: 0.008GB\n",
      "\n",
      "start training\n",
      "epoch: 0 \n",
      " min\\epoch : 1.73\n",
      ">>> Training complete in: 0:01:43.828462\n",
      ">>> Training (99.66%) performance time: min 0.19 avg 3.36e-01 seconds (+/- 3.96e-02)\n",
      ">>> Loading (0.41%) performance time: min 0.00 avg 5.88e-04 seconds (+/- 3.64e-04)\n",
      ">>> Forward  (75.71%) performance time: 2.46e-01 seconds (+/- 3.92e-02)\n",
      ">>> Backward  (23.87%) performance time: 8.04e-02 seconds (+/- 2.04e-03)\n",
      ">>> Plotting  (0.00%) performance time: 6.24e-04 seconds\n",
      ">>> Saving  (0.00%) performance time: 1.05e-05 seconds\n",
      ">>> PI-tracking  (0.00%) performance time: 3.65e-05 seconds\n",
      ">>> Scheduler update  (0.00%) performance time: 2.12e-04 seconds\n",
      ">>> Validation time: 0:00:36.760276\n",
      "\n",
      "Max GPU memory allocated: 5.13038969039917 GB\n",
      "Max GPU memory cached: 7.91796875 GB\n",
      "Max CPU memory allocated: 4.278850555419922 GB\n",
      "None\n",
      "\n",
      "--------------------------------------------------\n",
      "Reload dataset without shuffling on train set, and remove data_augmentation\n",
      "----------------------------------------\n",
      "Loading the Complete Dataset for K-fold splitting\n",
      "Coverage Period: 28224 elts between 2017-05-01 00:00:00 and 2017-08-06 23:55:00\n",
      "Invalid dates within this fold: 0\n",
      "\n",
      ">>>Tackle Target dataset: PeMS07\n",
      "   Load data from: /home/rrochas/prediction-validation/../../../../data/rrochas/prediction_validation/PEMS07\n",
      "   Data loaded with shape: (28224, 883)\n",
      "   len(predicted_serie): 27936\n",
      "   nb of nan values:  0\n",
      "   split_train_valid: 16761\n",
      "   split_valid_test: 22348\n",
      "   Access predicted_serie.iat[16761]: 2017-06-29 04:45:00\n",
      "   Access predicted_serie.iat[22348]: 2017-07-18 14:20:00\n",
      "   first_train_date: 2017-05-02 00:00:00, last_train_date: 2017-06-29 04:45:00\n",
      "   first_valid_date: 2017-06-29 04:45:00, last_valid_date: 2017-07-18 14:20:00\n",
      "   first_test_date: 2017-07-18 14:20:00, last_test_date: 2017-08-06 23:55:00\n",
      "   Init Dataset: 'torch.Size([28224, 883]). 0 Nan values\n",
      "   TRAIN contextual_ds: torch.Size([16761, 883, 7])\n",
      "   VALID contextual_ds: torch.Size([5587, 883, 7])\n",
      "   TEST contextual_ds: torch.Size([5587, 883, 7])\n",
      "Init U/Utarget size: torch.Size([27936, 883, 7])/torch.Size([27936, 883, 1]) Train/Valid/Test 16761 5587 5587\n",
      "\n",
      "----------------------------------------\n",
      "Loading the dataset for fold n°0\n",
      "Coverage Period: 17751 elts between 2017-05-01 00:00:00 and 2017-07-01 15:10:00\n",
      "Invalid dates within this fold: 0\n",
      "\n",
      ">>>Tackle Target dataset: PeMS07\n",
      "   Load data from: /home/rrochas/prediction-validation/../../../../data/rrochas/prediction_validation/PEMS07\n",
      "   Data loaded with shape: (17751, 883)\n",
      "   len(predicted_serie): 17463\n",
      "   nb of nan values:  0\n",
      "   split_train_valid: 6289\n",
      "   split_valid_test: 11876\n",
      "   Access predicted_serie.iat[6289]: 2017-05-23 20:05:00\n",
      "   Access predicted_serie.iat[11876]: 2017-06-12 05:40:00\n",
      "   first_train_date: 2017-05-02 00:00:00, last_train_date: 2017-05-23 20:05:00\n",
      "   first_valid_date: 2017-05-23 20:05:00, last_valid_date: 2017-06-12 05:40:00\n",
      "   first_test_date: 2017-06-12 05:40:00, last_test_date: 2017-07-01 15:10:00\n",
      "Values with issues:  0.113%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "Values with issues:  0.113%\n",
      "Regular Values that we have to set to 0:  0.020%\n",
      "Values with issues:  0.113%\n",
      "Regular Values that we have to set to 0:  0.001%\n",
      "Values with issues:  0.113%\n",
      "Regular Values that we have to set to 0:  0.000%\n",
      "Values with issues:  0.113%\n",
      "Regular Values that we have to set to 0:  0.020%\n",
      "Values with issues:  0.113%\n",
      "Regular Values that we have to set to 0:  0.001%\n",
      "   Init Dataset: 'torch.Size([17751, 883]). 0 Nan values\n",
      "   TRAIN contextual_ds: torch.Size([6289, 883, 7])\n",
      "   VALID contextual_ds: torch.Size([5587, 883, 7])\n",
      "   TEST contextual_ds: torch.Size([5586, 883, 7])\n",
      "Init U/Utarget size: torch.Size([17463, 883, 7])/torch.Size([17463, 883, 1]) Train/Valid/Test 6289 5587 5586\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "313344",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/root/anaconda3/envs/pytorch-2.0.1/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 313344",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 108\u001b[0m\n\u001b[1;32m    105\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain_and_valid(normalizer \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mnormalizer, mod \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m,mod_plot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m     (trainer,ds,ds_no_shuffle,args) \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_init\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mstation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstation\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mtraining_mode_to_visualise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_mode_to_visualise\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/prediction-validation/examples/train_and_visu_non_recurrent.py:62\u001b[0m, in \u001b[0;36mevaluate_config\u001b[0;34m(args_init, modification, fold_to_evaluate, training_mode_to_visualise, station, transfer_modes, type_POIs, spatial_units, apps, POI_or_stations, expanded, individual_poi, sum_ts_pois)\u001b[0m\n\u001b[1;32m     58\u001b[0m trainer,ds_no_shuffle \u001b[38;5;241m=\u001b[39m get_ds_without_shuffling_on_train_set(trainer,modification,args_init,fold_to_evaluate)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m training_mode \u001b[38;5;129;01min\u001b[39;00m training_mode_to_visualise:\n\u001b[0;32m---> 62\u001b[0m     \u001b[43manalysis_on_specific_training_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mds_no_shuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtraining_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtransfer_modes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtransfer_modes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtype_POIs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtype_POIs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mspatial_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspatial_units\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mapps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mPOI_or_stations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPOI_or_stations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mexpanded\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpanded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mstation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mindividual_poi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mindividual_poi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43msum_ts_pois\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msum_ts_pois\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(trainer,ds,ds_no_shuffle,args)\n",
      "File \u001b[0;32m~/prediction-validation/examples/train_and_visu_non_recurrent.py:132\u001b[0m, in \u001b[0;36manalysis_on_specific_training_mode\u001b[0;34m(trainer, ds, training_mode, transfer_modes, type_POIs, spatial_units, apps, POI_or_stations, expanded, station, individual_poi, sum_ts_pois)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     netmob_consumption \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m \u001b[43mvisualisation_special_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstation\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkick_off_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43mRANGE\u001b[49m\u001b[43m,\u001b[49m\u001b[43mWIDTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43mHEIGHT\u001b[49m\u001b[43m,\u001b[49m\u001b[43mMIN_FLOW\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtraining_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnetmob_consumption\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnetmob_consumption\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/prediction-validation/examples/train_and_visu_non_recurrent.py:173\u001b[0m, in \u001b[0;36mvisualisation_special_event\u001b[0;34m(trainer, df_true, df_prediction, station, kick_off_time, Range, width, height, min_flow, training_mode, netmob_consumption)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualisation_special_event\u001b[39m(trainer,df_true,df_prediction,station,kick_off_time\u001b[38;5;241m=\u001b[39m[],Range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1200\u001b[39m,height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,min_flow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,training_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m,netmob_consumption\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    172\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m''' Specific interactiv visualisation for Prediction, True Value, Error and loss function '''\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     p1 \u001b[38;5;241m=\u001b[39m \u001b[43mplot_single_point_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_prediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstation\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtraining_mode\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m Trafic Volume Prediction at each subway station \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mkick_off_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkick_off_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRange\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbool_show\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     p2 \u001b[38;5;241m=\u001b[39m plot_TS(netmob_consumption,width\u001b[38;5;241m=\u001b[39mwidth,height\u001b[38;5;241m=\u001b[39mheight,bool_show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m netmob_consumption \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (df_prediction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(df_prediction)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[0;32m~/prediction-validation/plotting/TS_analysis.py:89\u001b[0m, in \u001b[0;36mplot_single_point_prediction\u001b[0;34m(df_true, df_prediction, station, title, kick_off_time, range, width, height, bool_show)\u001b[0m\n\u001b[1;32m     86\u001b[0m       station \u001b[38;5;241m=\u001b[39m [station]\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,station_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(station):\n\u001b[0;32m---> 89\u001b[0m        c \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mline(x\u001b[38;5;241m=\u001b[39mdf_true\u001b[38;5;241m.\u001b[39mindex, line_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.5\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[43mdf_true\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstation_i\u001b[49m\u001b[43m]\u001b[49m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m,color \u001b[38;5;241m=\u001b[39m Plasma256[\u001b[38;5;28mint\u001b[39m(k\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m255\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(station))])\n\u001b[1;32m     90\u001b[0m        legend_it\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstation_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, [c]))\n\u001b[1;32m     92\u001b[0m        \u001b[38;5;28;01mif\u001b[39;00m df_prediction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \n",
      "File \u001b[0;32m/root/anaconda3/envs/pytorch-2.0.1/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/root/anaconda3/envs/pytorch-2.0.1/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 313344"
     ]
    }
   ],
   "source": [
    "# GET PARAMETERS\n",
    "import os \n",
    "import sys\n",
    "import torch \n",
    "# Get Parent folder : \n",
    "\n",
    "current_path = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_path, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "from plotting.plotting import error_per_station_calendar_pattern  \n",
    "from examples.benchmark import local_get_args\n",
    "from examples.train_and_visu_non_recurrent import evaluate_config,train_the_config,get_ds\n",
    "from high_level_DL_method import load_optimizer_and_scheduler\n",
    "from dl_models.full_model import full_model\n",
    "from trainer import Trainer\n",
    "\n",
    "# Init:\n",
    "#['subway_indiv','tramway_indiv','bus_indiv','velov','criter']\n",
    "target_data = 'PeMS03' #'subway_in'  # PeMS03 # PeMS04 # PeMS07 # PeMS08 # METR_LA \n",
    "dataset_names = ['PeMS03'] #['PeMS03'] #['subway_in'] ['subway_in','subway_indiv'] #[\"subway_in\",\"subway_out\"] # ['subway_in','netmob_POIs_per_station'],[\"subway_in\",\"subway_out\"],[\"subway_in\",\"calendar\"] # [\"subway_in\"] # ['data_bidon'] # ['METR_LA'] # ['PEMS_BAY']\n",
    "dataset_for_coverage = ['PeMS03']#['subway_in','subway_indiv'] # ['subway_in','netmob_image_per_station'] #  ['data_bidon','netmob'] #  ['subway_in','netmob']  # ['METR_LA'] # ['PEMS_BAY']\n",
    "model_name = 'STGCN' # 'STGCN', 'ASTGCN'\n",
    "station = [313344,313349,313438,313450]# ['BEL','PAR','AMP','SAN','FLA']   # 'BON'  #'GER'\n",
    "# ...\n",
    "\n",
    "# Modif \n",
    "modification = {'target_data': target_data, \n",
    "                'freq': '5min', #'15min',\n",
    "                'use_target_as_context': False,\n",
    "                'data_augmentation': False,\n",
    "        \n",
    "                'epochs' : 1, #100\n",
    "\n",
    "                'lr': 0.00105, # 5e-5,# 4e-4,\n",
    "                'weight_decay': 0.0188896655584368, # 0.05,\n",
    "                'dropout': 0.271795372610271, # 0.15,\n",
    "                'Kt': 2,\n",
    "                'stblock_num': 3,\n",
    "                'gso_type': 'sym_renorm_adj',\n",
    "                'temporal_h_dim': 256,\n",
    "                'spatial_h_dim': 32,\n",
    "                'output_h_dim': 16,\n",
    "                'scheduler': True,  # None\n",
    "                'torch_scheduler_milestone': 28.0, #5,\n",
    "                'torch_scheduler_gamma': 0.9958348861339396, # 0.997,\n",
    "                'torch_scheduler_lr_start_factor': 0.8809942312067847, # 1,\n",
    "\n",
    "\n",
    "                #'set_spatial_units':  station,   \n",
    "                'adj_type':'corr',\n",
    "                'threshold': 0.7,\n",
    "                'learnable_adj_matrix' : False, # True                              # EXIST ONLY IF MODEL = STGCN\n",
    "                'graph_conv_type': 'graph_conv', # 'cheb_graph_conv', 'graph_conv'  # EXIST ONLY IF MODEL = STGCN\n",
    "                'learnable_adj_top_k': 10,                                          # EXIST ONLY IF MODEL = STGCN\n",
    "                'learnable_adj_embd_dim': 16,                                       # EXIST ONLY IF MODEL = STGCN  \n",
    "\n",
    "                'stacked_contextual': True, # True # False\n",
    "                'temporal_graph_transformer_encoder': False,\n",
    "                'compute_node_attr_with_attn' : False, # True ??\n",
    "\n",
    "                #\n",
    "                #'graph_conv_type': 'graph_conv', # 'cheb_graph_conv', 'graph_conv'\n",
    "                #'learnable_adj_top_k': 10,\n",
    "                #'learnable_adj_embd_dim': 16, \n",
    "                ### ========\n",
    "\n",
    "                ### Temporal Graph Transfermer Encoder parametrs:\n",
    "                #'TGE_num_layers' : 4, #2\n",
    "                #'TGE_num_heads' :  1, #IMPOSSIBLE > 1 CAR DOIT DIVISER L = 7\n",
    "                #'TGE_FC_hdim' :  32, #32\n",
    "\n",
    "                ### Netmob Parametrs: \n",
    "                #'NetMob_only_epsilon': True,    # True # False\n",
    "                #'NetMob_selected_apps': ['Apple_iMessage','Web_Ads'],# ['Apple_iMessage','Web_Ads'], #,'Deezer','WhatsApp','Twitter'] #['Google_Maps']# ['Instagram','Google_Maps','Twitter']\n",
    "                #'NetMob_transfer_mode' :  ['DL'], #,'UL'] # ['DL'] # ['UL'] #['DL','UL']\n",
    "                #'NetMob_selected_tags' : ['station_epsilon100'],#['iris','stadium','station','university']#['park','stadium','university','station','shop','nightclub','parkings','theatre','iris','transit','public_transport']\n",
    "                #'NetMob_expanded' : '', # '' # '_expanded\n",
    "\n",
    "                ### Compute node with attention parameters: \n",
    "                #'vision_num_heads':6,\n",
    "                #\"vision_grn_out_dim\":48,\n",
    "                #'vision_model_name': 'VariableSelectionNetwork',\n",
    "                #'vision_concatenation_early':True,   \n",
    "                #'vision_concatenation_late':True,\n",
    "                           }\n",
    "# ...\n",
    "#1038945\n",
    "\n",
    "# Training and visu: \n",
    "args_init = local_get_args(model_name,\n",
    "                    args_init = None,\n",
    "                    dataset_names=dataset_names,\n",
    "                    dataset_for_coverage=dataset_for_coverage,\n",
    "                    modification = modification)\n",
    "\n",
    "training_mode_to_visualise = ['test'] # ['test','valid','train']\n",
    "\n",
    "if False:\n",
    "    ds,args,trial_id,save_folder,df_loss = get_ds(modification=modification,args_init=args_init)\n",
    "    model = full_model(ds, args).to(args.device)\n",
    "    optimizer,scheduler,loss_function = load_optimizer_and_scheduler(model,args)\n",
    "    trainer = Trainer(ds,model,args,optimizer,loss_function,scheduler = scheduler,show_figure = False,trial_id = trial_id, fold=0,save_folder = save_folder)\n",
    "    trainer.train_and_valid(normalizer = ds.normalizer, mod = 1000,mod_plot = None) \n",
    "\n",
    "else:\n",
    "    (trainer,ds,ds_no_shuffle,args) = evaluate_config(args_init = args_init,\n",
    "                                                    station=station,modification=modification,\n",
    "                                                    training_mode_to_visualise=training_mode_to_visualise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "DATE_COL = 'VAL_DATE'\n",
    "LOCATION_COL = 'VAL_ARRET_CODE'\n",
    "VALUE_COL = 'Flow'\n",
    "\n",
    "START = '2019-11-01' # Exemple basé sur head()\n",
    "END = '2020-04-30 23:30:00'\n",
    "MIN_AVG_DAILY_PASSENGER = 24*10\n",
    "\n",
    "dirname = '/home/rrochas/prediction-validation/../../../../data/rrochas/prediction_validation/agg_data/validation_individuelle/tramway_indiv_15min/tramway_indiv_15min.csv'\n",
    "df = pd.read_csv(dirname,index_col=0)\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "df['station_lane_sens'] = df['VAL_ARRET_CODE'].astype(str) + '_' + df['LIG_NUMERO_SAE'].astype(str) + '_' + df['CRS_SENS_TRAJET'].astype(str)\n",
    "df = df.pivot_table(index=DATE_COL,columns='station_lane_sens',values=VALUE_COL)\n",
    "reindex = pd.date_range(start=START, end=END, freq='15min')[:-1]\n",
    "df = df.reindex(reindex).fillna(0)\n",
    "\n",
    "# Filter useless stations: \n",
    "mask_init = df.resample('1D').sum().mean()>MIN_AVG_DAILY_PASSENGER\n",
    "mask = mask_init[mask_init].index.to_list()\n",
    "df = df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>> Load best CONFIG\n",
      "----------------------------------------\n",
      "Loading the Complete Dataset for K-fold splitting\n",
      "Coverage Period: 7392 elts between 2019-03-16 00:00:00 and 2019-05-31 23:45:00\n",
      "Invalid dates within this fold: 776\n",
      "\n",
      ">>>Tackle Target dataset: subway_in\n",
      "   Load data from: /home/rrochas/prediction-validation/../../../../data/rrochas/prediction_validation/subway_in/subway_in.csv\n",
      "   Init Dataset: 'torch.Size([7392, 40]). 0 Nan values\n",
      "   TRAIN contextual_ds: torch.Size([2821, 40, 7])\n",
      "   VALID contextual_ds: torch.Size([940, 40, 7])\n",
      "   TEST contextual_ds: torch.Size([940, 40, 7])\n",
      "\n",
      ">>>Tackle Contextual dataset:  subway_out\n",
      "   Load data from: /home/rrochas/prediction-validation/../../../../data/rrochas/prediction_validation/subway_out/subway_out.csv\n",
      "   Init Dataset: '[torch.Size([7392, 40])]. [tensor(0)] Nan values\n",
      "   TRAIN contextual_ds: [torch.Size([2821, 40, 7])]\n",
      "   VALID contextual_ds: [torch.Size([940, 40, 7])]\n",
      "   TEST contextual_ds: [torch.Size([940, 40, 7])]\n",
      "Init U/Utarget size: torch.Size([4702, 40, 7])/torch.Size([4702, 40, 1]) Train/Valid/Test 2821 940 940\n",
      "\n",
      " ===== ERROR WITH prefetch_factor====  \n",
      "ValueError: prefetch_factor option could only be specified in multiprocessing.let num_workers > 0 to enable multiprocessing\n",
      "\n",
      "----------------------------------------\n",
      "Loading the dataset for fold n°1\n",
      "Coverage Period: 5069 elts between 2019-03-16 00:00:00 and 2019-05-07 19:00:00\n",
      "Invalid dates within this fold: 481\n",
      "\n",
      ">>>Tackle Target dataset: subway_in\n",
      "   Load data from: /home/rrochas/prediction-validation/../../../../data/rrochas/prediction_validation/subway_in/subway_in.csv\n",
      "   Number of Considered Spatial-Unit:  40\n",
      "   Init Dataset: 'torch.Size([5069, 40]). 0 Nan values\n",
      "   TRAIN contextual_ds: torch.Size([1412, 40, 7])\n",
      "   VALID contextual_ds: torch.Size([940, 40, 7])\n",
      "   TEST contextual_ds: torch.Size([939, 40, 7])\n",
      "\n",
      ">>>Tackle Contextual dataset:  subway_out\n",
      "   Load data from: /home/rrochas/prediction-validation/../../../../data/rrochas/prediction_validation/subway_out/subway_out.csv\n",
      "   Number of Considered Spatial-Unit:  40\n",
      "   Init Dataset: '[torch.Size([5069, 40])]. [tensor(0)] Nan values\n",
      "   TRAIN contextual_ds: [torch.Size([1412, 40, 7])]\n",
      "   VALID contextual_ds: [torch.Size([940, 40, 7])]\n",
      "   TEST contextual_ds: [torch.Size([939, 40, 7])]\n",
      "Init U/Utarget size: torch.Size([3292, 40, 7])/torch.Size([3292, 40, 1]) Train/Valid/Test 1412 940 939\n",
      "\n",
      " ===== ERROR WITH prefetch_factor====  \n",
      "ValueError: prefetch_factor option could only be specified in multiprocessing.let num_workers > 0 to enable multiprocessing\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain_and_valid(normalizer \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mnormalizer, mod \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m,mod_plot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     (trainer,ds,ds_no_shuffle,args) \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_init\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mstation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstation\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mtraining_mode_to_visualise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_mode_to_visualise\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/prediction-validation/examples/train_and_visu_non_recurrent.py:54\u001b[0m, in \u001b[0;36mevaluate_config\u001b[0;34m(args_init, modification, fold_to_evaluate, training_mode_to_visualise, station, transfer_modes, type_POIs, spatial_units, apps, POI_or_stations, expanded, individual_poi, sum_ts_pois)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_config\u001b[39m(args_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m                     modification \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m     32\u001b[0m                     fold_to_evaluate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m                     sum_ts_pois \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     43\u001b[0m                     ):\n\u001b[1;32m     45\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    args: \u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    type_POIs : list of type of POIs.                         >>> ['stadium','nightclub']\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    expanded: '' if we look at the intensity of netmob consumption at the POI. '_expanded' if we look also one square around.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     trainer,ds,args,trial_id,df_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_the_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodification\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfold_to_evaluate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReload dataset without shuffling on train set, and remove data_augmentation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/prediction-validation/examples/train_and_visu_non_recurrent.py:76\u001b[0m, in \u001b[0;36mtrain_the_config\u001b[0;34m(args_init, modification, fold_to_evaluate)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_the_config\u001b[39m(args_init,modification,fold_to_evaluate):\n\u001b[0;32m---> 76\u001b[0m     ds,args,trial_id,save_folder,df_loss \u001b[38;5;241m=\u001b[39m \u001b[43mget_ds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodification\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfold_to_evaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_to_evaluate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     trainer,df_loss \u001b[38;5;241m=\u001b[39m train_on_ds(ds,args,trial_id,save_folder,df_loss)\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer,ds,args,trial_id,df_loss\n",
      "File \u001b[0;32m~/prediction-validation/examples/train_and_visu_non_recurrent.py:209\u001b[0m, in \u001b[0;36mget_ds\u001b[0;34m(model_name, dataset_names, dataset_for_coverage, modification, args_init, fold_to_evaluate)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_ds\u001b[39m(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,dataset_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,dataset_for_coverage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    199\u001b[0m            modification \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m    200\u001b[0m            args_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m    201\u001b[0m            fold_to_evaluate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    202\u001b[0m             ):\n\u001b[1;32m    203\u001b[0m     args_with_contextual,K_subway_ds \u001b[38;5;241m=\u001b[39m get_multi_ds(model_name \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m args_init\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[1;32m    204\u001b[0m                                                     dataset_names \u001b[38;5;28;01mif\u001b[39;00m dataset_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m args_init\u001b[38;5;241m.\u001b[39mdataset_names,\n\u001b[1;32m    205\u001b[0m                                                     dataset_for_coverage \u001b[38;5;28;01mif\u001b[39;00m dataset_for_coverage \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m args_init\u001b[38;5;241m.\u001b[39mdataset_for_coverage,\n\u001b[1;32m    206\u001b[0m                                                     modification,\n\u001b[1;32m    207\u001b[0m                                                     args_init,\n\u001b[1;32m    208\u001b[0m                                                     fold_to_evaluate)\n\u001b[0;32m--> 209\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mK_subway_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    210\u001b[0m     trial_id \u001b[38;5;241m=\u001b[39m get_trial_id(args_with_contextual)\n\u001b[1;32m    211\u001b[0m     save_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# GET PARAMETERS\n",
    "import os \n",
    "import sys\n",
    "import torch \n",
    "# Get Parent folder : \n",
    "\n",
    "current_path = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_path, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "from plotting.plotting import error_per_station_calendar_pattern  \n",
    "from examples.benchmark import local_get_args\n",
    "from examples.train_and_visu_non_recurrent import evaluate_config,train_the_config,get_ds\n",
    "from high_level_DL_method import load_optimizer_and_scheduler\n",
    "from dl_models.full_model import full_model\n",
    "from trainer import Trainer\n",
    "\n",
    "from examples.train_model_on_k_fold_validation import train_model_on_k_fold_validation,load_configuration,train_valid_1_model\n",
    "import torch\n",
    "save_folder = 'K_fold_validation/training_with_HP_tuning/subway_in_subway_out'\n",
    "trial_id = 'subway_in_subway_out_STGCN_MSELoss_2025_02_19_00_05_19271'\n",
    "args_init,folds = load_configuration(trial_id,True)\n",
    "\n",
    "training_mode_to_visualise = ['test'] # ['test','valid','train']\n",
    "station = ['BEL','PAR','AMP','SAN','FLA']   # 'BON'  #'GER'\n",
    "\n",
    "modification ={'keep_best_weights':True,\n",
    "                'epochs':100,\n",
    "                'device':torch.device(\"cuda:0\"),\n",
    "                'target_data':'subway_in',\n",
    "                \n",
    "                'use_target_as_context': False,\n",
    "                'freq':'15min',\n",
    "                'minmaxnorm':True,\n",
    "                'standardize': False,\n",
    "                'learnable_adj_matrix' : False,\n",
    "                'data_augmentation': False,\n",
    "                }\n",
    "\n",
    "config_diffs = {}\n",
    "config_diffs.update({'subway_in_subway_out':{'dataset_names':['subway_in','subway_out'],\n",
    "                                        'data_augmentation': True,\n",
    "                                        'DA_method': 'rich_interpolation',\n",
    "                                        'stacked_contextual': True,\n",
    "                                        'temporal_graph_transformer_encoder': False,\n",
    "                                        'compute_node_attr_with_attn': False,\n",
    "                                        }\n",
    "                            })\n",
    "for add_name_id,config_diff in config_diffs.items():\n",
    "    config_diff.update(modification)\n",
    "\n",
    "if False: \n",
    "    ds,args_ds,trial_id,save_folder,df_loss = get_ds(modification=modification,args_init=args_init)\n",
    "    model = full_model(ds, args_ds).to(args_ds.device)\n",
    "    optimizer,scheduler,loss_function = load_optimizer_and_scheduler(model,args_ds)\n",
    "    trainer = Trainer(ds,model,args_ds,optimizer,loss_function,scheduler = scheduler,show_figure = False,trial_id = trial_id, fold=0,save_folder = save_folder)\n",
    "    trainer.train_and_valid(normalizer = ds.normalizer, mod = 1000,mod_plot = None) \n",
    "else:\n",
    "    (trainer,ds,ds_no_shuffle,args) = evaluate_config(args_init = args_init,\n",
    "                                                    station=station,modification=modification,\n",
    "                                                    training_mode_to_visualise=training_mode_to_visualise)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'STGCN',\n",
       " 'dataset_names': ['subway_in', 'subway_out'],\n",
       " 'dataset_for_coverage': ['subway_in', 'netmob_POIs'],\n",
       " 'device': device(type='cuda', index=0),\n",
       " 'optimizer': 'adamw',\n",
       " 'single_station': False,\n",
       " 'loss_function_type': 'MSE',\n",
       " 'epsilon_clustering': 0.05,\n",
       " 'freq': '15min',\n",
       " 'contextual_positions': {'calendar': [0, 1], 'subway_out': 2},\n",
       " 'quick_vision': False,\n",
       " 'netmob_transfer_mode': 'DL',\n",
       " 'evaluate_complete_ds': True,\n",
       " 'train_valid_test_split_method': 'similar_length_method',\n",
       " 'set_spatial_units': None,\n",
       " 'hp_tuning_on_first_fold': True,\n",
       " 'keep_best_weights': True,\n",
       " 'num_workers': 0,\n",
       " 'persistent_workers': False,\n",
       " 'pin_memory': True,\n",
       " 'prefetch_factor': 2,\n",
       " 'drop_last': False,\n",
       " 'mixed_precision': False,\n",
       " 'non_blocking': True,\n",
       " 'torch_compile': False,\n",
       " 'backend': 'inductor',\n",
       " 'prefetch_all': False,\n",
       " 'NetMob_selected_apps': ['Google_Maps', 'Deezer', 'Instagram'],\n",
       " 'NetMob_transfer_mode': ['DL'],\n",
       " 'NetMob_selected_tags': ['iris'],\n",
       " 'NetMob_expanded': '',\n",
       " 'ray': False,\n",
       " 'ray_scheduler': 'ASHA',\n",
       " 'ray_search_alg': None,\n",
       " 'grace_period': 20,\n",
       " 'HP_max_epochs': 100,\n",
       " 'alpha': None,\n",
       " 'conformity_scores_type': 'max_residual',\n",
       " 'quantile_method': 'compute_quantile_by_class',\n",
       " 'calibration_calendar_class': 0,\n",
       " 'type_calib': 'classic',\n",
       " 'data_augmentation': True,\n",
       " 'DA_moment_to_focus': None,\n",
       " 'DA_min_count': 5,\n",
       " 'DA_method': 'rich_interpolation',\n",
       " 'DA_alpha': 1,\n",
       " 'DA_prop': 1,\n",
       " 'DA_noise_from': 'MSTL',\n",
       " 'H': 6,\n",
       " 'W': 0,\n",
       " 'D': 1,\n",
       " 'step_ahead': 1,\n",
       " 'L': 7,\n",
       " 'shuffle': True,\n",
       " 'train_prop': 0.6,\n",
       " 'calib_prop': None,\n",
       " 'valid_prop': 0.2,\n",
       " 'test_prop': 0.19999999999999996,\n",
       " 'track_pi': False,\n",
       " 'validation_split_method': 'forward_chaining_cv',\n",
       " 'min_fold_size_proportion': 0.75,\n",
       " 'no_common_dates_between_set': False,\n",
       " 'K_fold': 6,\n",
       " 'current_fold': 0,\n",
       " 'abs_path': '/home/rrochas/prediction-validation/',\n",
       " 'out_dim': 1,\n",
       " 'vision_model_name': None,\n",
       " 'vision_input_type': 'POIs',\n",
       " 'stacked_contextual': True,\n",
       " 'temporal_graph_transformer_encoder': False,\n",
       " 'compute_node_attr_with_attn': False,\n",
       " 'Kt': 2,\n",
       " 'stblock_num': 3,\n",
       " 'Ks': 2,\n",
       " 'graph_conv_type': 'graph_conv',\n",
       " 'gso_type': 'sym_renorm_adj',\n",
       " 'enable_bias': True,\n",
       " 'adj_type': 'corr',\n",
       " 'enable_padding': True,\n",
       " 'threshold': 0.3,\n",
       " 'act_func': 'glu',\n",
       " 'temporal_h_dim': 256,\n",
       " 'spatial_h_dim': 32,\n",
       " 'output_h_dim': 16,\n",
       " 'TGE_num_layers': 2,\n",
       " 'TGE_num_heads': 2,\n",
       " 'TGE_FC_hdim': 32,\n",
       " 'blocks': [[2], [32, 32, 32], [32, 32, 32], [64]],\n",
       " 'weight_decay': 0.0188896655584368,\n",
       " 'batch_size': 32,\n",
       " 'lr': 0.00105,\n",
       " 'dropout': 0.271795372610271,\n",
       " 'epochs': 100,\n",
       " 'scheduler': True,\n",
       " 'n_vertex': 40,\n",
       " 'C': 2,\n",
       " 'args_embedding': Namespace(),\n",
       " 'args_vision': Namespace(),\n",
       " 'n_units_subway_out': 40,\n",
       " 'input_dim_subway_out': 7,\n",
       " 'ds_which_need_spatial_attn': [],\n",
       " 'pos_node_attributes': [2],\n",
       " 'dict_node_attr2dataset': {2: 'subway_out'},\n",
       " 'node_attr_which_need_attn': [],\n",
       " 'torch_scheduler_milestone': 28.0,\n",
       " 'torch_scheduler_gamma': 0.9958348861339396,\n",
       " 'torch_scheduler_lr_start_factor': 0.8809942312067847,\n",
       " 'target_data': 'subway_in',\n",
       " 'use_target_as_context': False,\n",
       " 'minmaxnorm': True,\n",
       " 'standardize': False,\n",
       " 'learnable_adj_matrix': False,\n",
       " 'contextual_dataset_names': ['subway_out']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(args_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer,scheduler,loss_function = load_optimizer_and_scheduler(model,args_ds)\n",
    "trainer = Trainer(ds,model,args,optimizer,loss_function,scheduler = scheduler,show_figure = False,trial_id = trial_id, fold=0,save_folder = save_folder)\n",
    "trainer.train_and_valid(normalizer = ds.normalizer, mod = 1000,mod_plot = None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'blocks': [[1], [32, 32, 32], [32, 32, 32], [64]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'STGCN',\n",
       " 'dataset_names': ['subway_in', 'subway_out'],\n",
       " 'dataset_for_coverage': ['subway_in', 'netmob_POIs'],\n",
       " 'device': device(type='cuda'),\n",
       " 'optimizer': 'adamw',\n",
       " 'single_station': False,\n",
       " 'loss_function_type': 'MSE',\n",
       " 'epsilon_clustering': 0.05,\n",
       " 'freq': '15min',\n",
       " 'contextual_positions': {'subway_out': 2},\n",
       " 'quick_vision': False,\n",
       " 'netmob_transfer_mode': 'DL',\n",
       " 'evaluate_complete_ds': True,\n",
       " 'train_valid_test_split_method': 'similar_length_method',\n",
       " 'set_spatial_units': None,\n",
       " 'hp_tuning_on_first_fold': True,\n",
       " 'keep_best_weights': False,\n",
       " 'num_workers': 0,\n",
       " 'persistent_workers': False,\n",
       " 'pin_memory': True,\n",
       " 'prefetch_factor': 2,\n",
       " 'drop_last': False,\n",
       " 'mixed_precision': False,\n",
       " 'non_blocking': True,\n",
       " 'torch_compile': False,\n",
       " 'backend': 'inductor',\n",
       " 'prefetch_all': False,\n",
       " 'NetMob_selected_apps': ['Google_Maps', 'Deezer', 'Instagram'],\n",
       " 'NetMob_transfer_mode': ['DL'],\n",
       " 'NetMob_selected_tags': ['iris'],\n",
       " 'NetMob_expanded': '',\n",
       " 'ray': False,\n",
       " 'ray_scheduler': 'ASHA',\n",
       " 'ray_search_alg': None,\n",
       " 'grace_period': 20,\n",
       " 'HP_max_epochs': 100,\n",
       " 'alpha': None,\n",
       " 'conformity_scores_type': 'max_residual',\n",
       " 'quantile_method': 'compute_quantile_by_class',\n",
       " 'calibration_calendar_class': 0,\n",
       " 'type_calib': 'classic',\n",
       " 'data_augmentation': True,\n",
       " 'DA_moment_to_focus': None,\n",
       " 'DA_min_count': 5,\n",
       " 'DA_method': 'rich_interpolation',\n",
       " 'DA_alpha': 1,\n",
       " 'DA_prop': 1,\n",
       " 'DA_noise_from': 'MSTL',\n",
       " 'H': 6,\n",
       " 'W': 0,\n",
       " 'D': 1,\n",
       " 'step_ahead': 1,\n",
       " 'L': 7,\n",
       " 'shuffle': True,\n",
       " 'train_prop': 0.6,\n",
       " 'calib_prop': None,\n",
       " 'valid_prop': 0.2,\n",
       " 'test_prop': 0.19999999999999996,\n",
       " 'track_pi': False,\n",
       " 'validation_split_method': 'forward_chaining_cv',\n",
       " 'min_fold_size_proportion': 0.75,\n",
       " 'no_common_dates_between_set': False,\n",
       " 'K_fold': 6,\n",
       " 'current_fold': 0,\n",
       " 'abs_path': '/home/rrochas/prediction-validation/',\n",
       " 'out_dim': 1,\n",
       " 'vision_model_name': None,\n",
       " 'vision_input_type': 'POIs',\n",
       " 'stacked_contextual': True,\n",
       " 'temporal_graph_transformer_encoder': False,\n",
       " 'compute_node_attr_with_attn': False,\n",
       " 'Kt': 2,\n",
       " 'stblock_num': 3,\n",
       " 'Ks': 2,\n",
       " 'graph_conv_type': 'graph_conv',\n",
       " 'gso_type': 'sym_renorm_adj',\n",
       " 'enable_bias': True,\n",
       " 'adj_type': 'corr',\n",
       " 'enable_padding': True,\n",
       " 'threshold': 0.3,\n",
       " 'act_func': 'glu',\n",
       " 'temporal_h_dim': 256,\n",
       " 'spatial_h_dim': 32,\n",
       " 'output_h_dim': 16,\n",
       " 'TGE_num_layers': 2,\n",
       " 'TGE_num_heads': 2,\n",
       " 'TGE_FC_hdim': 32,\n",
       " 'blocks': [[1], [32, 32, 32], [32, 32, 32], [64]],\n",
       " 'weight_decay': 0.0188896655584368,\n",
       " 'batch_size': 32,\n",
       " 'lr': 0.00105,\n",
       " 'dropout': 0.271795372610271,\n",
       " 'epochs': 100,\n",
       " 'scheduler': True,\n",
       " 'n_vertex': 40,\n",
       " 'C': 2,\n",
       " 'args_embedding': Namespace(),\n",
       " 'args_vision': Namespace(),\n",
       " 'n_units_subway_out': 40,\n",
       " 'input_dim_subway_out': 7,\n",
       " 'ds_which_need_spatial_attn': [],\n",
       " 'pos_node_attributes': [2],\n",
       " 'dict_node_attr2dataset': {2: 'subway_out'},\n",
       " 'node_attr_which_need_attn': [],\n",
       " 'torch_scheduler_milestone': 28.0,\n",
       " 'torch_scheduler_gamma': 0.9958348861339396,\n",
       " 'torch_scheduler_lr_start_factor': 0.8809942312067847}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['METR_LA',\n",
       " 'PEMS_BAY',\n",
       " 'PEMS_d3',\n",
       " 'PEMS_d4',\n",
       " 'PEMS_d7',\n",
       " 'data_bidon',\n",
       " 'generate_data.ipynb',\n",
       " 'netmob_bidon',\n",
       " 'ref_subway.csv',\n",
       " 'subway_in',\n",
       " 'netmob_video_lyon',\n",
       " 'netmob_image_per_station',\n",
       " 'NetMob_lyon.geojson',\n",
       " 'lyon_iris_shapefile',\n",
       " 'POIs',\n",
       " 'subway_out',\n",
       " 'CRITER_3lanes',\n",
       " 'agg_data']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calendar_dayofweek :  torch.Size([971, 7])\n",
      "calendar_hour :  torch.Size([971, 18])\n",
      "subway_indiv :  torch.Size([834, 40, 7])\n"
     ]
    }
   ],
   "source": [
    "contextual_train = {name_i: contextual_i['train'] for name_i,contextual_i in ds.contextual_tensors.items()}  \n",
    "contextual_valid ={name_i: contextual_i['valid'] for name_i,contextual_i in ds.contextual_tensors.items() if 'valid' in contextual_i.keys()}  \n",
    "contextual_test  = {name_i: contextual_i['test'] for name_i,contextual_i in ds.contextual_tensors.items() if 'test' in contextual_i.keys()}  \n",
    "\n",
    "for name_i in contextual_train.keys():\n",
    "    print(name_i,': ',contextual_train[name_i].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontextual_positions: \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mcontextual_positions)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m contextual_i \u001b[38;5;129;01min\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mcontextual_tensors\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "print('contextual_positions: ',ds.contextual_positions)\n",
    "for contextual_i in ds.contextual_tensors.keys():\n",
    "    print()\n",
    "    for training_mode in ['train','valid','test']:\n",
    "        print(f\"contextual_tensors[{contextual_i}][{training_mode}]: \",ds.contextual_tensors[contextual_i][training_mode].size())\n",
    "\n",
    "print('\\nFirst/Last predicted train: ',ds.tensor_limits_keeper.first_predicted_train_date,ds.tensor_limits_keeper.last_predicted_train_date,'\\n',\n",
    "'First/Last predicted valid: ',ds.tensor_limits_keeper.first_predicted_valid_date,ds.tensor_limits_keeper.last_predicted_valid_date,'\\n',\n",
    "'First/Last predicted test: ',ds.tensor_limits_keeper.first_predicted_test_date,ds.tensor_limits_keeper.last_predicted_test_date\n",
    ")\n",
    "print('\\ndf dates: ')\n",
    "display(ds.tensor_limits_keeper.df_dates.sort_values(by='date'))\n",
    "print('\\ndf verif: ')\n",
    "ds.tensor_limits_keeper.df_verif\n",
    "\n",
    "output = next(iter(ds.dataloader['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2020-02-01 00:00:00', '2020-02-01 00:15:00',\n",
       "               '2020-02-01 00:30:00', '2020-02-01 00:45:00',\n",
       "               '2020-02-01 01:00:00', '2020-02-01 01:15:00',\n",
       "               '2020-02-01 01:30:00', '2020-02-01 01:45:00',\n",
       "               '2020-02-01 02:00:00', '2020-02-01 02:15:00',\n",
       "               ...\n",
       "               '2020-02-27 21:30:00', '2020-02-27 21:45:00',\n",
       "               '2020-02-27 22:00:00', '2020-02-27 22:15:00',\n",
       "               '2020-02-27 22:30:00', '2020-02-27 22:45:00',\n",
       "               '2020-02-27 23:00:00', '2020-02-27 23:15:00',\n",
       "               '2020-02-27 23:30:00', '2020-02-27 23:45:00'],\n",
       "              dtype='datetime64[ns]', name='VAL_DATE', length=2262, freq=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dirname = '/home/rrochas/prediction-validation/../../../../data/rrochas/prediction_validation/agg_data/validation_individuelle/subway_indiv_15min/'\n",
    "os.listdir(dirname)\n",
    "os.path.exists(f\"{dirname}/subway_indiv_15min.csv\")\n",
    "\n",
    "FILE_BASE_NAME = 'subway_indiv'\n",
    "DATA_SUBFOLDER = 'agg_data/validation_individuelle' # Sous-dossier dans FOLDER_PATH\n",
    "NATIVE_FREQ = '3min'\n",
    "START = '2019-10-01' # Exemple basé sur head()\n",
    "END = '2020-04-01'\n",
    "list_of_invalid_period = []\n",
    "C = 1\n",
    "\n",
    "DATE_COL = 'VAL_DATE'\n",
    "LOCATION_COL = 'COD_TRG'\n",
    "VALUE_COL = 'Flow'\n",
    "\n",
    "target_freq = '15min'\n",
    "file_name = f\"{FILE_BASE_NAME}_{target_freq}\"\n",
    "data_file_path = f\"{dirname}/{file_name}.csv\"\n",
    "\n",
    "df = pd.read_csv(data_file_path)\n",
    "df['VAL_DATE'] = pd.to_datetime(df['VAL_DATE'])\n",
    "df=df.set_index('VAL_DATE')\n",
    "df.index.unique().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start training\n",
      "\n",
      "Training Throughput:1063.21 sequences per seconds\n",
      ">>> Training complete in: 0:04:49.253988\n",
      ">>> Training performance time: min 0.016025543212890625 avg 0.028232812881469727 seconds (+/- 0.007409689556679389)\n",
      ">>> Loading performance time: min 0.00011897087097167969 avg 0.021025142456797832 seconds (+/- 0.0533871588248097)\n",
      ">>> Forward performance time: 0.014497923700226319 seconds (+/- 0.006785826679855182)\n",
      ">>> Backward performance time: 0.01471411531366729 seconds (+/- 0.0026613273244625673)\n",
      ">>> Plotting performance time: 2.2859429594260365e-06 seconds (+/- 7.815177432756738e-07)\n",
      ">>> Saving performance time: 0.6629389921824137 seconds (+/- 0.29361714449532417)\n",
      ">>> PI-tracking performance time: 5.977237643908016e-06 seconds (+/- 7.975377572890917e-06)\n",
      ">>> Scheduler-update performance time: 4.9972054946362674e-06 seconds (+/- 9.795780319996475e-06)\n",
      ">>> Validation time: 0:00:00.356755\n",
      "Proportion of time consumed for Loading: 41.6%\n",
      "Proportion of time consumed for Forward: 28.3%\n",
      "Proportion of time consumed for Backward: 28.7%\n",
      "Proportion of time consumed for Plotting: 0.0%\n",
      "Proportion of time consumed for CheckPoint Saving: 1.4%\n",
      "Proportion of time consumed for Tracking PI: 0.0%\n",
      "Proportion of time consumed for Update Scheduler: 0.0%\n",
      "Proportion of time consumed for Read all data on GPU: 0.0%\n",
      "\n",
      "Max GPU memory allocated: 0.18022966384887695 GB\n",
      "Max GPU memory cached: 0.2890625 GB\n",
      "Max CPU memory allocated: 3.2993011474609375 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "trainer.train_and_valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import eigs\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "L = (pd.DataFrame(np.random.rand(10,10))*10)\n",
    "\n",
    "\n",
    "lambda_max = eigs(L.values , k=1, which='LR')[0].real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'match_period_coverage_with_netmob' from 'utils.utilities_DL' (/home/rrochas/prediction-validation/utils/utilities_DL.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, working_dir)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Personnal import \u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities_DL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m match_period_coverage_with_netmob\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_args,update_modif\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpaths\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FOLDER_PATH,FILE_NAME,SAVE_DIRECTORY\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'match_period_coverage_with_netmob' from 'utils.utilities_DL' (/home/rrochas/prediction-validation/utils/utilities_DL.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Obtenir le chemin du dossier parent\n",
    "current_path = notebook_dir = os.getcwd()\n",
    "# current_path = os.path.dirname()\n",
    "working_dir = os.path.abspath(os.path.join(current_path, '..'))\n",
    "\n",
    "# Ajouter le dossier parent au chemin de recherche des modules\n",
    "if working_dir not in sys.path:\n",
    "    sys.path.insert(0, working_dir)\n",
    "\n",
    "# Personnal import \n",
    "from utils.utilities_DL import match_period_coverage_with_netmob\n",
    "from constants.config import get_args,update_modif\n",
    "from constants.paths import FOLDER_PATH,FILE_NAME,SAVE_DIRECTORY\n",
    "from K_fold_validation.K_fold_validation import KFoldSplitter\n",
    "from trainer import Trainer\n",
    "from high_level_DL_method import load_optimizer_and_scheduler\n",
    "from dl_models.full_model import full_model\n",
    "\n",
    "from plotting.plotting_bokeh import plot_bokeh\n",
    "\n",
    "\n",
    "# Load config\n",
    "model_name = 'STGCN' #'CNN'\n",
    "dataset_names = ['subway_in','netmob']\n",
    "args = get_args(model_name,dataset_names)\n",
    "\n",
    "# Modification : \n",
    "args.K_fold = 5\n",
    "\n",
    "args.ray = False\n",
    "args.W = 0  # IMPORTANT AVEC NETMOB\n",
    "\n",
    "args.epochs = 100\n",
    "args.loss_function_type = 'MSE' # 'quantile'\n",
    "\n",
    "# optimization:\n",
    "args.mixed_precision = True\n",
    "\n",
    "args = update_modif(args)\n",
    "\n",
    "# Coverage Period : \n",
    "small_ds = False\n",
    "coverage = match_period_coverage_with_netmob(FILE_NAME,dataset_names=['subway_in','netmob'])\n",
    "\n",
    "# Choose DataSet and VisionModel if needed: \n",
    "dataset_names = ['netmob','subway_in'] # ['calendar','netmob'] #['subway_in','netmob','calendar']\n",
    "vision_model_name = 'FeatureExtractor_ResNetInspired'  # 'ImageAvgPooling'  #'FeatureExtractor_ResNetInspired' #'MinimalFeatureExtractor',\n",
    "\n",
    "# Train and Evaluate Model: \n",
    "mod_plot = 1 # bokeh plotting every epoch \n",
    "\n",
    "# Load K-fold subway-ds \n",
    "folds = [0] # Here we use the first fold for HP-tuning. \n",
    "\n",
    "# In case we need to compute the Sliding K-fold validation:\n",
    "# folds = np.arange(1,args.K_fold)\n",
    "\n",
    "K_fold_splitter = KFoldSplitter(args,folds)\n",
    "K_subway_ds,args = K_fold_splitter.split_k_fold()\n",
    "subway_ds = K_subway_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model, trainer, and train it:\n",
    "model = load_model(args,dic_class2rpz)\n",
    "optimizer,scheduler,loss_function = load_optimizer_and_scheduler(model,args)\n",
    "trainer = Trainer(subway_ds,model,args,optimizer,loss_function,scheduler = scheduler,dic_class2rpz = dic_class2rpz,show_figure = True)# Ajoute dans trainer, if calibration_prop is not None .... et on modifie le dataloader en ajoutant un clabration set\n",
    "trainer.train_and_valid(mod = 1000,mod_plot = None)  # Récupère les conformity scores sur I1, avec les estimations faites precedemment \n",
    "\n",
    "# Plotting: \n",
    "pi,pi_cqr = plot_bokeh(trainer,subway_ds.normalizer,subway_ds.tensor_limits_keeper.df_verif_test,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model, trainer, and train it:\n",
    "model = load_model(args,dic_class2rpz)\n",
    "optimizer,scheduler,loss_function = load_optimizer_and_scheduler(model,args)\n",
    "trainer = Trainer(subway_ds,model,args,optimizer,loss_function,scheduler = scheduler,dic_class2rpz = dic_class2rpz,show_figure = True)# Ajoute dans trainer, if calibration_prop is not None .... et on modifie le dataloader en ajoutant un clabration set\n",
    "trainer.train_and_valid(mod = 1000,mod_plot = None)  # Récupère les conformity scores sur I1, avec les estimations faites precedemment \n",
    "\n",
    "# Plotting: \n",
    "pi,pi_cqr = plot_bokeh(trainer,subway_ds.normalizer,subway_ds.tensor_limits_keeper.df_verif_test,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init tensor:  tensor([2.2318e+09])\n",
      "Init feature vect:  tensor([7.0868e+09])\n",
      "\n",
      "block1:\n",
      "s_conv:  tensor(3.0692e+08) tconv:  tensor(1.2277e+09)\n",
      "\n",
      "block2:\n",
      "s_conv:  tensor(1.5548e+09) tconv:  tensor(1.2098e+09)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "netmob_init = torch.Tensor([7392*4*263*287])\n",
    "netmob_tensor = torch.Tensor([2934*4*263*287*8])\n",
    "\n",
    "print('Init tensor: ',netmob_init)\n",
    "print('Init feature vect: ',netmob_tensor)\n",
    "\n",
    "print('\\nblock1:')\n",
    "sconv = torch.prod(torch.Tensor([64, 32, 131, 143, 8]))\n",
    "tconv = torch.prod(torch.Tensor([64, 128, 131, 143, 8]))\n",
    "print('s_conv: ',sconv, 'tconv: ',tconv)\n",
    "\n",
    "print('\\nblock2:')\n",
    "sconv = torch.prod(torch.Tensor([64, 658, 65, 71, 8]))\n",
    "tconv = torch.prod(torch.Tensor([64, 512, 65, 71, 8]))\n",
    "print('s_conv: ',sconv, 'tconv: ',tconv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.0.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
