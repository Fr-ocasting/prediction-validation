{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Hyper-parameter tuning with Ray is not possible\n",
      ">>>>Model: STGCN; K_fold = 6; Loss function: MSE \n",
      ">>>>Model: STGCN; K_fold = 6; Loss function: MSE \n",
      "Invalid dates within this fold: 776\n",
      "\n",
      "Init Dataset: 'torch.Size([7392, 40]) with 295680 Total nb of elements and 0 Nan values\n",
      "nb subway_in invalid dates:  776\n",
      "T_subway_out:  torch.Size([7392, 40])\n",
      "vision_input_type POIs\n",
      "vision_model_name None\n",
      "Init U/Utarget size: torch.Size([4702, 40, 7])/torch.Size([4702, 40, 1]) Train/Valid/Test 2821 940 940\n",
      "Invalid dates within this fold: 481\n",
      "Considered Spatial-Unit:  Index(['AMP', 'BEL', 'BRO', 'COR', 'CUI', 'CUS', 'FLA', 'GOR', 'BLA', 'GRA',\n",
      "       'GUI', 'GIL', 'HEN', 'HOT', 'LAE', 'MAS', 'MER', 'LUM', 'PRY', 'PER',\n",
      "       'SAN', 'SAX', 'VMY', 'JEA', 'BON', 'CHA', 'VAI', 'VEN', 'MAC', 'GAR',\n",
      "       'FOC', 'REP', 'GER', 'DEB', 'JAU', 'CPA', 'CRO', 'PAR', 'SOI', 'OGA'],\n",
      "      dtype='object', name='COD_TRG')\n",
      "\n",
      "Init Dataset: 'torch.Size([4597, 40]) with 183880 Total nb of elements and 0 Nan values\n",
      "nb subway_in invalid dates:  481\n",
      "Considered Spatial-Unit:  Index(['AMP', 'BEL', 'BRO', 'COR', 'CUI', 'CUS', 'FLA', 'GOR', 'BLA', 'GRA',\n",
      "       'GUI', 'GIL', 'HEN', 'HOT', 'LAE', 'MAS', 'MER', 'LUM', 'PRY', 'PER',\n",
      "       'SAN', 'SAX', 'VMY', 'JEA', 'BON', 'CHA', 'VAI', 'VEN', 'MAC', 'GAR',\n",
      "       'FOC', 'REP', 'GER', 'DEB', 'JAU', 'CPA', 'CRO', 'PAR', 'SOI', 'OGA'],\n",
      "      dtype='object', name='COD_TRG')\n",
      "T_subway_out:  torch.Size([4597, 40])\n",
      "vision_input_type POIs\n",
      "vision_model_name None\n",
      "Init U/Utarget size: torch.Size([2940, 40, 7])/torch.Size([2940, 40, 1]) Train/Valid/Test 1059 941 939\n",
      "Invalid dates within this fold: 481\n",
      "Considered Spatial-Unit:  Index(['AMP', 'BEL', 'BRO', 'COR', 'CUI', 'CUS', 'FLA', 'GOR', 'BLA', 'GRA',\n",
      "       'GUI', 'GIL', 'HEN', 'HOT', 'LAE', 'MAS', 'MER', 'LUM', 'PRY', 'PER',\n",
      "       'SAN', 'SAX', 'VMY', 'JEA', 'BON', 'CHA', 'VAI', 'VEN', 'MAC', 'GAR',\n",
      "       'FOC', 'REP', 'GER', 'DEB', 'JAU', 'CPA', 'CRO', 'PAR', 'SOI', 'OGA'],\n",
      "      dtype='object', name='COD_TRG')\n",
      "\n",
      "Init Dataset: 'torch.Size([4597, 40]) with 183880 Total nb of elements and 0 Nan values\n",
      "nb subway_in invalid dates:  481\n",
      "Considered Spatial-Unit:  Index(['AMP', 'BEL', 'BRO', 'COR', 'CUI', 'CUS', 'FLA', 'GOR', 'BLA', 'GRA',\n",
      "       'GUI', 'GIL', 'HEN', 'HOT', 'LAE', 'MAS', 'MER', 'LUM', 'PRY', 'PER',\n",
      "       'SAN', 'SAX', 'VMY', 'JEA', 'BON', 'CHA', 'VAI', 'VEN', 'MAC', 'GAR',\n",
      "       'FOC', 'REP', 'GER', 'DEB', 'JAU', 'CPA', 'CRO', 'PAR', 'SOI', 'OGA'],\n",
      "      dtype='object', name='COD_TRG')\n",
      "T_subway_out:  torch.Size([4597, 40])\n",
      "vision_input_type POIs\n",
      "vision_model_name None\n",
      "Init U/Utarget size: torch.Size([2940, 40, 7])/torch.Size([2940, 40, 1]) Train/Valid/Test 1059 941 939\n",
      "\n",
      "NetMob Vision is NONE\n",
      "Model size: 0.003GB\n",
      "number of total parameters: 938529\n",
      "number of trainable parameters: 938529\n",
      "\n",
      "start training\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 67\u001b[0m\n\u001b[1;32m     60\u001b[0m args \u001b[38;5;241m=\u001b[39m local_get_args(model_name,\n\u001b[1;32m     61\u001b[0m                     args_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     62\u001b[0m                     dataset_names\u001b[38;5;241m=\u001b[39mdataset_names,\n\u001b[1;32m     63\u001b[0m                     dataset_for_coverage\u001b[38;5;241m=\u001b[39mdataset_for_coverage,\n\u001b[1;32m     64\u001b[0m                     modification \u001b[38;5;241m=\u001b[39m modification)\n\u001b[1;32m     66\u001b[0m training_mode_to_visualise \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# ['test','valid','train']\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m (trainer,ds,ds_no_shuffle,args) \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_init\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mstation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstation\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mtraining_mode_to_visualise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_mode_to_visualise\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/prediction-validation/examples/train_and_visu_non_recurrent.py:54\u001b[0m, in \u001b[0;36mevaluate_config\u001b[0;34m(args_init, modification, fold_to_evaluate, training_mode_to_visualise, station, transfer_modes, type_POIs, spatial_units, apps, POI_or_stations, expanded, individual_poi, sum_ts_pois)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_config\u001b[39m(args_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m                     modification \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m     32\u001b[0m                     fold_to_evaluate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m                     sum_ts_pois \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     43\u001b[0m                     ):\n\u001b[1;32m     45\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    args: \u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    type_POIs : list of type of POIs.                         >>> ['stadium','nightclub']\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    expanded: '' if we look at the intensity of netmob consumption at the POI. '_expanded' if we look also one square around.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     trainer,ds,args,trial_id,df_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_the_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodification\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfold_to_evaluate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     trainer,ds_no_shuffle \u001b[38;5;241m=\u001b[39m get_ds_without_shuffling_on_train_set(trainer,modification,args_init,fold_to_evaluate)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m training_mode \u001b[38;5;129;01min\u001b[39;00m training_mode_to_visualise:\n",
      "File \u001b[0;32m~/prediction-validation/examples/train_and_visu_non_recurrent.py:74\u001b[0m, in \u001b[0;36mtrain_the_config\u001b[0;34m(args_init, modification, fold_to_evaluate)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_the_config\u001b[39m(args_init,modification,fold_to_evaluate):\n\u001b[1;32m     73\u001b[0m     ds,args,trial_id,save_folder,df_loss \u001b[38;5;241m=\u001b[39m get_ds(modification\u001b[38;5;241m=\u001b[39mmodification,args_init\u001b[38;5;241m=\u001b[39margs_init,fold_to_evaluate\u001b[38;5;241m=\u001b[39mfold_to_evaluate)\n\u001b[0;32m---> 74\u001b[0m     trainer,df_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_on_ds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrial_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43msave_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer,ds,args,trial_id,df_loss\n",
      "File \u001b[0;32m~/prediction-validation/examples/benchmark.py:70\u001b[0m, in \u001b[0;36mtrain_on_ds\u001b[0;34m(ds, args, trial_id, save_folder, df_loss)\u001b[0m\n\u001b[1;32m     68\u001b[0m optimizer,scheduler,loss_function \u001b[38;5;241m=\u001b[39m load_optimizer_and_scheduler(model,args)\n\u001b[1;32m     69\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(ds,model,args,optimizer,loss_function,scheduler \u001b[38;5;241m=\u001b[39m scheduler,show_figure \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,trial_id \u001b[38;5;241m=\u001b[39m trial_id, fold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,save_folder \u001b[38;5;241m=\u001b[39m save_folder)\n\u001b[0;32m---> 70\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmod_plot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[1;32m     71\u001b[0m df_loss[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_train_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain_loss\n\u001b[1;32m     72\u001b[0m df_loss[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_valid_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mvalid_loss\n",
      "File \u001b[0;32m~/prediction-validation/trainer.py:235\u001b[0m, in \u001b[0;36mTrainer.train_and_valid\u001b[0;34m(self, normalizer, df_verif_test, mod, mod_plot, station)\u001b[0m\n\u001b[1;32m    233\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Train and Valid each epoch \u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_valid_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Save best model (only if it's not a ray tuning)\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_loss[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_valid) \u001b[38;5;241m&\u001b[39m (\u001b[38;5;129;01mnot\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mray)):\n",
      "File \u001b[0;32m~/prediction-validation/trainer.py:145\u001b[0m, in \u001b[0;36mTrainer.train_valid_one_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()   \u001b[38;5;66;03m#Activate Dropout \u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()   \u001b[38;5;66;03m# Desactivate Dropout \u001b[39;00m\n",
      "File \u001b[0;32m~/prediction-validation/trainer.py:420\u001b[0m, in \u001b[0;36mTrainer.loop_epoch\u001b[0;34m(self, track_loss)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_mode\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    419\u001b[0m     loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_loader()\n\u001b[0;32m--> 420\u001b[0m     Preds,Y_true,T_labels,nb_samples,loss_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop_through_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_tracking()\n",
      "File \u001b[0;32m~/prediction-validation/trainer.py:393\u001b[0m, in \u001b[0;36mTrainer.loop_through_batches\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    390\u001b[0m     contextual_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    392\u001b[0m x_b,y_b,contextual_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_to_device(x_b,y_b,contextual_b)\n\u001b[0;32m--> 393\u001b[0m pred,y_b,t_label,nb_samples,loss_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcontextual_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnb_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m Preds\u001b[38;5;241m.\u001b[39mappend(pred)\n\u001b[1;32m    396\u001b[0m Y_true\u001b[38;5;241m.\u001b[39mappend(y_b)\n",
      "File \u001b[0;32m~/prediction-validation/trainer.py:351\u001b[0m, in \u001b[0;36mTrainer.loop_batch\u001b[0;34m(self, x_b, y_b, contextual_b, nb_samples, loss_epoch)\u001b[0m\n\u001b[1;32m    344\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function(pred\u001b[38;5;241m.\u001b[39mfloat(),y_b)\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;66;03m#print('loss: ',loss)\u001b[39;00m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;66;03m#print('pred: ', pred.dtype, pred.size())\u001b[39;00m\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;66;03m#print('y_b: ', y_b.dtype, y_b.size())\u001b[39;00m\n\u001b[1;32m    348\u001b[0m         \u001b[38;5;66;03m#print(self.loss_function)\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;66;03m#print('\\nNo Mixed Precision')  \u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcontextual_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function(pred\u001b[38;5;241m.\u001b[39mfloat(),y_b)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;66;03m#print('\\nloss: ',loss) \u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# Back propagation (after each mini-batch)\u001b[39;00m\n",
      "File \u001b[0;32m/root/anaconda3/envs/pytorch-2.0.1/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/prediction-validation/dl_models/full_model.py:311\u001b[0m, in \u001b[0;36mfull_model.forward\u001b[0;34m(self, x, contextual)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m#print('x after Calendar model: ',x.size())\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m#print('CalendarEmbedded Vector: ',time_elt.size())\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# Core model \u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcore_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     x\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mextracted_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtime_elt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m#print('x after Core Model: ',x.size())\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# ...\u001b[39;00m\n\u001b[1;32m    314\u001b[0m x \u001b[38;5;241m=\u001b[39m reshaping(x)\n",
      "File \u001b[0;32m/root/anaconda3/envs/pytorch-2.0.1/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/prediction-validation/dl_models/STGCN/STGCN.py:132\u001b[0m, in \u001b[0;36mSTGCN.forward\u001b[0;34m(self, x, x_vision, x_calendar)\u001b[0m\n\u001b[1;32m    130\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;66;03m# [B,C,L,N] -> [B, C_out, L-4*nb_blocks, N]\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mst_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m### ---\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKo \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# Causal_TempConv2D - FC(128,128) -- FC(128,1) -- LN - ReLU --> [B,1,1,N]\u001b[39;00m\n",
      "File \u001b[0;32m/root/anaconda3/envs/pytorch-2.0.1/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/root/anaconda3/envs/pytorch-2.0.1/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/root/anaconda3/envs/pytorch-2.0.1/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/prediction-validation/dl_models/STGCN/STGCN_layer.py:336\u001b[0m, in \u001b[0;36mSTConvBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    334\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtmp_conv1(x)\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m#print('\\nShape après tmp_conv1: ',x.size())\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m#print('\\nShape après graph conv: ',x.size())\u001b[39;00m\n\u001b[1;32m    338\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m/root/anaconda3/envs/pytorch-2.0.1/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/prediction-validation/dl_models/STGCN/STGCN_layer.py:294\u001b[0m, in \u001b[0;36mGraphConvLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    292\u001b[0m     x_gc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheb_graph_conv(x_gc_in)\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_conv_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraph_conv\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 294\u001b[0m     x_gc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_gc_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m x_gc \u001b[38;5;241m=\u001b[39m x_gc\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    296\u001b[0m x_gc_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39madd(x_gc, x_gc_in)\n",
      "File \u001b[0;32m/root/anaconda3/envs/pytorch-2.0.1/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/prediction-validation/dl_models/STGCN/STGCN_layer.py:261\u001b[0m, in \u001b[0;36mGraphConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m#bs, c_in, ts, n_vertex = x.shape\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# If learnable adjacency matrix: \u001b[39;00m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_constructor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:    \n\u001b[1;32m    256\u001b[0m         \u001b[38;5;66;03m# Load adjacency matrix (i.e gso ?)\u001b[39;00m\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;66;03m#if idx is None:\u001b[39;00m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;66;03m#    gso = self.g_constructor(self.idx)\u001b[39;00m\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;66;03m#    gso = self.g_constructor(idx)\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m         gso \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mg_constructor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    265\u001b[0m     first_mul \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhi,btij->bthj\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgso, x)\n",
      "File \u001b[0;32m/root/anaconda3/envs/pytorch-2.0.1/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/prediction-validation/dl_models/MTGNN/MTGNN_layer.py:31\u001b[0m, in \u001b[0;36mgraph_constructor.forward\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_feat \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m         nodevec1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memb1\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m         nodevec2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb2(idx)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/root/anaconda3/envs/pytorch-2.0.1/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/root/anaconda3/envs/pytorch-2.0.1/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/root/anaconda3/envs/pytorch-2.0.1/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "# GET PARAMETERS\n",
    "import os \n",
    "import sys\n",
    "import torch \n",
    "# Get Parent folder : \n",
    "current_path = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_path, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "from examples.train_and_visu_non_recurrent import evaluate_config\n",
    "from plotting.plotting import error_per_station_calendar_pattern  \n",
    "from examples.benchmark import local_get_args\n",
    "# Init:\n",
    "dataset_names = [\"subway_in\",\"subway_out\"] # ['subway_in','netmob_POIs_per_station'],[\"subway_in\",\"subway_out\"],[\"subway_in\",\"calendar\"] # [\"subway_in\"] # ['data_bidon'] # ['METR_LA'] # ['PEMS_BAY']\n",
    "dataset_for_coverage = ['subway_in','netmob_image_per_station'] #  ['data_bidon','netmob'] #  ['subway_in','netmob']  # ['METR_LA'] # ['PEMS_BAY']\n",
    "model_name = 'STGCN'\n",
    "station = ['BEL','PAR','AMP','SAN','FLA']   # 'BON'  #'GER'\n",
    "# ...\n",
    "\n",
    "# Modif \n",
    "modification = {'epochs' : 10, #100\n",
    "                'lr':5e-4,# 4e-4,\n",
    "                'weight_decay':0.05,\n",
    "                'dropout':0.15,\n",
    "                #'set_spatial_units':  station,   \n",
    "                #  \n",
    "                'scheduler':None,\n",
    "                #'scheduler':True,\n",
    "                #'torch_scheduler_milestone': 5,\n",
    "                #'torch_scheduler_gamma':0.997,\n",
    "                #'torch_scheduler_lr_start_factor':1,\n",
    "\n",
    "                'temporal_graph_transformer_encoder': False,\n",
    "                #'TGE_num_layers' : 4, #2\n",
    "                #'TGE_num_heads' :  1, #IMPOSSIBLE > 1 CAR DOIT DIVISER L = 7\n",
    "                #'TGE_FC_hdim' :  32, #32\n",
    "\n",
    "                'NetMob_only_epsilon': True,    # True # False\n",
    "                'NetMob_selected_apps': ['Apple_iMessage','Web_Ads'],# ['Apple_iMessage','Web_Ads'], #,'Deezer','WhatsApp','Twitter'] #['Google_Maps']# ['Instagram','Google_Maps','Twitter']\n",
    "                'NetMob_transfer_mode' :  ['DL'], #,'UL'] # ['DL'] # ['UL'] #['DL','UL']\n",
    "                'NetMob_selected_tags' : ['station_epsilon100'],#['iris','stadium','station','university']#['park','stadium','university','station','shop','nightclub','parkings','theatre','iris','transit','public_transport']\n",
    "                'NetMob_expanded' : '', # '' # '_expanded'\n",
    "                'stacked_contextual': True, # True # False\n",
    "\n",
    "                'learnable_adj_matrix' : True,\n",
    "                \n",
    "                #'vision_num_heads':6,\n",
    "                #\"vision_grn_out_dim\":48,\n",
    "                #'vision_model_name': 'VariableSelectionNetwork',\n",
    "                #'vision_concatenation_early':True,   \n",
    "                #'vision_concatenation_late':True,\n",
    "\n",
    "                'compute_node_attr_with_attn' : False # True ??\n",
    "                           }\n",
    "# ...\n",
    "#1038945\n",
    "\n",
    "# Training and visu: \n",
    "args = local_get_args(model_name,\n",
    "                    args_init = None,\n",
    "                    dataset_names=dataset_names,\n",
    "                    dataset_for_coverage=dataset_for_coverage,\n",
    "                    modification = modification)\n",
    "\n",
    "training_mode_to_visualise = ['test'] # ['test','valid','train']\n",
    "(trainer,ds,ds_no_shuffle,args) = evaluate_config(args_init = args,\n",
    "                                                   station=station,modification=modification,\n",
    "                                                   training_mode_to_visualise=training_mode_to_visualise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Hyper-parameter tuning with Ray is not possible\n",
      "\n",
      ">>>> Load best CONFIG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Namespace(model_name='STGCN', dataset_names=['subway_in', 'netmob_POIs'], dataset_for_coverage=['subway_in', 'netmob_POIs'], n_vertex=40, C=1, device=device(type='cuda'), optimizer='adamw', single_station=False, loss_function_type='MSE', epsilon_clustering=0.05, contextual_positions={'netmob_POIs': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]}, quick_vision=False, netmob_transfer_mode='DL', evaluate_complete_ds=True, train_valid_test_split_method='similar_length_method', set_spatial_units=None, num_workers=0, persistent_workers=False, pin_memory=True, prefetch_factor=2, drop_last=False, mixed_precision=False, non_blocking=True, torch_compile=False, backend='inductor', prefetch_all=False, ray=False, ray_scheduler='ASHA', ray_search_alg=None, grace_period=20, HP_max_epochs=100, alpha=None, conformity_scores_type='max_residual', quantile_method='compute_quantile_by_class', calibration_calendar_class=0, type_calib='classic', H=6, W=0, D=1, step_ahead=1, L=7, shuffle=True, train_prop=0.6, calib_prop=None, valid_prop=0.2, test_prop=0.2, track_pi=False, validation='sliding_window', no_common_dates_between_set=False, K_fold=6, current_fold=0, abs_path='/home/rrochas/prediction-validation/', out_dim=1, vision_model_name='VariableSelectionNetwork', vision_input_type='POIs', Kt=3, stblock_num=2, Ks=2, graph_conv_type='graph_conv', gso_type='sym_norm_lap', enable_bias=True, adj_type='corr', enable_padding=True, threshold=0.3, act_func='glu', temporal_h_dim=256, spatial_h_dim=256, output_h_dim=8, blocks=[[1], [32, 32, 32], [32, 32, 32], [64]], weight_decay=0.0070518434559899, batch_size=32, lr=0.00065, dropout=0.2780612702300137, epochs=100, scheduler=True, args_embedding=Namespace(), args_vision=Namespace(grn_h_dim=8, grn_out_dim=64, contextual_static_dim=None, concatenation_late=False, concatenation_early=True, num_heads=2, List_input_sizes=[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7], List_nb_channels=[34, 36, 30, 39, 67, 21, 32, 48, 23, 55, 39, 42, 35, 39, 29, 31, 34, 33, 33, 40, 28, 40, 16, 34, 38, 33, 30, 69, 29, 33, 32, 44, 21, 29, 22, 38, 42, 34, 65, 57], out_dim=64, dataset_name='netmob_POIs', model_name='VariableSelectionNetwork', input_type='POIs'), torch_scheduler_milestone=25.0, torch_scheduler_gamma=0.991592836845444, torch_scheduler_lr_start_factor=0.7529123608527055)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Obtenir le chemin du dossier parent\n",
    "current_path = notebook_dir = os.getcwd()\n",
    "# current_path = os.path.dirname()\n",
    "working_dir = os.path.abspath(os.path.join(current_path, '..'))\n",
    "\n",
    "# Ajouter le dossier parent au chemin de recherche des modules\n",
    "if working_dir not in sys.path:\n",
    "    sys.path.insert(0, working_dir)\n",
    "\n",
    "# Personnal import \n",
    "from utils.utilities_DL import match_period_coverage_with_netmob\n",
    "from constants.config import get_args,update_modif\n",
    "from constants.paths import FOLDER_PATH,FILE_NAME,SAVE_DIRECTORY\n",
    "from K_fold_validation.K_fold_validation import KFoldSplitter\n",
    "from trainer import Trainer\n",
    "from high_level_DL_method import load_model,load_optimizer_and_scheduler\n",
    "from plotting.plotting_bokeh import plot_bokeh\n",
    "\n",
    "\n",
    "# Load config\n",
    "model_name = 'STGCN' #'CNN'\n",
    "dataset_names = ['subway_in','netmob']\n",
    "args = get_args(model_name,dataset_names)\n",
    "\n",
    "# Modification : \n",
    "args.K_fold = 5\n",
    "\n",
    "args.ray = False\n",
    "args.W = 0  # IMPORTANT AVEC NETMOB\n",
    "\n",
    "args.epochs = 100\n",
    "args.loss_function_type = 'MSE' # 'quantile'\n",
    "\n",
    "# optimization:\n",
    "args.mixed_precision = True\n",
    "\n",
    "args = update_modif(args)\n",
    "\n",
    "# Coverage Period : \n",
    "small_ds = False\n",
    "coverage = match_period_coverage_with_netmob(FILE_NAME,dataset_names=['subway_in','netmob'])\n",
    "\n",
    "# Choose DataSet and VisionModel if needed: \n",
    "dataset_names = ['netmob','subway_in'] # ['calendar','netmob'] #['subway_in','netmob','calendar']\n",
    "vision_model_name = 'FeatureExtractor_ResNetInspired'  # 'ImageAvgPooling'  #'FeatureExtractor_ResNetInspired' #'MinimalFeatureExtractor',\n",
    "\n",
    "# Train and Evaluate Model: \n",
    "mod_plot = 1 # bokeh plotting every epoch \n",
    "\n",
    "# Load K-fold subway-ds \n",
    "folds = [0] # Here we use the first fold for HP-tuning. \n",
    "\n",
    "# In case we need to compute the Sliding K-fold validation:\n",
    "# folds = np.arange(1,args.K_fold)\n",
    "\n",
    "K_fold_splitter = KFoldSplitter(args,folds)\n",
    "K_subway_ds,args = K_fold_splitter.split_k_fold()\n",
    "subway_ds = K_subway_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model, trainer, and train it:\n",
    "model = load_model(args,dic_class2rpz)\n",
    "optimizer,scheduler,loss_function = load_optimizer_and_scheduler(model,args)\n",
    "trainer = Trainer(subway_ds,model,args,optimizer,loss_function,scheduler = scheduler,dic_class2rpz = dic_class2rpz,show_figure = True)# Ajoute dans trainer, if calibration_prop is not None .... et on modifie le dataloader en ajoutant un clabration set\n",
    "trainer.train_and_valid(mod = 1000,mod_plot = None)  # Récupère les conformity scores sur I1, avec les estimations faites precedemment \n",
    "\n",
    "# Plotting: \n",
    "pi,pi_cqr = plot_bokeh(trainer,subway_ds.normalizer,subway_ds.tensor_limits_keeper.df_verif_test,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model, trainer, and train it:\n",
    "model = load_model(args,dic_class2rpz)\n",
    "optimizer,scheduler,loss_function = load_optimizer_and_scheduler(model,args)\n",
    "trainer = Trainer(subway_ds,model,args,optimizer,loss_function,scheduler = scheduler,dic_class2rpz = dic_class2rpz,show_figure = True)# Ajoute dans trainer, if calibration_prop is not None .... et on modifie le dataloader en ajoutant un clabration set\n",
    "trainer.train_and_valid(mod = 1000,mod_plot = None)  # Récupère les conformity scores sur I1, avec les estimations faites precedemment \n",
    "\n",
    "# Plotting: \n",
    "pi,pi_cqr = plot_bokeh(trainer,subway_ds.normalizer,subway_ds.tensor_limits_keeper.df_verif_test,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init tensor:  tensor([2.2318e+09])\n",
      "Init feature vect:  tensor([7.0868e+09])\n",
      "\n",
      "block1:\n",
      "s_conv:  tensor(3.0692e+08) tconv:  tensor(1.2277e+09)\n",
      "\n",
      "block2:\n",
      "s_conv:  tensor(1.5548e+09) tconv:  tensor(1.2098e+09)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "netmob_init = torch.Tensor([7392*4*263*287])\n",
    "netmob_tensor = torch.Tensor([2934*4*263*287*8])\n",
    "\n",
    "print('Init tensor: ',netmob_init)\n",
    "print('Init feature vect: ',netmob_tensor)\n",
    "\n",
    "print('\\nblock1:')\n",
    "sconv = torch.prod(torch.Tensor([64, 32, 131, 143, 8]))\n",
    "tconv = torch.prod(torch.Tensor([64, 128, 131, 143, 8]))\n",
    "print('s_conv: ',sconv, 'tconv: ',tconv)\n",
    "\n",
    "print('\\nblock2:')\n",
    "sconv = torch.prod(torch.Tensor([64, 658, 65, 71, 8]))\n",
    "tconv = torch.prod(torch.Tensor([64, 512, 65, 71, 8]))\n",
    "print('s_conv: ',sconv, 'tconv: ',tconv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.0.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
