{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Hyper-parameter tuning with Ray is not possible\n",
      ">>>>Model: ASTGCN; K_fold = 6; Loss function: MSE \n",
      ">>>>Model: ASTGCN; K_fold = 6; Loss function: MSE \n",
      "Invalid dates within this fold: 776\n",
      "\n",
      "Init Dataset: 'torch.Size([7392, 40]) with 295680 Total nb of elements and 0 Nan values\n",
      ">>>Tackle dataset_name: subway_indiv\n",
      "Chargement des données depuis : /home/rrochas/prediction-validation/../../../../data/rrochas/prediction_validation/agg_data/validation_individuelle/subway_indiv_15min/subway_indiv_15min.csv\n",
      "df.head():    COD_TRG             VAL_DATE  Flow\n",
      "0     AMP  2020-02-01 00:00:00    49\n",
      "1     AMP  2020-02-01 00:15:00    14\n",
      "2     AMP  2020-02-01 00:30:00    43\n",
      "3     AMP  2020-02-01 00:45:00    17\n",
      "4     AMP  2020-02-01 01:00:00    20\n",
      "Len coverage_period:  7392\n",
      "df.head():    COD_TRG            VAL_DATE  Flow\n",
      "0     AMP 2020-02-01 00:00:00    49\n",
      "1     AMP 2020-02-01 00:15:00    14\n",
      "2     AMP 2020-02-01 00:30:00    43\n",
      "3     AMP 2020-02-01 00:45:00    17\n",
      "4     AMP 2020-02-01 01:00:00    20\n",
      "df.head():                      COD_TRG  Flow\n",
      "VAL_DATE                         \n",
      "2020-02-01 00:00:00     AMP    49\n",
      "2020-02-01 00:15:00     AMP    14\n",
      "2020-02-01 00:30:00     AMP    43\n",
      "2020-02-01 00:45:00     AMP    17\n",
      "2020-02-01 01:00:00     AMP    20\n",
      "Check the current coverage period on the trial: (2019-03-16 00:00:00 - 2019-05-31 23:45:00)\n",
      "And the maximum coverage period of subway_indiv_15min: (2020-02-01 00:00:00 - 2020-02-27 23:45:00)\n",
      "df_reindexed.head():  Empty DataFrame\n",
      "Columns: [COD_TRG, Flow]\n",
      "Index: []\n",
      "ERROR : Not any remainig data in subway_indiv_15min.csv\n",
      "Check the current coverage period on the trial: (2019-03-16 00:00:00 - 2019-05-31 23:45:00)\n",
      "And the maximum coverage period of subway_indiv_15min: (2020-02-01 00:00:00 - 2020-02-27 23:45:00)\n",
      "ERREUR pendant le prétraitement des données subway_indiv_15min.csv: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 80\u001b[0m\n\u001b[1;32m     73\u001b[0m args \u001b[38;5;241m=\u001b[39m local_get_args(model_name,\n\u001b[1;32m     74\u001b[0m                     args_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     75\u001b[0m                     dataset_names\u001b[38;5;241m=\u001b[39mdataset_names,\n\u001b[1;32m     76\u001b[0m                     dataset_for_coverage\u001b[38;5;241m=\u001b[39mdataset_for_coverage,\n\u001b[1;32m     77\u001b[0m                     modification \u001b[38;5;241m=\u001b[39m modification)\n\u001b[1;32m     79\u001b[0m training_mode_to_visualise \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# ['test','valid','train']\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m (trainer,ds,ds_no_shuffle,args) \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_init\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mstation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstation\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mtraining_mode_to_visualise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_mode_to_visualise\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/prediction-validation/examples/train_and_visu_non_recurrent.py:54\u001b[0m, in \u001b[0;36mevaluate_config\u001b[0;34m(args_init, modification, fold_to_evaluate, training_mode_to_visualise, station, transfer_modes, type_POIs, spatial_units, apps, POI_or_stations, expanded, individual_poi, sum_ts_pois)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_config\u001b[39m(args_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m                     modification \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m     32\u001b[0m                     fold_to_evaluate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m                     sum_ts_pois \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     43\u001b[0m                     ):\n\u001b[1;32m     45\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    args: \u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    type_POIs : list of type of POIs.                         >>> ['stadium','nightclub']\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    expanded: '' if we look at the intensity of netmob consumption at the POI. '_expanded' if we look also one square around.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     trainer,ds,args,trial_id,df_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_the_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodification\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfold_to_evaluate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     trainer,ds_no_shuffle \u001b[38;5;241m=\u001b[39m get_ds_without_shuffling_on_train_set(trainer,modification,args_init,fold_to_evaluate)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m training_mode \u001b[38;5;129;01min\u001b[39;00m training_mode_to_visualise:\n",
      "File \u001b[0;32m~/prediction-validation/examples/train_and_visu_non_recurrent.py:73\u001b[0m, in \u001b[0;36mtrain_the_config\u001b[0;34m(args_init, modification, fold_to_evaluate)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_the_config\u001b[39m(args_init,modification,fold_to_evaluate):\n\u001b[0;32m---> 73\u001b[0m     ds,args,trial_id,save_folder,df_loss \u001b[38;5;241m=\u001b[39m \u001b[43mget_ds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodification\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfold_to_evaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_to_evaluate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     trainer,df_loss \u001b[38;5;241m=\u001b[39m train_on_ds(ds,args,trial_id,save_folder,df_loss)\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer,ds,args,trial_id,df_loss\n",
      "File \u001b[0;32m~/prediction-validation/examples/train_and_visu_non_recurrent.py:200\u001b[0m, in \u001b[0;36mget_ds\u001b[0;34m(model_name, dataset_names, dataset_for_coverage, modification, args_init, fold_to_evaluate)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_ds\u001b[39m(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,dataset_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,dataset_for_coverage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    196\u001b[0m            modification \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m    197\u001b[0m            args_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m    198\u001b[0m            fold_to_evaluate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    199\u001b[0m             ):\n\u001b[0;32m--> 200\u001b[0m     args_with_contextual,K_subway_ds \u001b[38;5;241m=\u001b[39m \u001b[43mget_multi_ds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs_init\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mdataset_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs_init\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mdataset_for_coverage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset_for_coverage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs_init\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_for_coverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mmodification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43margs_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mfold_to_evaluate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     ds \u001b[38;5;241m=\u001b[39m K_subway_ds[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    207\u001b[0m     trial_id \u001b[38;5;241m=\u001b[39m get_trial_id(args_with_contextual)\n",
      "File \u001b[0;32m~/prediction-validation/examples/train_and_visu_non_recurrent.py:247\u001b[0m, in \u001b[0;36mget_multi_ds\u001b[0;34m(model_name, dataset_names, dataset_for_coverage, modification, args_init, fold_to_evaluate)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args_init\u001b[38;5;241m.\u001b[39mhp_tuning_on_first_fold:\n\u001b[1;32m    244\u001b[0m         folds \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m folds\n\u001b[0;32m--> 247\u001b[0m K_fold_splitter,K_subway_ds,args_with_contextual \u001b[38;5;241m=\u001b[39m \u001b[43mget_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m args_with_contextual \u001b[38;5;241m=\u001b[39m modification_contextual_args(args_with_contextual,modification)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args_with_contextual,K_subway_ds\n",
      "File \u001b[0;32m~/prediction-validation/examples/benchmark.py:56\u001b[0m, in \u001b[0;36mget_inputs\u001b[0;34m(args, folds)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_inputs\u001b[39m(args,folds):\n\u001b[1;32m     55\u001b[0m     K_fold_splitter \u001b[38;5;241m=\u001b[39m KFoldSplitter(args,folds)\n\u001b[0;32m---> 56\u001b[0m     K_subway_ds,args \u001b[38;5;241m=\u001b[39m \u001b[43mK_fold_splitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_k_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# Keep the first fold or not : \u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mhp_tuning_on_first_fold:\n",
      "File \u001b[0;32m~/prediction-validation/K_fold_validation/K_fold_validation.py:79\u001b[0m, in \u001b[0;36mKFoldSplitter.split_k_fold\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_blocked_cv()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_split_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward_chaining_cv\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_chaining_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "File \u001b[0;32m~/prediction-validation/K_fold_validation/K_fold_validation.py:114\u001b[0m, in \u001b[0;36mKFoldSplitter.forward_chaining_cv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03mImplement validation following 'forward chaining cross validation' method. \u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03mSplit ds with respected proportion train_prop/valid_prop and keep initial test dataset:       \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m>>> The first few folds are particularly small, leading to worse metrics and large variance that can significantly impact the overall average MSE.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    113\u001b[0m K_ds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 114\u001b[0m target_ds_init,_,args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_init_ds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load 'U' and 'U_target'. # Define already feature vect for the K-th fold with proportion train/valid/test.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m#print('Considered Spatial-Unit: ',args.set_spatial_units)\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Get Init Coverage Period\u001b[39;00m\n\u001b[1;32m    117\u001b[0m df_verif_init \u001b[38;5;241m=\u001b[39m target_ds_init\u001b[38;5;241m.\u001b[39mtensor_limits_keeper\u001b[38;5;241m.\u001b[39mdf_verif \n",
      "File \u001b[0;32m~/prediction-validation/K_fold_validation/K_fold_validation.py:71\u001b[0m, in \u001b[0;36mKFoldSplitter.load_init_ds\u001b[0;34m(self, normalize)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_init_ds\u001b[39m(\u001b[38;5;28mself\u001b[39m,normalize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 71\u001b[0m     target_ds,contextual_ds,args \u001b[38;5;241m=\u001b[39m \u001b[43mload_complete_ds\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#,dic_class2rpz\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(target_ds,contextual_ds,args)\n",
      "File \u001b[0;32m~/prediction-validation/build_inputs/load_preprocessed_dataset.py:234\u001b[0m, in \u001b[0;36mload_complete_ds\u001b[0;34m(args, coverage_period, normalize)\u001b[0m\n\u001b[1;32m    231\u001b[0m args \u001b[38;5;241m=\u001b[39m get_args_embedding(args,dict_calendar_U_train)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Contextual: \u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m args,contextual_ds \u001b[38;5;241m=\u001b[39m \u001b[43mtackle_contextual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43minvalid_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43mintersect_coverage_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_ds.U_valid\u001b[39m\u001b[38;5;124m'\u001b[39m,target_ds\u001b[38;5;241m.\u001b[39mU_valid\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m contextual_ds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/prediction-validation/build_inputs/load_contextual_data.py:203\u001b[0m, in \u001b[0;36mtackle_contextual\u001b[0;34m(target_ds, invalid_dates, intersect_coverage_period, args, normalize)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# USE CONTEXTUAL DATA\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args\u001b[38;5;241m.\u001b[39mcontextual_dataset_names) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[0;32m--> 203\u001b[0m     contextual_ds,args \u001b[38;5;241m=\u001b[39m \u001b[43mtackle_input_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43minvalid_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43mintersect_coverage_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# TACKLE THE FEATURE EXTRACTOR MODULE \u001b[39;00m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvision_input_type\u001b[39m\u001b[38;5;124m'\u001b[39m, args\u001b[38;5;241m.\u001b[39mvision_input_type)\n",
      "File \u001b[0;32m~/prediction-validation/build_inputs/load_contextual_data.py:97\u001b[0m, in \u001b[0;36mtackle_input_data\u001b[0;34m(invalid_dates, intersect_coverage_period, args, normalize)\u001b[0m\n\u001b[1;32m     90\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(module)\n\u001b[1;32m     91\u001b[0m contextual_ds_i \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mload_data(parent_dir,FOLDER_PATH,\n\u001b[1;32m     92\u001b[0m                                    coverage_period \u001b[38;5;241m=\u001b[39m intersect_coverage_period,\n\u001b[1;32m     93\u001b[0m                                    invalid_dates\u001b[38;5;241m=\u001b[39minvalid_dates,\n\u001b[1;32m     94\u001b[0m                                    args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m     95\u001b[0m                                    normalize\u001b[38;5;241m=\u001b[39mnormalize\n\u001b[1;32m     96\u001b[0m                                    )\n\u001b[0;32m---> 97\u001b[0m \u001b[43mcontextual_ds_i\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m \u001b[38;5;241m=\u001b[39m dataset_name\n\u001b[1;32m     98\u001b[0m contextual_ds[dataset_name] \u001b[38;5;241m=\u001b[39m contextual_ds_i\n\u001b[1;32m     99\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" END NEW\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "# GET PARAMETERS\n",
    "import os \n",
    "import sys\n",
    "import torch \n",
    "# Get Parent folder : \n",
    "\n",
    "current_path = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_path, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "from examples.train_and_visu_non_recurrent import evaluate_config\n",
    "from plotting.plotting import error_per_station_calendar_pattern  \n",
    "from examples.benchmark import local_get_args\n",
    "# Init:\n",
    "dataset_names = ['subway_in','subway_indiv'] # ['subway_in'] #[\"subway_in\",\"subway_out\"] # ['subway_in','netmob_POIs_per_station'],[\"subway_in\",\"subway_out\"],[\"subway_in\",\"calendar\"] # [\"subway_in\"] # ['data_bidon'] # ['METR_LA'] # ['PEMS_BAY']\n",
    "dataset_for_coverage = ['subway_in','netmob_image_per_station'] #  ['data_bidon','netmob'] #  ['subway_in','netmob']  # ['METR_LA'] # ['PEMS_BAY']\n",
    "model_name = 'ASTGCN' # 'STGCN', 'ASTGCN'\n",
    "station = ['BEL','PAR','AMP','SAN','FLA']   # 'BON'  #'GER'\n",
    "# ...\n",
    "\n",
    "# Modif \n",
    "modification = {'target_data': 'subway_in', # 'subway_in'\n",
    "                'freq': '15min',\n",
    "                'use_target_as_context': False,\n",
    "                \n",
    "                \n",
    "                'epochs' : 1, #100\n",
    "                'lr':5e-5,# 4e-4,\n",
    "                'weight_decay':0.05,\n",
    "                'dropout':0.15,\n",
    "                #'set_spatial_units':  station,   \n",
    "                #  \n",
    "                'scheduler':None,\n",
    "                'adj_type':'corr',\n",
    "                'threshold': 0.7,\n",
    "                #'scheduler':True,\n",
    "                #'torch_scheduler_milestone': 5,\n",
    "                #'torch_scheduler_gamma':0.997,\n",
    "                #'torch_scheduler_lr_start_factor':1,\n",
    "\n",
    "                'temporal_graph_transformer_encoder': False,\n",
    "                #'TGE_num_layers' : 4, #2\n",
    "                #'TGE_num_heads' :  1, #IMPOSSIBLE > 1 CAR DOIT DIVISER L = 7\n",
    "                #'TGE_FC_hdim' :  32, #32\n",
    "\n",
    "                #'NetMob_only_epsilon': True,    # True # False\n",
    "                #'NetMob_selected_apps': ['Apple_iMessage','Web_Ads'],# ['Apple_iMessage','Web_Ads'], #,'Deezer','WhatsApp','Twitter'] #['Google_Maps']# ['Instagram','Google_Maps','Twitter']\n",
    "                #'NetMob_transfer_mode' :  ['DL'], #,'UL'] # ['DL'] # ['UL'] #['DL','UL']\n",
    "                #'NetMob_selected_tags' : ['station_epsilon100'],#['iris','stadium','station','university']#['park','stadium','university','station','shop','nightclub','parkings','theatre','iris','transit','public_transport']\n",
    "                #'NetMob_expanded' : '', # '' # '_expanded'\n",
    "                'stacked_contextual': True, # True # False\n",
    "\n",
    "                ### EXIST ONLY IF MODEL = STGCN\n",
    "                #'learnable_adj_matrix' : False, # True\n",
    "                #'graph_conv_type': 'graph_conv', # 'cheb_graph_conv', 'graph_conv'\n",
    "                #'learnable_adj_top_k': 10,\n",
    "                #'learnable_adj_embd_dim': 16, \n",
    "                ### ========\n",
    "\n",
    "                #'vision_num_heads':6,\n",
    "                #\"vision_grn_out_dim\":48,\n",
    "                #'vision_model_name': 'VariableSelectionNetwork',\n",
    "                #'vision_concatenation_early':True,   \n",
    "                #'vision_concatenation_late':True,\n",
    "                'compute_node_attr_with_attn' : False, # True ??\n",
    "                #'adj_type':'corr'\n",
    "                           }\n",
    "# ...\n",
    "#1038945\n",
    "\n",
    "# Training and visu: \n",
    "args = local_get_args(model_name,\n",
    "                    args_init = None,\n",
    "                    dataset_names=dataset_names,\n",
    "                    dataset_for_coverage=dataset_for_coverage,\n",
    "                    modification = modification)\n",
    "\n",
    "training_mode_to_visualise = ['test'] # ['test','valid','train']\n",
    "(trainer,ds,ds_no_shuffle,args) = evaluate_config(args_init = args,\n",
    "                                                   station=station,modification=modification,\n",
    "                                                   training_mode_to_visualise=training_mode_to_visualise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2020-02-01 00:00:00', '2020-02-01 00:15:00',\n",
       "               '2020-02-01 00:30:00', '2020-02-01 00:45:00',\n",
       "               '2020-02-01 01:00:00', '2020-02-01 01:15:00',\n",
       "               '2020-02-01 01:30:00', '2020-02-01 01:45:00',\n",
       "               '2020-02-01 02:00:00', '2020-02-01 02:15:00',\n",
       "               ...\n",
       "               '2020-02-27 21:30:00', '2020-02-27 21:45:00',\n",
       "               '2020-02-27 22:00:00', '2020-02-27 22:15:00',\n",
       "               '2020-02-27 22:30:00', '2020-02-27 22:45:00',\n",
       "               '2020-02-27 23:00:00', '2020-02-27 23:15:00',\n",
       "               '2020-02-27 23:30:00', '2020-02-27 23:45:00'],\n",
       "              dtype='datetime64[ns]', name='VAL_DATE', length=2262, freq=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dirname = '/home/rrochas/prediction-validation/../../../../data/rrochas/prediction_validation/agg_data/validation_individuelle/subway_indiv_15min/'\n",
    "os.listdir(dirname)\n",
    "os.path.exists(f\"{dirname}/subway_indiv_15min.csv\")\n",
    "\n",
    "FILE_BASE_NAME = 'subway_indiv'\n",
    "DATA_SUBFOLDER = 'agg_data/validation_individuelle' # Sous-dossier dans FOLDER_PATH\n",
    "NATIVE_FREQ = '3min'\n",
    "START = '2019-10-01' # Exemple basé sur head()\n",
    "END = '2020-04-01'\n",
    "list_of_invalid_period = []\n",
    "C = 1\n",
    "\n",
    "DATE_COL = 'VAL_DATE'\n",
    "LOCATION_COL = 'COD_TRG'\n",
    "VALUE_COL = 'Flow'\n",
    "\n",
    "target_freq = '15min'\n",
    "file_name = f\"{FILE_BASE_NAME}_{target_freq}\"\n",
    "data_file_path = f\"{dirname}/{file_name}.csv\"\n",
    "\n",
    "df = pd.read_csv(data_file_path)\n",
    "df['VAL_DATE'] = pd.to_datetime(df['VAL_DATE'])\n",
    "df=df.set_index('VAL_DATE')\n",
    "df.index.unique().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start training\n",
      "\n",
      "Training Throughput:1063.21 sequences per seconds\n",
      ">>> Training complete in: 0:04:49.253988\n",
      ">>> Training performance time: min 0.016025543212890625 avg 0.028232812881469727 seconds (+/- 0.007409689556679389)\n",
      ">>> Loading performance time: min 0.00011897087097167969 avg 0.021025142456797832 seconds (+/- 0.0533871588248097)\n",
      ">>> Forward performance time: 0.014497923700226319 seconds (+/- 0.006785826679855182)\n",
      ">>> Backward performance time: 0.01471411531366729 seconds (+/- 0.0026613273244625673)\n",
      ">>> Plotting performance time: 2.2859429594260365e-06 seconds (+/- 7.815177432756738e-07)\n",
      ">>> Saving performance time: 0.6629389921824137 seconds (+/- 0.29361714449532417)\n",
      ">>> PI-tracking performance time: 5.977237643908016e-06 seconds (+/- 7.975377572890917e-06)\n",
      ">>> Scheduler-update performance time: 4.9972054946362674e-06 seconds (+/- 9.795780319996475e-06)\n",
      ">>> Validation time: 0:00:00.356755\n",
      "Proportion of time consumed for Loading: 41.6%\n",
      "Proportion of time consumed for Forward: 28.3%\n",
      "Proportion of time consumed for Backward: 28.7%\n",
      "Proportion of time consumed for Plotting: 0.0%\n",
      "Proportion of time consumed for CheckPoint Saving: 1.4%\n",
      "Proportion of time consumed for Tracking PI: 0.0%\n",
      "Proportion of time consumed for Update Scheduler: 0.0%\n",
      "Proportion of time consumed for Read all data on GPU: 0.0%\n",
      "\n",
      "Max GPU memory allocated: 0.18022966384887695 GB\n",
      "Max GPU memory cached: 0.2890625 GB\n",
      "Max CPU memory allocated: 3.2993011474609375 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "trainer.train_and_valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import eigs\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "L = (pd.DataFrame(np.random.rand(10,10))*10)\n",
    "\n",
    "\n",
    "lambda_max = eigs(L.values , k=1, which='LR')[0].real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'match_period_coverage_with_netmob' from 'utils.utilities_DL' (/home/rrochas/prediction-validation/utils/utilities_DL.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, working_dir)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Personnal import \u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities_DL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m match_period_coverage_with_netmob\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_args,update_modif\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpaths\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FOLDER_PATH,FILE_NAME,SAVE_DIRECTORY\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'match_period_coverage_with_netmob' from 'utils.utilities_DL' (/home/rrochas/prediction-validation/utils/utilities_DL.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Obtenir le chemin du dossier parent\n",
    "current_path = notebook_dir = os.getcwd()\n",
    "# current_path = os.path.dirname()\n",
    "working_dir = os.path.abspath(os.path.join(current_path, '..'))\n",
    "\n",
    "# Ajouter le dossier parent au chemin de recherche des modules\n",
    "if working_dir not in sys.path:\n",
    "    sys.path.insert(0, working_dir)\n",
    "\n",
    "# Personnal import \n",
    "from utils.utilities_DL import match_period_coverage_with_netmob\n",
    "from constants.config import get_args,update_modif\n",
    "from constants.paths import FOLDER_PATH,FILE_NAME,SAVE_DIRECTORY\n",
    "from K_fold_validation.K_fold_validation import KFoldSplitter\n",
    "from trainer import Trainer\n",
    "from high_level_DL_method import load_model,load_optimizer_and_scheduler\n",
    "from plotting.plotting_bokeh import plot_bokeh\n",
    "\n",
    "\n",
    "# Load config\n",
    "model_name = 'STGCN' #'CNN'\n",
    "dataset_names = ['subway_in','netmob']\n",
    "args = get_args(model_name,dataset_names)\n",
    "\n",
    "# Modification : \n",
    "args.K_fold = 5\n",
    "\n",
    "args.ray = False\n",
    "args.W = 0  # IMPORTANT AVEC NETMOB\n",
    "\n",
    "args.epochs = 100\n",
    "args.loss_function_type = 'MSE' # 'quantile'\n",
    "\n",
    "# optimization:\n",
    "args.mixed_precision = True\n",
    "\n",
    "args = update_modif(args)\n",
    "\n",
    "# Coverage Period : \n",
    "small_ds = False\n",
    "coverage = match_period_coverage_with_netmob(FILE_NAME,dataset_names=['subway_in','netmob'])\n",
    "\n",
    "# Choose DataSet and VisionModel if needed: \n",
    "dataset_names = ['netmob','subway_in'] # ['calendar','netmob'] #['subway_in','netmob','calendar']\n",
    "vision_model_name = 'FeatureExtractor_ResNetInspired'  # 'ImageAvgPooling'  #'FeatureExtractor_ResNetInspired' #'MinimalFeatureExtractor',\n",
    "\n",
    "# Train and Evaluate Model: \n",
    "mod_plot = 1 # bokeh plotting every epoch \n",
    "\n",
    "# Load K-fold subway-ds \n",
    "folds = [0] # Here we use the first fold for HP-tuning. \n",
    "\n",
    "# In case we need to compute the Sliding K-fold validation:\n",
    "# folds = np.arange(1,args.K_fold)\n",
    "\n",
    "K_fold_splitter = KFoldSplitter(args,folds)\n",
    "K_subway_ds,args = K_fold_splitter.split_k_fold()\n",
    "subway_ds = K_subway_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model, trainer, and train it:\n",
    "model = load_model(args,dic_class2rpz)\n",
    "optimizer,scheduler,loss_function = load_optimizer_and_scheduler(model,args)\n",
    "trainer = Trainer(subway_ds,model,args,optimizer,loss_function,scheduler = scheduler,dic_class2rpz = dic_class2rpz,show_figure = True)# Ajoute dans trainer, if calibration_prop is not None .... et on modifie le dataloader en ajoutant un clabration set\n",
    "trainer.train_and_valid(mod = 1000,mod_plot = None)  # Récupère les conformity scores sur I1, avec les estimations faites precedemment \n",
    "\n",
    "# Plotting: \n",
    "pi,pi_cqr = plot_bokeh(trainer,subway_ds.normalizer,subway_ds.tensor_limits_keeper.df_verif_test,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model, trainer, and train it:\n",
    "model = load_model(args,dic_class2rpz)\n",
    "optimizer,scheduler,loss_function = load_optimizer_and_scheduler(model,args)\n",
    "trainer = Trainer(subway_ds,model,args,optimizer,loss_function,scheduler = scheduler,dic_class2rpz = dic_class2rpz,show_figure = True)# Ajoute dans trainer, if calibration_prop is not None .... et on modifie le dataloader en ajoutant un clabration set\n",
    "trainer.train_and_valid(mod = 1000,mod_plot = None)  # Récupère les conformity scores sur I1, avec les estimations faites precedemment \n",
    "\n",
    "# Plotting: \n",
    "pi,pi_cqr = plot_bokeh(trainer,subway_ds.normalizer,subway_ds.tensor_limits_keeper.df_verif_test,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init tensor:  tensor([2.2318e+09])\n",
      "Init feature vect:  tensor([7.0868e+09])\n",
      "\n",
      "block1:\n",
      "s_conv:  tensor(3.0692e+08) tconv:  tensor(1.2277e+09)\n",
      "\n",
      "block2:\n",
      "s_conv:  tensor(1.5548e+09) tconv:  tensor(1.2098e+09)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "netmob_init = torch.Tensor([7392*4*263*287])\n",
    "netmob_tensor = torch.Tensor([2934*4*263*287*8])\n",
    "\n",
    "print('Init tensor: ',netmob_init)\n",
    "print('Init feature vect: ',netmob_tensor)\n",
    "\n",
    "print('\\nblock1:')\n",
    "sconv = torch.prod(torch.Tensor([64, 32, 131, 143, 8]))\n",
    "tconv = torch.prod(torch.Tensor([64, 128, 131, 143, 8]))\n",
    "print('s_conv: ',sconv, 'tconv: ',tconv)\n",
    "\n",
    "print('\\nblock2:')\n",
    "sconv = torch.prod(torch.Tensor([64, 658, 65, 71, 8]))\n",
    "tconv = torch.prod(torch.Tensor([64, 512, 65, 71, 8]))\n",
    "print('s_conv: ',sconv, 'tconv: ',tconv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.0.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
