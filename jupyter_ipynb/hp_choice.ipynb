{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Obtenir le chemin du dossier parent\n",
    "current_path = notebook_dir = os.getcwd()\n",
    "# current_path = os.path.dirname()\n",
    "working_dir = os.path.abspath(os.path.join(current_path, '..'))\n",
    "\n",
    "# Ajouter le dossier parent au chemin de recherche des modules\n",
    "if working_dir not in sys.path:\n",
    "    sys.path.insert(0, working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'pynvml' is not available on this environment.\n"
     ]
    }
   ],
   "source": [
    "# Personnal import \n",
    "from utils.utilities_DL import match_period_coverage_with_netmob,get_small_ds\n",
    "from constants.config import get_args,update_modif\n",
    "from constants.paths import FOLDER_PATH,FILE_NAME,SAVE_DIRECTORY\n",
    "from K_fold_validation.K_fold_validation import KFoldSplitter\n",
    "\n",
    "# Hp Tuning\n",
    "from HP_tuning.hyperparameter_tuning_ray import HP_tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: STGCN, K_fold = 5\n",
      "!!! Loss function: MSE \n"
     ]
    }
   ],
   "source": [
    "# Load config\n",
    "model_name = 'STGCN' #'CNN'\n",
    "dataset_names = ['subway_in','netmob']\n",
    "args = get_args(model_name,dataset_names)\n",
    "\n",
    "# Modification : \n",
    "args.K_fold = 5\n",
    "args.ray = True\n",
    "args.W = 0  # IMPORTANT AVEC NETMOB\n",
    "args.epochs = 100\n",
    "args.loss_function_type = 'MSE' # 'quantile'\n",
    "\n",
    "args = update_modif(args)\n",
    "\n",
    "# Coverage Period : \n",
    "small_ds = False\n",
    "coverage = match_period_coverage_with_netmob(FILE_NAME,dataset_names=['subway_in','netmob'])\n",
    "(coverage,args) = get_small_ds(small_ds,coverage,args)\n",
    "\n",
    "# Choose DataSet and VisionModel if needed: \n",
    "dataset_names = ['netmob','subway_in'] # ['calendar','netmob'] #['subway_in','netmob','calendar']\n",
    "vision_model_name = 'FeatureExtractor_ResNetInspired'  # 'ImageAvgPooling'  #'FeatureExtractor_ResNetInspired' #'MinimalFeatureExtractor',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-step per hour: 4.0\n",
      "coverage period: 2019-03-16 00:00:00 - 2019-05-31 23:45:00\n",
      "\n",
      "Init Subway-In Dataset:  torch.Size([7392, 40])\n",
      "Number of Nan Value:  tensor(0)\n",
      "Total Number of Elements:  295680 \n",
      "\n",
      "\n",
      "U size:  torch.Size([6719, 40, 7]) Utarget size:  torch.Size([6719, 40, 1])\n",
      "U_train size:  torch.Size([3912, 40, 7]) Utarget_train size:  torch.Size([3912, 40, 1])\n",
      "U_valid size:  torch.Size([1304, 40, 7]) Utarget_valid size:  torch.Size([1304, 40, 1])\n",
      "U_test size:  torch.Size([1310, 40, 7]) Utarget_test size:  torch.Size([1310, 40, 1])\n",
      "U_train min:  tensor(0.) U_train max:  tensor(10798.)\n",
      "U_valid min:  tensor(0.) U_valid max:  tensor(1405.)\n",
      "U_test min:  tensor(0.) U_test max:  tensor(1760.)\n",
      "Init NetMob Dataset:  torch.Size([7392, 40, 6, 22, 22])\n",
      "Number of Nan Value:  tensor(0)\n",
      "Total Number of Elements:  858654720 \n",
      "\n",
      "\n",
      "U size:  torch.Size([6719, 40, 6, 22, 22, 7]) Utarget size:  torch.Size([6719, 40, 6, 22, 22, 1])\n",
      "U_train size:  torch.Size([3912, 40, 6, 22, 22, 7]) Utarget_train size:  torch.Size([3912, 40, 6, 22, 22, 1])\n",
      "U_valid size:  torch.Size([1304, 40, 6, 22, 22, 7]) Utarget_valid size:  torch.Size([1304, 40, 6, 22, 22, 1])\n",
      "U_test size:  torch.Size([1310, 40, 6, 22, 22, 7]) Utarget_test size:  torch.Size([1310, 40, 6, 22, 22, 1])\n",
      "U_train min:  tensor(0.) U_train max:  tensor(284421.)\n",
      "U_valid min:  tensor(0.) U_valid max:  tensor(268781.)\n",
      "U_test min:  tensor(0.) U_test max:  tensor(764339.)\n",
      "\n",
      " ===== ERROR ==== \n",
      "Try with torch >= 2.0.0 (works with 2.0.1) \n",
      "ValueError: prefetch_factor option could only be specified in multiprocessing.let num_workers > 0 to enable multiprocessing\n",
      "\n",
      " ===== ERROR ==== \n",
      "Try with torch >= 2.0.0 (works with 2.0.1) \n",
      "ValueError: prefetch_factor option could only be specified in multiprocessing.let num_workers > 0 to enable multiprocessing\n",
      "\n",
      " ===== ERROR ==== \n",
      "Try with torch >= 2.0.0 (works with 2.0.1) \n",
      "ValueError: prefetch_factor option could only be specified in multiprocessing.let num_workers > 0 to enable multiprocessing\n",
      "\n",
      " ===== ERROR ==== \n",
      "Try with torch >= 2.0.0 (works with 2.0.1) \n",
      "ValueError: prefetch_factor option could only be specified in multiprocessing.let num_workers > 0 to enable multiprocessing\n",
      "\n",
      "Fold n°0\n",
      "Time-step per hour: 4.0\n",
      "coverage period: 2019-03-16 00:00:00 - 2019-04-16 03:45:00\n",
      "\n",
      "Init Subway-In Dataset:  torch.Size([2992, 40])\n",
      "Number of Nan Value:  tensor(0)\n",
      "Total Number of Elements:  119680 \n",
      "\n",
      "\n",
      " Tackling Training Set\n",
      "\n",
      " Tackling Validation Set\n",
      "\n",
      " Tackling Training Set\n",
      "\n",
      " Tackling Validation Set\n",
      "\n",
      "U size:  torch.Size([2896, 40, 7]) Utarget size:  torch.Size([2896, 40, 1])\n",
      "U_train size:  torch.Size([2177, 40, 7]) Utarget_train size:  torch.Size([2177, 40, 1])\n",
      "U_valid size:  torch.Size([622, 40, 7]) Utarget_valid size:  torch.Size([622, 40, 1])\n",
      "U_train min:  tensor(0.) U_train max:  tensor(1.)\n",
      "U_valid min:  tensor(0.) U_valid max:  tensor(1.1906)\n"
     ]
    }
   ],
   "source": [
    "# Train and Evaluate Model: \n",
    "mod_plot = 1 # bokeh plotting every epoch \n",
    "\n",
    "# Load K-fold subway-ds \n",
    "folds = [0] # Here we use the first fold for HP-tuning. \n",
    "\n",
    "# In case we need to compute the Sliding K-fold validation:\n",
    "# folds = np.arange(1,args.K_fold)\n",
    "\n",
    "K_fold_splitter = KFoldSplitter(dataset_names,args,coverage,vision_model_name,folds)\n",
    "K_subway_ds,dic_class2rpz = K_fold_splitter.split_k_fold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HP Tuning on Fold0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 20\n",
    "subway_ds = K_subway_ds[0]\n",
    "analysis = HP_tuning(subway_ds,args,args_embedding,args_vision,num_samples,dic_class2rpz,working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 0.004GB\n",
      "number of total parameters: 1151659\n",
      "number of trainable parameters: 1151659\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Python 3.11+ not yet supported for torch.compile",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load Optimizer, Scheduler, Loss function: \u001b[39;00m\n\u001b[1;32m     14\u001b[0m optimizer,scheduler,loss_function \u001b[38;5;241m=\u001b[39m load_optimizer_and_scheduler(model,args)\n\u001b[0;32m---> 16\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(subway_ds,model,args,optimizer,loss_function,scheduler \u001b[38;5;241m=\u001b[39m scheduler,args_embedding  \u001b[38;5;241m=\u001b[39margs_embedding,dic_class2rpz \u001b[38;5;241m=\u001b[39m dic_class2rpz,show_figure \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;66;03m# Ajoute dans trainer, if calibration_prop is not None .... et on modifie le dataloader en ajoutant un clabration set\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Train Model \u001b[39;00m\n\u001b[1;32m     18\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain_and_valid(mod \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m,mod_plot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Récupère les conformity scores sur I1, avec les estimations faites precedemment \u001b[39;00m\n",
      "File \u001b[0;32m/home/rrochas/prediction_validation/trainer.py:113\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, dataset, model, args, optimizer, loss_function, scheduler, args_embedding, dic_class2rpz, fold, trial_id1, trial_id2, show_figure)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function \u001b[38;5;241m=\u001b[39m loss_function\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mtorch_compile:\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcompile(model,\n\u001b[1;32m    114\u001b[0m                                fullgraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    115\u001b[0m                                backend \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mbackend,\n\u001b[1;32m    116\u001b[0m                                mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    117\u001b[0m                                 )\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model \n",
      "File \u001b[0;32m~/anaconda3/envs/HP_tuning/lib/python3.11/site-packages/torch/__init__.py:1441\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(model, fullgraph, dynamic, backend, mode, options, disable)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minductor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1440\u001b[0m     backend \u001b[38;5;241m=\u001b[39m _TorchCompileInductorWrapper(mode, options, dynamic)\n\u001b[0;32m-> 1441\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39moptimize(backend\u001b[38;5;241m=\u001b[39mbackend, nopython\u001b[38;5;241m=\u001b[39mfullgraph, dynamic\u001b[38;5;241m=\u001b[39mdynamic, disable\u001b[38;5;241m=\u001b[39mdisable)(model)\n",
      "File \u001b[0;32m~/anaconda3/envs/HP_tuning/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:413\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(backend, nopython, guard_export_fn, guard_fail_fn, disable, dynamic)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    381\u001b[0m     backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minductor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m     dynamic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    388\u001b[0m ):\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m    The main entrypoint of TorchDynamo.  Do graph capture and call\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03m    backend() to optimize extracted graphs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;03m            ...\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     check_if_dynamo_supported()\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;66;03m# Note: The hooks object could be global instead of passed around, *however* that would make\u001b[39;00m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# for a confusing API usage and plumbing story wherein we nest multiple .optimize calls.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# There is some prior art around this, w/r/t nesting backend calls are enforced to be the same\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;66;03m# compiler, however, this feels onerous for callback and hooks, and it feels better to give our users an\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;66;03m# easier to understand UX at the cost of a little more plumbing on our end.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     hooks \u001b[38;5;241m=\u001b[39m Hooks(guard_export_fn\u001b[38;5;241m=\u001b[39mguard_export_fn, guard_fail_fn\u001b[38;5;241m=\u001b[39mguard_fail_fn)\n",
      "File \u001b[0;32m~/anaconda3/envs/HP_tuning/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:377\u001b[0m, in \u001b[0;36mcheck_if_dynamo_supported\u001b[0;34m()\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWindows not yet supported for torch.compile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[0;32m--> 377\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython 3.11+ not yet supported for torch.compile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Python 3.11+ not yet supported for torch.compile"
     ]
    }
   ],
   "source": [
    "from trainer import Trainer\n",
    "from high_level_DL_method import load_model,load_optimizer_and_scheduler\n",
    "from plotting.plotting_bokeh import plot_bokeh\n",
    "\n",
    "args.mixed_precision = True\n",
    "#args.torch_compile = True\n",
    "#args_vision.h_dim = 16\n",
    "subway_ds = K_subway_ds[0]\n",
    "args.epochs = 3\n",
    "# Load Model:\n",
    "model = load_model(subway_ds,args,dic_class2rpz)\n",
    "\n",
    "# Load Optimizer, Scheduler, Loss function: \n",
    "optimizer,scheduler,loss_function = load_optimizer_and_scheduler(model,args)\n",
    "\n",
    "trainer = Trainer(subway_ds,model,args,optimizer,loss_function,scheduler = scheduler,dic_class2rpz = dic_class2rpz,show_figure = True)# Ajoute dans trainer, if calibration_prop is not None .... et on modifie le dataloader en ajoutant un clabration set\n",
    "# Train Model \n",
    "trainer.train_and_valid(mod = 1000,mod_plot = None)  # Récupère les conformity scores sur I1, avec les estimations faites precedemment \n",
    "\n",
    "pi,pi_cqr = plot_bokeh(trainer,subway_ds.normalizer,subway_ds.tensor_limits_keeper.df_verif_test,args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
