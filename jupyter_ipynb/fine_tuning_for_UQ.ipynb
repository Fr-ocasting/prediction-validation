{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET PARAMETERS\n",
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "import torch \n",
    "\n",
    "# Get Parent folder : \n",
    "current_path = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_path, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "from constants.paths import SAVE_DIRECTORY\n",
    "from examples.train_and_visu_non_recurrent import get_ds,evaluate_config,analysis_on_specific_training_mode\n",
    "from high_level_DL_method import load_model,load_optimizer_and_scheduler\n",
    "from examples.load_best_config import load_args_of_a_specific_trial\n",
    "from trainer import Trainer\n",
    "\n",
    "\n",
    "def apply_transfer_learning(model,current_path,save_folder,trial_id,add_name_id,fold_name):\n",
    "    # Load trained weights:\n",
    "    model_param = torch.load(f\"{current_path}/{SAVE_DIRECTORY}/{save_folder}/best_models/{trial_id}{add_name_id}_f{fold_name}.pkl\")\n",
    "\n",
    "    # Dupplicate Output Weights if needed: \n",
    "    output_weight = f'core_model.output.fc2.weight'\n",
    "    output_bias = f'core_model.output.fc2.bias'\n",
    "\n",
    "    size_output_init = model_param['state_dict'][output_weight].size()\n",
    "    size_output_current = model.state_dict()[output_weight].size()\n",
    "\n",
    "    if not (size_output_current == size_output_init):\n",
    "        model_param['state_dict'][output_weight] = model_param['state_dict'][output_weight].repeat(size_output_current[0],1)\n",
    "        model_param['state_dict'][output_bias] = model_param['state_dict'][output_bias].repeat(size_output_current[0])\n",
    "    # ...\n",
    "\n",
    "    # Tranfer learning: \n",
    "    model.load_state_dict(model_param['state_dict'], strict=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "def fine_tune_model(model,ds,args,reduce_lr,freeze,stations_to_plot,training_mode,epochs_fine_tune):\n",
    "    # Modification: \n",
    "    args.epochs = epochs_fine_tune\n",
    "    # Reduce LR: \n",
    "    if reduce_lr:\n",
    "        args.lr =args.lr/5\n",
    "\n",
    "    # Freeze weights, excepted the 'output' module : \n",
    "    if freeze:\n",
    "        for name, param in model.named_parameters(): #model.core_model.named_parameters():\n",
    "            param.requires_grad = False \n",
    "        for name, param in model.core_model.output.named_parameters():\n",
    "            param.requires_grad = True  \n",
    "        # ...\n",
    "\n",
    "\n",
    "    optimizer,scheduler,loss_function = load_optimizer_and_scheduler(model,args)\n",
    "    trainer = Trainer(ds,model,args,optimizer,loss_function,scheduler = scheduler)\n",
    "    trainer.train_and_valid(normalizer =ds.normalizer)\n",
    "\n",
    "    analysis_on_specific_training_mode(trainer,ds,training_mode,station = stations_to_plot)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the config of a trained model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trained Model with Subway-in / Subway-out \n",
    "if True:\n",
    "    save_folder = 'K_fold_validation/training_with_HP_tuning/re_validation'\n",
    "    add_name_id = ''\n",
    "    trial_id ='subway_in_STGCN_MSELoss_2025_01_20_14_27_20569'\n",
    "\n",
    "if False:\n",
    "    save_folder = 'K_fold_validation/training_with_HP_tuning/re_validation'\n",
    "    add_name_id = 'concat_early'\n",
    "    trial_id ='subway_in_subway_out_STGCN_VariableSelectionNetwork_MSELoss_2025_01_20_05_38_87836' \n",
    "    #trainer2,ds2,args2 = get_trainer_and_ds_from_saved_trial(trial_id2,add_name_id2,save_folder2,modification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change Objective function and the output-dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modification = {'shuffle':True,\n",
    "                'loss_function_type':'quantile',\n",
    "                'alpha':0.05,\n",
    "                'track_pi':True,\n",
    "                'type_calib':'classic',\n",
    "                #'data_augmentation':False\n",
    "                }\n",
    "fold_name = 'complete_dataset'\n",
    "\n",
    "#args,_ = load_configuration(trial_id1,load_config=True)\n",
    "args = load_args_of_a_specific_trial(trial_id,add_name_id,save_folder,fold_name)\n",
    "fold_to_evaluate=[args.K_fold-1]\n",
    "ds,args,_,_,_ =  get_ds(args_init=args,modification = modification,fold_to_evaluate=fold_to_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load weights of trained model and trasfer learning on the quantile regressor:\n",
    "Il semblerait que les meilleurs résultats obtenues pour le moments soien avec:\n",
    "- reduce_lr = True \n",
    "- freeze = False\n",
    "Donc plutôt ne pas Freeze les couches entrainé. \n",
    "\n",
    "Détail des étapes pour load un modèle, transferer des poids déjà appris (et duppliquer les poids le long des nouveaux output-dim), et fine tuner le model: \n",
    "```\n",
    "model = load_model(ds, args)\n",
    "transfered_model = apply_transfer_learning(model,current_path,save_folder,trial_id,add_name_id,fold_name)\n",
    "trainer = fine_tune_model(transfered_model,ds,args,reduce_lr,freeze,stations_to_plot,training_mode,epochs_fine_tune)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tune the model :\n",
    "- only on the output layers if `freeze = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_fine_tune = 10\n",
    "\n",
    "## Plot Quantificaiton of Uncertainty: \n",
    "stations_to_plot = ['CHA','PER','PAR']\n",
    "training_mode = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze = False\n",
    "reduce_lr = True \n",
    "\n",
    "model = load_model(ds, args)\n",
    "transfered_model = apply_transfer_learning(model,current_path,save_folder,trial_id,add_name_id,fold_name)\n",
    "trainer = fine_tune_model(transfered_model,ds,args,reduce_lr,freeze,stations_to_plot,training_mode,epochs_fine_tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison des résultats avec un training à partir de 0:\n",
    "#### Entrainement complet (100 epochs, plus long: )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modification_bis = {key:value for key,value in modification.items()}\n",
    "trainer,ds,ds_no_shuffle,args = evaluate_config(args.model_name,args.dataset_names,args.dataset_for_coverage,\n",
    "                                                station = stations_to_plot,\n",
    "                                                modification=modification_bis,\n",
    "                                                training_mode_to_visualise=[training_mode],\n",
    "                                                args_init =args,\n",
    "                                                fold_to_evaluate =fold_to_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autant d'epoch que pour fine-tune: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modification_bis = {key:value for key,value in modification.items()}\n",
    "modification_bis.update({'epochs':epochs_fine_tune})\n",
    "trainer,ds,ds_no_shuffle,args = evaluate_config(args.model_name,args.dataset_names,args.dataset_for_coverage,\n",
    "                                                station = stations_to_plot,\n",
    "                                                modification=modification_bis,\n",
    "                                                training_mode_to_visualise=[training_mode],\n",
    "                                                args_init =args,\n",
    "                                                fold_to_evaluate =fold_to_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maintenant Fine-tune sur le modèle qui utilise 'subway-out', voir si il y a toujours des gains : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init\n",
    "save_folder = 'K_fold_validation/training_with_HP_tuning/re_validation'\n",
    "add_name_id = 'concat_early'\n",
    "trial_id ='subway_in_subway_out_STGCN_VariableSelectionNetwork_MSELoss_2025_01_20_05_38_87836' \n",
    "epochs_fine_tune = 10\n",
    "stations_to_plot = ['CHA','PER','PAR']\n",
    "training_mode = 'test'\n",
    "freeze = False\n",
    "reduce_lr = True \n",
    "# ...\n",
    "\n",
    "modification = {'shuffle':True,\n",
    "                'loss_function_type':'quantile',\n",
    "                'alpha':0.05,\n",
    "                'track_pi':True,\n",
    "                'type_calib':'classic',\n",
    "                #'data_augmentation':False\n",
    "                }\n",
    "fold_name = 'complete_dataset'\n",
    "\n",
    "# Load config : \n",
    "args = load_args_of_a_specific_trial(trial_id,add_name_id,save_folder,fold_name)\n",
    "fold_to_evaluate=[args.K_fold-1]\n",
    "ds,args,_,_,_ =  get_ds(args_init=args,modification = modification,fold_to_evaluate=fold_to_evaluate)\n",
    "\n",
    "# Load model and fine tune: \n",
    "model = load_model(ds, args)\n",
    "transfered_model = apply_transfer_learning(model,current_path,save_folder,trial_id,add_name_id,fold_name)\n",
    "trainer = fine_tune_model(transfered_model,ds,args,reduce_lr,freeze,stations_to_plot,training_mode,epochs_fine_tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the transfer learning worked well: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_to_plot = ['CHA','PER','PAR']\n",
    "training_mode = 'test'\n",
    "\n",
    "plot_prediction(trainer,ds,stations_to_plot,training_mode)\n",
    "\n",
    "# Load Trained Model without modificiation :\n",
    "\n",
    "modification = {}\n",
    "fold_name = 'complete_dataset'\n",
    "#args,_ = load_configuration(trial_id1,load_config=True)\n",
    "args_init = load_args_of_a_specific_trial(trial_id,add_name_id,save_folder,fold_name)\n",
    "ds_init,args_init,_,_,_ =  get_ds(args_init=args_init,modification = modification,fold_to_evaluate=[args_init.K_fold-1])\n",
    "model_init = load_model(ds_init, args_init)\n",
    "\n",
    "transfered_model_init = apply_transfer_learning(model_init,current_path,save_folder,trial_id,add_name_id,fold_name)\n",
    "\n",
    "trainer_init = get_trainer(ds_init,transfered_model_init,args_init)\n",
    "\n",
    "plot_prediction(trainer_init,ds_init,stations_to_plot,training_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.0.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
