{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Evalue la qualité de la prédiction d'un modèle / d'une config en regards des métriques obtenues sur différents pas de temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Hyper-parameter tuning with Ray is not possible\n"
     ]
    }
   ],
   "source": [
    "# GET PARAMETERS\n",
    "import os \n",
    "import sys\n",
    "# Get Parent folder : \n",
    "current_path = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_path, '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "from examples.train_and_visu_non_recurrent import evaluate_config\n",
    "from plotting.plotting import error_per_station_calendar_pattern\n",
    "from examples.train_model_on_k_fold_validation import load_configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>> Load best CONFIG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considered Spatial-Unit:  ['BEL', 'PER', 'PAR', 'GER', 'CHA']\n",
      "\n",
      "Init Dataset: 'torch.Size([7392, 5]) with 36960 Total nb of elements and 0 Nan values\n",
      "92 train samples had been added thank to Data Augmentation\n",
      "calendar_dayofweek data augmented by dupplication but not modified\n",
      "calendar_hour data augmented by dupplication but not modified\n",
      "Init U/Utarget size: torch.Size([4702, 5, 7])/torch.Size([4702, 5, 1]) Train/Valid/Test 2913 940 940\n",
      "\n",
      " ===== ERROR ==== \n",
      "Try with torch >= 2.0.0 (works with 2.0.1) to allow 'prefetch_factor' \n",
      "ValueError: prefetch_factor option could only be specified in multiprocessing.let num_workers > 0 to enable multiprocessing\n",
      "\n",
      " ===== ERROR ==== \n",
      "Try with torch >= 2.0.0 (works with 2.0.1) to allow 'prefetch_factor' \n",
      "ValueError: prefetch_factor option could only be specified in multiprocessing.let num_workers > 0 to enable multiprocessing\n",
      "\n",
      " ===== ERROR ==== \n",
      "Try with torch >= 2.0.0 (works with 2.0.1) to allow 'prefetch_factor' \n",
      "ValueError: prefetch_factor option could only be specified in multiprocessing.let num_workers > 0 to enable multiprocessing\n"
     ]
    }
   ],
   "source": [
    "from examples.train_and_visu_non_recurrent import get_ds\n",
    "#trial_id ='subway_in_STGCN_MSELoss_2025_01_06_08_00_94523'\n",
    "trial_id ='subway_in_subway_out_STGCN_VariableSelectionNetwork_MSELoss_2025_01_06_02_04_17963'\n",
    "args,_ = load_configuration(trial_id,load_config=True,epochs=None)\n",
    "\n",
    "\n",
    "station = ['BEL','PAR','PER','FLA']   # 'BON'  #'GER'\n",
    "training_mode_to_visualise = ['test']#,'valid','train']\n",
    "modification ={'keep_best_weights':True,\n",
    "                'epochs':1,\n",
    "                'validation_split_method' : 'forward_chaining_cv',\n",
    "                'min_fold_size_proportion': 0.75,\n",
    "                'train_prop':0.6,\n",
    "                'valid_prop':0.2,\n",
    "                'test_prop':0.2,\n",
    "\n",
    "                'set_spatial_units':['BEL','PER','PAR','GER','CHA'],\n",
    "        \n",
    "                'data_augmentation': True, #True,  #False\n",
    "                'DA_method':'noise', # 'noise' # 'interpolation\n",
    "                'DA_moment_to_focus' :[{'hours':[0,23],'weekdays':[1,3]}], # None\n",
    "                'DA_min_count': 5,\n",
    "                'DA_alpha' : 1,\n",
    "                'DA_prop' : 1,\n",
    "                }\n",
    "ds,args_modif,trial_id,save_folder,df_loss = get_ds(args.model_name,args.dataset_names,args.dataset_for_coverage,\n",
    "                                              modification=modification,args_init=args,fold_to_evaluate=[args.K_fold-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Config from HP tuning\n",
    "```trial_id = 'subway_in_STGCN_MSELoss_2025_01_06_08_00_94523'\n",
    "# \"subway_in_netmob_POIs_STGCN_VariableSelectionNetwork_MSELoss_2025_01_07_23_12_41192\"\n",
    "# \"subway_in_subway_out_STGCN_VariableSelectionNetwork_MSELoss_2025_01_06_02_04_17963\"\n",
    "# 'subway_in_netmob_POIs_STGCN_VariableSelectionNetwork_MSELoss_2025_01_07_05_04_80480'\n",
    "# \"subway_in_STGCN_MSELoss_2025_01_06_08_00_94523\"\n",
    "args_best_model,_ = load_configuration(trial_id,load_config=True,epochs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial_id in ['subway_in_subway_out_STGCN_VariableSelectionNetwork_MSELoss_2025_01_06_02_04_17963',\n",
    "                 'subway_in_STGCN_MSELoss_2025_01_06_08_00_94523',\n",
    "                 #\"subway_in_netmob_POIs_STGCN_VariableSelectionNetwork_MSELoss_2025_01_09_07_54_72902\" # contient  Instagram, Google Maps, Deezer, WhatsApps, Twiteter, DL, UL\n",
    "                 ]:\n",
    "\n",
    "    print('Trial id: ',trial_id)\n",
    "    args_best_model,_ = load_configuration(trial_id,load_config=True,epochs=None)\n",
    "\n",
    "    station = ['BEL','PAR','PER','FLA']   # 'BON'  #'GER'\n",
    "    training_mode_to_visualise = ['test','valid'] # ['test','valid','train']\n",
    "    modification ={'keep_best_weights':True,\n",
    "                   'epochs':100\n",
    "                   }\n",
    "    #modification = {'epochs':1}\n",
    "\n",
    "    (trainer,ds,ds_no_shuffle,args) = evaluate_config(args_best_model.model_name,args_best_model.dataset_names,args_best_model.dataset_for_coverage,\n",
    "                                                    station = station,modification=modification,\n",
    "                                                    training_mode_to_visualise=training_mode_to_visualise,\n",
    "                                                    args_init =args_best_model,\n",
    "                                                    fold_to_evaluate = [0])\n",
    "\n",
    "    # Init\n",
    "    for training_mode in training_mode_to_visualise:\n",
    "        min_flow = 20  # Minimal Flow considered for MAPE, otherwise set error = 0\n",
    "        limit_percentage_error = 200 # 300% plus mauvais que quand on se sert du previous \n",
    "        fig,axes = error_per_station_calendar_pattern(trainer,ds,training_mode,metrics = ['mse','mae','mape','previous_value'],\n",
    "                                                    freq='1h',\n",
    "                                                    min_flow=min_flow,\n",
    "                                                    figsize = (30,5*len(station)),\n",
    "                                                    limit_percentage_error = limit_percentage_error,\n",
    "                                                    stations = station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sans Week-ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial_id in ['subway_in_STGCN_MSELoss_2025_01_06_08_00_94523',\n",
    "                 ]:\n",
    "    print('Trial id: ',trial_id)\n",
    "    args_best_model,_ = load_configuration(trial_id,load_config=True,epochs=None)\n",
    "\n",
    "    station = ['BEL','PAR','PER','FLA']   # 'BON'  #'GER'\n",
    "    training_mode_to_visualise = ['test']#,'valid','train']\n",
    "    modification ={'keep_best_weights':True,\n",
    "                   'epochs':10,\n",
    "                   'validation_split_method' : 'forward_chaining_cv',\n",
    "                   'min_fold_size_proportion': 0.75,\n",
    "                   'train_prop':0.6,\n",
    "                   'valid_prop':0.2,\n",
    "                   'test_prop':0.2,\n",
    "                   'data_augmentation':True,  #False\n",
    "                   'DA_method':'interpolation',\n",
    "                   'DA_moment_to_focus' :[{'hours':[0,23],'weekdays':[1,3]}], # None\n",
    "                   #'W' : args_best_model.Weeks,\n",
    "                   #'D' : args_best_model.Days,\n",
    "                   #'historical_length':args_best_model.H\n",
    "                   }\n",
    "    #modification = {'epochs':1}\n",
    "\n",
    "    (trainer,ds,ds_no_shuffle,args) = evaluate_config(args_best_model.model_name,args_best_model.dataset_names,args_best_model.dataset_for_coverage,\n",
    "                                                    station = station,modification=modification,\n",
    "                                                    training_mode_to_visualise=training_mode_to_visualise,\n",
    "                                                    args_init =args_best_model,\n",
    "                                                    fold_to_evaluate = [args_best_model.K_fold-1] #[0]\n",
    "                                                    \n",
    "                                                    )\n",
    "\n",
    "    # Init\n",
    "    for training_mode in training_mode_to_visualise:\n",
    "        min_flow = 40  # Minimal Flow considered for MAPE, otherwise set error = 0\n",
    "        limit_percentage_error = 200 # 300% plus mauvais que quand on se sert du previous \n",
    "        day_date = getattr(ds_no_shuffle.tensor_limits_keeper,f\"df_verif_{training_mode}\").iloc[:,-1].dt.date.unique()\n",
    "        nb_days = (day_date.max()-day_date.min()).days\n",
    "        y_size = 1.2*nb_days #4,15\n",
    "        x_size = 6.8*len(station)\n",
    "        fig,axes = error_per_station_calendar_pattern(trainer,ds_no_shuffle,training_mode,metrics = ['mse','mae','mape','previous_value'],\n",
    "                                                    freq='1h',\n",
    "                                                    min_flow=min_flow,\n",
    "                                                    figsize = (x_size,y_size),\n",
    "                                                    limit_percentage_error = limit_percentage_error,\n",
    "                                                    stations = station,\n",
    "                                                    index_matshow = 'date',\n",
    "                                                    columns_matshow = 'hour')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avec Week-ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial_id in ['subway_in_STGCN_MSELoss_2025_01_06_08_00_94523',\n",
    "                 ]:\n",
    "    print('Trial id: ',trial_id)\n",
    "    args_best_model,_ = load_configuration(trial_id,load_config=True,epochs=None)\n",
    "\n",
    "    station = ['BEL','PAR','PER','GER']   # 'BON'  #'GER'\n",
    "    training_mode_to_visualise = ['test','valid','train']#,'valid','train']\n",
    "    modification ={'keep_best_weights':True,\n",
    "                   'epochs':100,\n",
    "                   'validation_split_method' : 'forward_chaining_cv',\n",
    "                   'min_fold_size_proportion': 0.75,\n",
    "                   'train_prop':0.6,\n",
    "                   'valid_prop':0.2,\n",
    "                   'test_prop':0.2,\n",
    "                   'data_augmentation':True\n",
    "                   }\n",
    "    #modification = {'epochs':1}\n",
    "\n",
    "    (trainer,ds,ds_no_shuffle,args) = evaluate_config(args_best_model.model_name,args_best_model.dataset_names,args_best_model.dataset_for_coverage,\n",
    "                                                    station = station,modification=modification,\n",
    "                                                    training_mode_to_visualise=training_mode_to_visualise,\n",
    "                                                    args_init =args_best_model,\n",
    "                                                    fold_to_evaluate = [args_best_model.K_fold-1] #[0]\n",
    "                                                    \n",
    "                                                    )\n",
    "\n",
    "    # Init\n",
    "    for training_mode in training_mode_to_visualise:\n",
    "        min_flow = 40  # Minimal Flow considered for MAPE, otherwise set error = 0\n",
    "        limit_percentage_error = 200 # 300% plus mauvais que quand on se sert du previous \n",
    "        \n",
    "        # Set figsize:\n",
    "        day_date = getattr(ds_no_shuffle.tensor_limits_keeper,f\"df_verif_{training_mode}\").iloc[:,-1].dt.date.unique()\n",
    "        nb_days = (day_date.max()-day_date.min()).days\n",
    "        y_size = 1.2*nb_days #4,15\n",
    "        x_size = 6.8*len(station)\n",
    "        fig,axes = error_per_station_calendar_pattern(trainer,ds_no_shuffle,training_mode,metrics = ['mse','mae','mape','previous_value'],\n",
    "                                                    freq='1h',\n",
    "                                                    min_flow=min_flow,\n",
    "                                                    figsize = (x_size,y_size),\n",
    "                                                    limit_percentage_error = limit_percentage_error,\n",
    "                                                    stations = station,\n",
    "                                                    index_matshow = 'date',\n",
    "                                                    columns_matshow = 'hour')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial_id in ['subway_in_subway_out_STGCN_VariableSelectionNetwork_MSELoss_2025_01_06_02_04_17963',\n",
    "                 ]:\n",
    "\n",
    "    print('Trial id: ',trial_id)\n",
    "    args_best_model,_ = load_configuration(trial_id,load_config=True,epochs=None)\n",
    "\n",
    "    station = ['BEL','PAR','PER','FLA']   # 'BON'  #'GER'\n",
    "    training_mode_to_visualise = ['test']#,'valid','train']\n",
    "    modification ={'keep_best_weights':True,\n",
    "                   'epochs':100,\n",
    "                   'validation_split_method' : 'forward_chaining_cv',\n",
    "                   'min_fold_size_proportion': 0.75,\n",
    "                   'train_prop':0.6,\n",
    "                   'valid_prop':0.2,\n",
    "                   'test_prop':0.2,\n",
    "                   'data_augmentation':True\n",
    "                   }\n",
    "    #modification = {'epochs':1}\n",
    "\n",
    "    (trainer,ds,ds_no_shuffle,args) = evaluate_config(args_best_model.model_name,args_best_model.dataset_names,args_best_model.dataset_for_coverage,\n",
    "                                                    station = station,modification=modification,\n",
    "                                                    training_mode_to_visualise=training_mode_to_visualise,\n",
    "                                                    args_init =args_best_model,\n",
    "                                                    fold_to_evaluate = [args_best_model.K_fold-1] #[0]\n",
    "                                                    \n",
    "                                                    )\n",
    "\n",
    "    # Init\n",
    "    for training_mode in training_mode_to_visualise:\n",
    "        min_flow = 40  # Minimal Flow considered for MAPE, otherwise set error = 0\n",
    "        limit_percentage_error = 200 # 300% plus mauvais que quand on se sert du previous \n",
    "        fig,axes = error_per_station_calendar_pattern(trainer,ds,training_mode,metrics = ['mse','mae','mape','previous_value'],\n",
    "                                                    freq='1h',\n",
    "                                                    min_flow=min_flow,\n",
    "                                                    figsize = (30,5*len(station)),\n",
    "                                                    limit_percentage_error = limit_percentage_error,\n",
    "                                                    stations = station,\n",
    "                                                    index_matshow = 'date',\n",
    "                                                    columns_matshow = 'hour')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial_id in ['subway_in_netmob_POIs_STGCN_VariableSelectionNetwork_MSELoss_2025_01_07_23_12_41192',\n",
    "                 'subway_in_STGCN_MSELoss_2025_01_06_08_00_94523',\n",
    "                 \"subway_in_subway_out_STGCN_VariableSelectionNetwork_MSELoss_2025_01_06_02_04_17963\",\n",
    "                 #\"subway_in_netmob_POIs_STGCN_VariableSelectionNetwork_MSELoss_2025_01_09_07_54_72902\" # contient  Instagram, Google Maps, Deezer, WhatsApps, Twiteter, DL, UL\n",
    "                 ]:\n",
    "\n",
    "    print('Trial id: ',trial_id)\n",
    "    args_best_model,_ = load_configuration(trial_id,load_config=True,epochs=None)\n",
    "\n",
    "    station = ['BEL','PAR','PER','FLA']   # 'BON'  #'GER'\n",
    "    training_mode_to_visualise = ['test','valid'] # ['test','valid','train']\n",
    "    modification ={'keep_best_weights':True,\n",
    "                   'epochs':100\n",
    "                   }\n",
    "    #modification = {'epochs':1}\n",
    "\n",
    "    (trainer,ds,ds_no_shuffle,args) = evaluate_config(args_best_model.model_name,args_best_model.dataset_names,args_best_model.dataset_for_coverage,\n",
    "                                                    station = station,modification=modification,\n",
    "                                                    training_mode_to_visualise=training_mode_to_visualise,\n",
    "                                                    args_init =args_best_model,\n",
    "                                                    fold_to_evaluate = [0])\n",
    "\n",
    "    # Init\n",
    "    for training_mode in training_mode_to_visualise:\n",
    "        min_flow = 20  # Minimal Flow considered for MAPE, otherwise set error = 0\n",
    "        limit_percentage_error = 200 # 300% plus mauvais que quand on se sert du previous \n",
    "        fig,axes = error_per_station_calendar_pattern(trainer,ds,training_mode,metrics = ['mse','mae','mape','previous_value'],\n",
    "                                                    freq='1h',\n",
    "                                                    min_flow=min_flow,\n",
    "                                                    figsize = (30,5*len(station)),\n",
    "                                                    limit_percentage_error = limit_percentage_error,\n",
    "                                                    stations = station)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.tensor_limits_keeper.df_verif_train.iloc[t+]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for training_mode in ['train']:\n",
    "    min_flow = 20  # Minimal Flow considered for MAPE, otherwise set error = 0\n",
    "    limit_percentage_error = 200 # 300% plus mauvais que quand on se sert du previous \n",
    "    fig,axes = error_per_station_calendar_pattern(trainer,ds,training_mode,metrics = ['mse','mae','mape','previous_value'],\n",
    "                                                freq='1h',\n",
    "                                                min_flow=min_flow,\n",
    "                                                figsize = (30,5*len(station)),\n",
    "                                                limit_percentage_error = limit_percentage_error,\n",
    "                                                stations = station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting.plotting import get_y_size_from_temporal_agg,temporal_aggregation_of_attn_weight\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "def plot_attn_weight(trainer,nb_calendar_data,ds,training_mode = None,temporal_agg = None,save=None,stations= None):\n",
    "\n",
    "    # Load Inputs : \n",
    "    X,Y,X_c,nb_contextual = trainer.load_all_inputs_from_training_mode(training_mode)\n",
    "    \n",
    "\n",
    "    X = X.to(trainer.args.device)\n",
    "    Y = Y.to(trainer.args.device)\n",
    "    X_c = [x_c.to(trainer.args.device)for x_c in X_c]\n",
    "    # Init:\n",
    "    num_heads = trainer.args.args_vision.num_heads\n",
    "    spatial_units = list(ds.spatial_unit)\n",
    "    if stations is not None :\n",
    "        nb_stations_to_plot = len(stations)*num_heads \n",
    "    else :\n",
    "        stations = list(ds.spatial_unit)\n",
    "        nb_stations_to_plot = Y.size(1)*num_heads \n",
    "    num_cols = 4\n",
    "    \n",
    "\n",
    "    nb_rows = (nb_stations_to_plot + num_cols - 1) // num_cols  \n",
    "    y_size = get_y_size_from_temporal_agg(temporal_agg)\n",
    "    #plt.figure(figsize=(5*num_cols,y_size))  \n",
    "    plt.figure(figsize=(5*num_cols*max(1,nb_stations_to_plot//15),int(y_size*max(1,nb_stations_to_plot//num_cols))))\n",
    "\n",
    "\n",
    "    for station_i in range(nb_stations_to_plot//num_heads):\n",
    "\n",
    "        station_ind  = spatial_units.index(stations[station_i])\n",
    "\n",
    "        enhanced_x,attn_weights = trainer.model.netmob_vision.model[station_ind](X[:,station_ind,:],X_c[station_ind+nb_calendar_data],x_known = None)\n",
    "        nb_contextuals = attn_weights.size(-1)\n",
    "\n",
    "        mh_attn_weights_reshaped = attn_weights.squeeze().detach().cpu().numpy()  # Shape [B,num_heads,P]\n",
    "        for head_i in range(num_heads):\n",
    "            if num_heads == 1:\n",
    "                attn_weights_reshaped = mh_attn_weights_reshaped\n",
    "            else:\n",
    "                attn_weights_reshaped = mh_attn_weights_reshaped[:,head_i,:]\n",
    "            \n",
    "\n",
    "            # Temporal Aggregation of attn weight:\n",
    "            attn_weights_reshaped,str_dates = temporal_aggregation_of_attn_weight(attn_weights_reshaped,ds,training_mode,temporal_agg)\n",
    "            ax = plt.subplot(nb_rows, num_cols, (station_i*num_heads)+head_i + 1)  # Créer un subplot\n",
    "\n",
    "            vmin,vmax = 0,min(1,1/(nb_contextuals/3)) \n",
    "            im = ax.imshow(attn_weights_reshaped, cmap='hot', aspect='auto',vmin=vmin,vmax=vmax)\n",
    "            plt.colorbar(im,label='Attention Weight',shrink = 0.25)\n",
    "            \n",
    "            if temporal_agg is None:\n",
    "                plt.title(f'Attention Weight\\nof station {station_ind} ({spatial_units[station_ind]}) head {head_i} \\nfor each sample of the batch')\n",
    "                plt.ylabel('Samples')\n",
    "            else:\n",
    "                plt.title(f'Mean Attention Weight\\nof station {station_i}({spatial_units[station_ind]}) head {head_i} \\nby calendar class') \n",
    "                plt.ylabel('Calendar class')\n",
    "            plt.xlabel('Contextual time-series')\n",
    "\n",
    "\n",
    "            num_samples, nb_contextual_on_plot_i = attn_weights_reshaped.shape\n",
    "            plt.xticks(ticks=np.arange(nb_contextual_on_plot_i), labels=[f'Unit {i}' for i in range(nb_contextual_on_plot_i)], rotation=45)\n",
    "            plt.yticks(ticks=np.arange(num_samples), labels=str_dates)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "    if save is not None:\n",
    "        plt.savefig(f'{save}.pdf',format = 'pdf',bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def temporal_aggregation_of_attn_weight(attn_weights_reshaped,ds,training_mode,temporal_agg):\n",
    "    ''' \n",
    "    Return the temporal aggregation of attn weights to visualise them \n",
    "\n",
    "    args:\n",
    "    ------\n",
    "    temporal_agg : choices ['hour','weekday','weekday_hour','weekday_hour_minutes']\n",
    "    '''\n",
    "\n",
    "    if temporal_agg is not None:\n",
    "        index_df = getattr(ds.tensor_limits_keeper,f\"df_verif_{training_mode}\").iloc[:,-1]\n",
    "        df = pd.DataFrame(attn_weights_reshaped,index = index_df) #,columns = ds.spatial_unit)\n",
    "        weekdays = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "\n",
    "        if temporal_agg == 'hour':\n",
    "            df_agg = df.groupby([df.index.hour]).agg('mean')\n",
    "            str_dates = list(df_agg.index.map(lambda x: f\"{x:02d}\"))\n",
    "\n",
    "        elif temporal_agg == 'weekday':\n",
    "            df_agg = df.groupby([df.index.weekday]).agg('mean')\n",
    "            str_dates = list(df_agg.index.map(lambda x: weekdays[x]))\n",
    "\n",
    "        elif temporal_agg == 'weekday_hour':\n",
    "            df_agg = df.groupby([df.index.weekday,df.index.hour]).agg('mean')\n",
    "            str_dates = list(df_agg.index.map(lambda x: f\"{weekdays[x[0]]} {x[1]:02d}\"))\n",
    "\n",
    "        elif temporal_agg == 'weekday_hour_minute':\n",
    "            df_agg = df.groupby([df.index.weekday,df.index.hour,df.index.minute]).agg('mean')\n",
    "            str_dates = list(df_agg.index.map(lambda x: f\"{weekdays[x[0]]} {x[1]:02d}:{x[2]:02d}\"))\n",
    "        else:\n",
    "            raise NotImplementedError(f'Temporal aggregation {temporal_agg} has not been implemented')\n",
    "        attn_weights_reshaped = df_agg.values  \n",
    "    else:\n",
    "        str_dates = list(df.index.strftime('%Y-%m-%d %H:%M'))\n",
    "\n",
    "    return attn_weights_reshaped,str_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants.paths import CALENDAR_TYPE\n",
    "nb_calendar_data = len(CALENDAR_TYPE)\n",
    "\n",
    "training_mode = 'test'\n",
    "temporal_agg = 'weekday_hour' # 'hour' # 'weekday' # 'weekday_hour'\n",
    "save = 'attn_weight'\n",
    "\n",
    "plot_attn_weight(trainer,nb_calendar_data,ds= ds,training_mode = training_mode,temporal_agg = temporal_agg,save=save,stations= ['CUS','PER','PAR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.netmob_vision.model[0].attention.W_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting.plotting import plot_attn_weight\n",
    "from constants.paths import CALENDAR_TYPE\n",
    "nb_calendar_data = len(CALENDAR_TYPE)\n",
    "\n",
    "training_mode = 'test'\n",
    "temporal_agg = 'weekday_hour' # 'hour' # 'weekday' # 'weekday_hour'\n",
    "save = 'attn_weight'\n",
    "plot_attn_weight(trainer,nb_calendar_data,ds,training_mode,temporal_agg,save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate config :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial_id in ['subway_in_STGCN_MSELoss_2025_01_06_08_00_94523',\n",
    "                 \"subway_in_subway_out_STGCN_VariableSelectionNetwork_MSELoss_2025_01_06_02_04_17963\",\n",
    "                 \"subway_in_netmob_POIs_STGCN_VariableSelectionNetwork_MSELoss_2025_01_07_23_12_41192\"]:\n",
    "\n",
    "    print('Trial id: ',trial_id)\n",
    "    args_best_model,_ = load_configuration(trial_id,load_config=True,epochs=None)\n",
    "\n",
    "    station = ['BEL','PAR','PER','FLA']   # 'BON'  #'GER'\n",
    "    training_mode_to_visualise = ['test','valid'] # ['test','valid','train']\n",
    "    modification ={'keep_best_weights':True}\n",
    "    #modification = {'epochs':1}\n",
    "\n",
    "    (trainer,ds,ds_no_shuffle,args) = evaluate_config(args_best_model.model_name,args_best_model.dataset_names,args_best_model.dataset_for_coverage,\n",
    "                                                    station = station,modification=modification,\n",
    "                                                    training_mode_to_visualise=training_mode_to_visualise,\n",
    "                                                    args_init =args_best_model,\n",
    "                                                    fold_to_evaluate = [0])\n",
    "\n",
    "\n",
    "    # Init\n",
    "    for training_mode in training_mode_to_visualise:\n",
    "        min_flow = 20  # Minimal Flow considered for MAPE, otherwise set error = 0\n",
    "        limit_percentage_error = 200 # 300% plus mauvais que quand on se sert du previous \n",
    "        fig,axes = error_per_station_calendar_pattern(trainer,ds,training_mode,metrics = ['mse','mae','mape','previous_value'],\n",
    "                                                    freq='1h',\n",
    "                                                    min_flow=min_flow,\n",
    "                                                    figsize = (30,5*len(station)),\n",
    "                                                    limit_percentage_error = limit_percentage_error,\n",
    "                                                    stations = station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config defined 'by hand' :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\"subway_in\"] # [\"subway_in\",\"calendar\"] # [\"subway_in\"] # ['data_bidon'] # ['METR_LA'] # ['PEMS_BAY']\n",
    "dataset_for_coverage = ['subway_in','netmob_image_per_station'] #  ['data_bidon','netmob'] #  ['subway_in','netmob']  # ['METR_LA'] # ['PEMS_BAY']\n",
    "model_name = 'STGCN'\n",
    "\n",
    "station = ['BEL','PAR','AMP','FLA']   # 'BON'  #'GER'\n",
    "# ...\n",
    "\n",
    "modification = {'epochs' : 1, #100\n",
    "                'lr':4e-4}\n",
    "training_mode_to_visualise = ['test']\n",
    "\n",
    "(trainer,ds,ds_no_shuffle,args) = evaluate_config(model_name,dataset_names,dataset_for_coverage,\n",
    "                                                   station = station,modification=modification,training_mode_to_visualise=training_mode_to_visualise)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.0.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
