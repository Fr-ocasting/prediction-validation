{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.profiler import profile, tensorboard_trace_handler, ProfilerActivity, schedule\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp \n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time \n",
    "import os \n",
    "\n",
    "from dl_models.CNN_based_model import CNN\n",
    "\n",
    "import psutil \n",
    "\n",
    "\n",
    "# ============ Profiler Memory Usage ============\n",
    "def get_memory_usage(max_memory):\n",
    "    print(f\"\\nMax GPU memory allocated: {torch.cuda.max_memory_allocated() / 1024**3} GB\")\n",
    "    print(f\"Max GPU memory cached: {torch.cuda.max_memory_reserved() / 1024**3} GB\")\n",
    "    print(f\"Max CPU memory allocated: {max_memory} GB\")\n",
    "# ============ .... ============    \n",
    "\n",
    "# ============ DDP ============\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = '137.121.170.69'\n",
    "    #os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "# ============ .... ============   \n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,X,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,c_in = 4,h_dim1 = 16, h_dim2 = 16, c_out = 1):\n",
    "        super(MLP,self).__init__()\n",
    "        self.linear1 = nn.Linear(c_in,h_dim1)\n",
    "        self.linear2 = nn.Linear(h_dim1,h_dim2)\n",
    "        self.linear3 = nn.Linear(h_dim2,c_out)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        return(self.linear3(self.relu(self.linear2(self.relu(self.linear1(x))))))\n",
    "    \n",
    "def load_profile_dataloader(dataset,B,num_workers,persistent_workers,pin_memory,prefetch_factor,drop_last,dataparallel=False,prof = False):\n",
    "    if prof:\n",
    "        activities = [ProfilerActivity.CPU, ProfilerActivity.CUDA] if torch.cuda.is_available() else [ProfilerActivity.CPU]\n",
    "        prof =  profile(activities=activities,\n",
    "                        schedule=schedule(wait=1, warmup=1, active=12, repeat=1),\n",
    "                        on_trace_ready=tensorboard_trace_handler('./profiler/trial_profiler'),\n",
    "                        profile_memory=True,\n",
    "                        record_shapes=False, \n",
    "                        with_stack=False,\n",
    "                        with_flops=False\n",
    "                        )\n",
    "    else:\n",
    "        prof = None\n",
    "    \n",
    "    if dataparallel:\n",
    "        sampler = torch.utils.data.distributed.DistributedSampler(dataset,\n",
    "                                                                num_replicas=2,\n",
    "                                                                rank=0,\n",
    "                                                                shuffle=True)\n",
    "    else: \n",
    "        sampler = None\n",
    "        \n",
    "    \n",
    "\n",
    "    dataloader = DataLoader(dataset,batch_size=B,shuffle =False if dataparallel else True ,\n",
    "                            num_workers=num_workers, \n",
    "                            persistent_workers= False if num_workers == 0 else persistent_workers,\n",
    "                            pin_memory=pin_memory,\n",
    "                            prefetch_factor=None if num_workers==0 else prefetch_factor, #2,3,4,5...\n",
    "                            drop_last=drop_last,\n",
    "                           sampler = sampler \n",
    "                           )\n",
    "    \n",
    "    return(prof,dataloader,sampler)\n",
    "\n",
    "\n",
    "def load_model_loss_opt(c_in=1,h_dim1=16,h_dim2=16,c_out=1,device='cuda',dataparallel = False,model_name = 'MLP',H_dims =[64,128,64],C_outs = [64,32,1],kernel_size= (1,2),input_shape = [6000,40,8]\n",
    "                        ,compile_model=False,memory_format_last = False, compile_backend = 'inductor', rank = 0, world_size = 2):\n",
    "    loss = nn.MSELoss()\n",
    "    if model_name == 'MLP': \n",
    "        model = MLP(c_in,h_dim1, h_dim2, c_out).to(device)\n",
    "    if model_name == 'CNN':\n",
    "        model = CNN(c_in,L,H_dims, C_outs, kernel_size, input_shape = input_shape).to(device)\n",
    "        if memory_format_last:\n",
    "            model = model.to(memory_format = torch.channels_last)\n",
    "        \n",
    "    # DataParallel:\n",
    "    if dataparallel:\n",
    "        dist.init_process_group(backend='nccl',# init_method='env://',\n",
    "                                world_size=world_size, \n",
    "                                rank=rank)\n",
    "        model = DDP(model,device_ids = [rank])\n",
    "    # ....\n",
    "    \n",
    "    # Compiler :\n",
    "    if compile_model:\n",
    "        model = torch.compile(model,backend = compile_backend)\n",
    "        #model = torch.jit.script(model)\n",
    "    # ...\n",
    "        \n",
    "    optimizer = torch.optim.SGD(model.parameters(), 1e-3)\n",
    "    \n",
    "    print('number of total parameters: {}'.format(sum([p.numel() for p in model.parameters()])))\n",
    "    print('number of trainable parameters: {}'.format(sum([p.numel() for p in model.parameters() if p.requires_grad])))\n",
    "\n",
    "    return(loss,model,optimizer)\n",
    "\n",
    "\n",
    "def training(model,optimizer,loss,prof,epochs,dataloader,device,scaler,memory_format_last,mixed_precision):\n",
    "    global max_memory\n",
    "    t_epochs,t_batchs, t_coms, t_forwards, t_backwards,total_time = 0,0,0,0,0,0\n",
    "    for epoch in range(epochs):\n",
    "        if sampler is not None:\n",
    "            sampler.set_epoch(epoch)\n",
    "        epoch1 = time.time()\n",
    "        t_epoch = time.time()\n",
    "        for x,y in dataloader:\n",
    "            \n",
    "            # ==== Mesure le temps de lecture CPU -> GPU\n",
    "            t_com = time.time()\n",
    "            if  memory_format_last:\n",
    "                x,y = x.to(device,memory_format = torch.channels_last),x.to(device)\n",
    "            else:\n",
    "                x,y = x.to(device),y.to(device)\n",
    "            t_coms += time.time()-t_com\n",
    "            # ==== ...\n",
    "            \n",
    "            \n",
    "            # ==== Mesure le temps total d'entrainement des batch (forward + backward + Optimizer update):\n",
    "            t_batch = time.time()\n",
    "\n",
    "            if mixed_precision:\n",
    "                with autocast():\n",
    "                    # Mesure temps de backward: \n",
    "                    t_forward = time.time()\n",
    "                    pred = model(x)\n",
    "                    t_forwards += time.time()-t_forward\n",
    "                    # ...\n",
    "                        \n",
    "                    if len(y.size()) != len(pred.size()): \n",
    "                        pred = pred.squeeze()   \n",
    "                        \n",
    "                    # Mesure temps de backward: \n",
    "                    t_backward = time.time()\n",
    "                    l = loss(pred,y)\n",
    "                    t_backwards += time.time() - t_backward\n",
    "                    # ...    \n",
    "    \n",
    "            else:\n",
    "                # Mesure temps de backward: \n",
    "                t_forward = time.time()\n",
    "                pred = model(x)\n",
    "                t_forwards += time.time()-t_forward\n",
    "                # ...\n",
    "                        \n",
    "                if len(y.size()) != len(pred.size()): \n",
    "                    pred = pred.squeeze()\n",
    "                \n",
    "                # Mesure temps de backward: \n",
    "                t_backward = time.time()\n",
    "                l = loss(pred,y)\n",
    "                t_backwards += time.time() - t_backward\n",
    "                # ...\n",
    "            \n",
    "            if scaler is not None:\n",
    "                scaler.scale(l).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                l.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            t_batchs += time.time()-t_batch\n",
    "            # ==== ...\n",
    "            \n",
    "        t_epochs += time.time()-t_epoch\n",
    "        if epoch == 0:\n",
    "            epochs1 = time.time()-epoch1\n",
    "        if prof:\n",
    "            prof.step()\n",
    "            \n",
    "        process = psutil.Process()\n",
    "        max_memory = max(max_memory, process.memory_info().rss / 1024**3)\n",
    "    get_memory_usage(max_memory)\n",
    "    return(t_epochs,t_batchs,epochs1,t_coms,t_forwards,t_backwards)\n",
    "\n",
    "def train_model(model,optimizer,loss,prof,epochs,dataloader,sampler,device,T,memory_format_last,mixed_precision):\n",
    "    total_t = time.time()\n",
    "    if sampler is not None:\n",
    "        scaler = GradScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "    if prof is not None:\n",
    "        with prof:\n",
    "            (t_epochs,epochs1,t_coms,t_forwards,t_backwards) = training(model,optimizer,loss,prof,epochs,dataloader,device,scaler,memory_format_last,mixed_precision)\n",
    "    else:\n",
    "        (t_epochs,t_batchs,epochs1,t_coms,t_forwards,t_backwards) = training(model,optimizer,loss,prof,epochs,dataloader,device,scaler,memory_format_last,mixed_precision)\n",
    "        \n",
    "    total_time = time.time() - total_t\n",
    "    throughput = f\"{'{:.0f}'.format(T*epochs/total_time)} sequences /s \"\n",
    "\n",
    "    print(f\"Throughput: {throughput} \\nTotal time: {total_time} \\nTime per epoch: {(t_epochs)/(epochs-1)} \\\n",
    "    \\nTotal TimeEpoch {t_epochs} and Total TimeBatch: {t_batchs}. Difference from CPU dataloader : {t_epochs-t_batchs} \\\n",
    "        \\nTime first epoch: {(epochs1)} \\nTime Communication: {t_coms}\\\n",
    "            \\nTime forwards: {t_forwards} \\nTime Backward: {t_backwards}\\\n",
    "            \")\n",
    "    \n",
    "    \n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,c_in,L,H_dims =[64,128,64], C_outs = [64,32,1], kernel_size = (1,2),dilation = 1, stride = 1,args_embedding = None,dic_class2rpz=None, input_shape = [6000,40,8]):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.c_out = C_outs[-1]\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        if len(input_shape) == 4:\n",
    "            self.Convs = nn.ModuleList([nn.Conv2d(c_in_, c_out_, kernel_size,padding=0,dilation=dilation) for c_in_,c_out_ in zip([c_in]+H_dims[:-1], H_dims)])\n",
    "            l_out_add = (2*0 - dilation*(kernel_size[1]-1) -1)/stride + 1\n",
    "        if len(input_shape)== 3:\n",
    "            self.Convs = nn.ModuleList([nn.Conv1d(c_in_, c_out_, kernel_size,padding=0,dilation=dilation) for c_in_,c_out_ in zip([c_in]+H_dims[:-1], H_dims)])\n",
    "            l_out_add = (2*0 - dilation*(kernel_size[0]-1) -1)/stride + 1\n",
    "\n",
    "        \n",
    "        l_out = int(L/stride**len(H_dims) + sum([l_out_add/stride**k for k in range(len(H_dims))])) \n",
    "        \n",
    "        self.l_out = l_out\n",
    "\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "\n",
    "        self.Dense_outs = nn.ModuleList([nn.Linear(c_in_,c_out_) for c_in_,c_out_ in zip([l_out*H_dims[-1]]+C_outs[:-1], C_outs)])\n",
    "\n",
    "            \n",
    "    def forward(self,x):\n",
    "        if len(x.shape) == 3:\n",
    "            B,N,L = x.shape\n",
    "            C = 1\n",
    "            x = x.unsqueeze(2)\n",
    "            x = x.reshape(B*N,C,L)\n",
    "        if len(x.shape) == 4:\n",
    "            B,C,N,L = x.shape\n",
    "            x.reshape(B*N,C,L)\n",
    "             \n",
    "        # Conv Layers :        \n",
    "        for conv in self.Convs:\n",
    "            x = self.dropout(self.relu(conv(x)))\n",
    "\n",
    "        # Flatten :\n",
    "        if False:\n",
    "            x = x.permute(0,2,1,3)\n",
    "            x = x.reshape(x.shape[0]*x.shape[1],-1)\n",
    "        x = self.flatten(x)\n",
    "        # Output Module : \n",
    "        for dense_out in self.Dense_outs[:-1]:\n",
    "            x = self.dropout(self.relu(dense_out(x)))\n",
    "\n",
    "        x = self.Dense_outs[-1](x)    # No activation\n",
    "        # Reshape \n",
    "        x = x.reshape(B,N,self.c_out)\n",
    "        return(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test CNN. Profiling Code. Pourquoi c'est si long sur mon framework ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of total parameters: 55809\n",
      "number of trainable parameters: 55809\n",
      "\n",
      "Max GPU memory allocated: 0.2066488265991211 GB\n",
      "Max GPU memory cached: 0.298828125 GB\n",
      "Max CPU memory allocated: 3.2891807556152344 GB\n",
      "Throughput: 23792 sequences /s  \n",
      "Total time: 12.609472274780273 \n",
      "Time per epoch: 0.2571245018316775     \n",
      "Total TimeEpoch 12.599100589752197 and Total TimeBatch: 6.229737758636475. Difference from CPU dataloader : 6.369362831115723         \n",
      "Time first epoch: 1.9661011695861816 \n",
      "Time Communication: 4.335877180099487            \n",
      "Time forwards: 2.5469892024993896 \n",
      "Time Backward: 0.09984564781188965            \n"
     ]
    }
   ],
   "source": [
    "model_name = 'CNN'\n",
    "\n",
    "\n",
    "B = 256 #8\n",
    "T = 6000\n",
    "L = 8\n",
    "N = 40\n",
    "epochs = 50\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "c_in,h_dim1,h_dim2,c_out = L, 64, 64, 1\n",
    "\n",
    "# Profiling: \n",
    "max_memory = 0\n",
    "# ...\n",
    "    \n",
    "    \n",
    "# DataLoader: \n",
    "num_workers = 0\n",
    "persistent_workers = False\n",
    "pin_memory = False\n",
    "prefetch_factor = None\n",
    "drop_last = False\n",
    "# ....\n",
    "\n",
    "# DataParallel:\n",
    "dataparallel = False\n",
    "# ...\n",
    "\n",
    "# Compiler :\n",
    "compile_model = False\n",
    "# ...\n",
    "\n",
    "# Memory format = channel_last:\n",
    "memory_format_last = False\n",
    "# ....\n",
    "\n",
    "\n",
    "# Mixed-Precision : FP16 Tesor Core\n",
    "# mplémenter l'autocasting (le changement de précision, FP32 à FP16) dans le forward , avec la ligne with autocast(): dans la boucle de TRAINING ET la boucle de VALIDATION\n",
    "mixed_precision = False\n",
    "# ...\n",
    "\n",
    "\n",
    "# Inputs : \n",
    "X,Y=  torch.randn(T,N,L),torch.randn(T,N)\n",
    "inputs = CustomDataset(X,Y)\n",
    "# ....\n",
    "\n",
    "\n",
    "(prof,dataloader,sampler) = load_profile_dataloader(inputs,B,num_workers,persistent_workers,pin_memory,prefetch_factor,drop_last)\n",
    "if model_name == 'MLP':\n",
    "    (loss,model,optimizer) = load_model_loss_opt(c_in,h_dim1,h_dim2,c_out,device,model_name =model_name,compile_model = compile_model)\n",
    "if model_name == 'CNN':\n",
    "    # kernel_size= (1,2)\n",
    "    kernel_size = (2,)\n",
    "    (loss,model,optimizer) = load_model_loss_opt(H_dims =[64,128,64],C_outs = [64,32,1],kernel_size= kernel_size,c_in=1,device=device,dataparallel = dataparallel,model_name =model_name,input_shape = [6000,40,8],\n",
    "                                                 compile_model = compile_model,memory_format_last = memory_format_last)\n",
    "\n",
    "train_model(model,optimizer,loss,prof,epochs,dataloader,sampler,device,T,memory_format_last,mixed_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajout de la mixed Precision : \n",
    "Aucun changement pour des petits batch size (8). Mais x2 pour un batch-size plus grand ! (256)\n",
    "Etonnament, alors que 'with autocast():' n'est pas censé être compris dans le chargement du dataloader, on remarque un temps bien plus court pour çalorsque la mixed precision est activée (et pas pour le forward + calcul de loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of total parameters: 55809\n",
      "number of trainable parameters: 55809\n",
      "\n",
      "Max GPU memory allocated: 0.2066488265991211 GB\n",
      "Max GPU memory cached: 0.30078125 GB\n",
      "Max CPU memory allocated: 3.300487518310547 GB\n",
      "Throughput: 33981 sequences /s  \n",
      "Total time: 8.828543663024902 \n",
      "Time per epoch: 0.1799228288689438     \n",
      "Total TimeEpoch 8.816218614578247 and Total TimeBatch: 5.945619106292725. Difference from CPU dataloader : 2.8705995082855225         \n",
      "Time first epoch: 0.19704151153564453 \n",
      "Time Communication: 0.26018214225769043            \n",
      "Time forwards: 1.5356342792510986 \n",
      "Time Backward: 0.15899991989135742            \n"
     ]
    }
   ],
   "source": [
    "# Profiling: \n",
    "max_memory = 0\n",
    "# ...\n",
    "    \n",
    "    \n",
    "# Mixed-Precision : FP16 Tesor Core\n",
    "mixed_precision = True\n",
    "# ...\n",
    "\n",
    "\n",
    "# Inputs : \n",
    "X,Y=  torch.randn(T,N,L),torch.randn(T,N)\n",
    "inputs = CustomDataset(X,Y)\n",
    "# ....\n",
    "\n",
    "\n",
    "(prof,dataloader,sampler) = load_profile_dataloader(inputs,B,num_workers,persistent_workers,pin_memory,prefetch_factor,drop_last)\n",
    "(loss,model,optimizer) = load_model_loss_opt(H_dims =[64,128,64],C_outs = [64,32,1],kernel_size= kernel_size,c_in=1,device=device,dataparallel = dataparallel,model_name =model_name,input_shape = [6000,40,8],\n",
    "                                                 compile_model = compile_model,memory_format_last = memory_format_last)\n",
    "train_model(model,optimizer,loss,prof,epochs,dataloader,sampler,device,T,memory_format_last,mixed_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajout de Num workers: \n",
    "Pour un petit batch, num_worker = 1 semble meilleur (x2). Pour un Gros Batch, pas du tout, baisse de throughput ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      "Num workers: 0\n",
      "number of total parameters: 55809\n",
      "number of trainable parameters: 55809\n",
      "\n",
      "Max GPU memory allocated: 0.2066488265991211 GB\n",
      "Max GPU memory cached: 0.30078125 GB\n",
      "Max CPU memory allocated: 3.305156707763672 GB\n",
      "Throughput: 29619 sequences /s  \n",
      "Total time: 10.128560304641724 \n",
      "Time per epoch: 0.20653580159557108     \n",
      "Total TimeEpoch 10.120254278182983 and Total TimeBatch: 2.8240091800689697. Difference from CPU dataloader : 7.296245098114014         \n",
      "Time first epoch: 0.21078705787658691 \n",
      "Time Communication: 5.36291241645813            \n",
      "Time forwards: 1.031101942062378 \n",
      "Time Backward: 0.10753703117370605            \n",
      "------------------------------\n",
      "\n",
      "Num workers: 1\n",
      "number of total parameters: 55809\n",
      "number of trainable parameters: 55809\n",
      "\n",
      "Max GPU memory allocated: 0.2066488265991211 GB\n",
      "Max GPU memory cached: 0.30078125 GB\n",
      "Max CPU memory allocated: 3.305248260498047 GB\n",
      "Throughput: 26092 sequences /s  \n",
      "Total time: 11.4976167678833 \n",
      "Time per epoch: 0.23449512890407018     \n",
      "Total TimeEpoch 11.490261316299438 and Total TimeBatch: 4.034123659133911. Difference from CPU dataloader : 7.456137657165527         \n",
      "Time first epoch: 0.3088259696960449 \n",
      "Time Communication: 6.269730091094971            \n",
      "Time forwards: 0.9192049503326416 \n",
      "Time Backward: 0.07871532440185547            \n",
      "------------------------------\n",
      "\n",
      "Num workers: 2\n",
      "number of total parameters: 55809\n",
      "number of trainable parameters: 55809\n",
      "\n",
      "Max GPU memory allocated: 0.2066488265991211 GB\n",
      "Max GPU memory cached: 0.30078125 GB\n",
      "Max CPU memory allocated: 3.305267333984375 GB\n",
      "Throughput: 25388 sequences /s  \n",
      "Total time: 11.816447019577026 \n",
      "Time per epoch: 0.24098558815158144     \n",
      "Total TimeEpoch 11.80829381942749 and Total TimeBatch: 4.120935916900635. Difference from CPU dataloader : 7.6873579025268555         \n",
      "Time first epoch: 0.35427141189575195 \n",
      "Time Communication: 6.369086503982544            \n",
      "Time forwards: 1.049985647201538 \n",
      "Time Backward: 0.11220455169677734            \n"
     ]
    }
   ],
   "source": [
    "# Profiling: \n",
    "max_memory = 0\n",
    "# ...\n",
    "    \n",
    "\n",
    "# DataLoader: \n",
    "persistent_workers = True\n",
    "pin_memory = True\n",
    "prefetch_factor = 3\n",
    "drop_last = False\n",
    "# ....\n",
    "\n",
    "# Mixed-Precision : FP16 Tesor Core\n",
    "mixed_precision = False\n",
    "# ...\n",
    "\n",
    "# Inputs : \n",
    "X,Y=  torch.randn(T,N,L),torch.randn(T,N)\n",
    "inputs = CustomDataset(X,Y)\n",
    "# ....\n",
    "\n",
    "for num_workers in [0,1,2]:\n",
    "    print(f'------------------------------')\n",
    "    print(f'\\nNum workers: {num_workers}')\n",
    "    (prof,dataloader,sampler) = load_profile_dataloader(inputs,B,num_workers,persistent_workers,pin_memory,prefetch_factor,drop_last)\n",
    "    (loss,model,optimizer) = load_model_loss_opt(H_dims =[64,128,64],C_outs = [64,32,1],kernel_size= kernel_size,c_in=1,device=device,dataparallel = dataparallel,model_name =model_name,input_shape = [6000,40,8],\n",
    "                                                     compile_model = compile_model,memory_format_last = memory_format_last)\n",
    "\n",
    "    train_model(model,optimizer,loss,prof,epochs,dataloader,sampler,device,T,memory_format_last,mixed_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choix de num_workers avec la mixed precision activée:\n",
    "Visiblement, il vaut mieux utiliser 1 Worker avec la mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      "Num workers: 0\n",
      "number of total parameters: 55809\n",
      "number of trainable parameters: 55809\n",
      "\n",
      "Max GPU memory allocated: 0.2066488265991211 GB\n",
      "Max GPU memory cached: 0.30078125 GB\n",
      "Max CPU memory allocated: 3.3061561584472656 GB\n",
      "Throughput: 47927 sequences /s  \n",
      "Total time: 6.259514093399048 \n",
      "Time per epoch: 0.12758264736253389     \n",
      "Total TimeEpoch 6.25154972076416 and Total TimeBatch: 3.850377321243286. Difference from CPU dataloader : 2.401172399520874         \n",
      "Time first epoch: 0.14849138259887695 \n",
      "Time Communication: 0.10798120498657227            \n",
      "Time forwards: 1.3080604076385498 \n",
      "Time Backward: 0.13345575332641602            \n",
      "------------------------------\n",
      "\n",
      "Num workers: 1\n",
      "number of total parameters: 55809\n",
      "number of trainable parameters: 55809\n",
      "\n",
      "Max GPU memory allocated: 0.2066488265991211 GB\n",
      "Max GPU memory cached: 0.30078125 GB\n",
      "Max CPU memory allocated: 3.3061752319335938 GB\n",
      "Throughput: 59691 sequences /s  \n",
      "Total time: 5.0258495807647705 \n",
      "Time per epoch: 0.10242202817177286     \n",
      "Total TimeEpoch 5.01867938041687 and Total TimeBatch: 3.6973440647125244. Difference from CPU dataloader : 1.3213353157043457         \n",
      "Time first epoch: 0.19508647918701172 \n",
      "Time Communication: 0.3265371322631836            \n",
      "Time forwards: 1.190375804901123 \n",
      "Time Backward: 0.1380774974822998            \n",
      "------------------------------\n",
      "\n",
      "Num workers: 2\n",
      "number of total parameters: 55809\n",
      "number of trainable parameters: 55809\n",
      "\n",
      "Max GPU memory allocated: 0.2066488265991211 GB\n",
      "Max GPU memory cached: 0.30078125 GB\n",
      "Max CPU memory allocated: 3.306243896484375 GB\n",
      "Throughput: 47883 sequences /s  \n",
      "Total time: 6.265251398086548 \n",
      "Time per epoch: 0.1276261806488037     \n",
      "Total TimeEpoch 6.253682851791382 and Total TimeBatch: 4.602159738540649. Difference from CPU dataloader : 1.6515231132507324         \n",
      "Time first epoch: 0.22296643257141113 \n",
      "Time Communication: 0.31322312355041504            \n",
      "Time forwards: 1.6359035968780518 \n",
      "Time Backward: 0.15229511260986328            \n"
     ]
    }
   ],
   "source": [
    "# Profiling: \n",
    "max_memory = 0\n",
    "# ...\n",
    "    \n",
    "# DataLoader: \n",
    "persistent_workers = True\n",
    "pin_memory = True\n",
    "prefetch_factor = 3\n",
    "drop_last = False\n",
    "# ....\n",
    "\n",
    "# Mixed-Precision : FP16 Tesor Core\n",
    "mixed_precision = True\n",
    "# ...\n",
    "\n",
    "# Inputs : \n",
    "X,Y=  torch.randn(T,N,L),torch.randn(T,N)\n",
    "inputs = CustomDataset(X,Y)\n",
    "# ....\n",
    "\n",
    "for num_workers in [0,1,2]:\n",
    "    print(f'------------------------------')\n",
    "    print(f'\\nNum workers: {num_workers}')\n",
    "    (prof,dataloader,sampler) = load_profile_dataloader(inputs,B,num_workers,persistent_workers,pin_memory,prefetch_factor,drop_last)\n",
    "    (loss,model,optimizer) = load_model_loss_opt(H_dims =[64,128,64],C_outs = [64,32,1],kernel_size= kernel_size,c_in=1,device=device,dataparallel = dataparallel,model_name =model_name,input_shape = [6000,40,8],\n",
    "                                                     compile_model = compile_model,memory_format_last = memory_format_last)\n",
    "\n",
    "    train_model(model,optimizer,loss,prof,epochs,dataloader,sampler,device,T,memory_format_last,mixed_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choix de Batch-size: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      "Batch size: 8\n",
      "number of total parameters: 55809\n",
      "number of trainable parameters: 55809\n",
      "\n",
      "Max GPU memory allocated: 0.2066488265991211 GB\n",
      "Max GPU memory cached: 0.30078125 GB\n",
      "Max CPU memory allocated: 3.314830780029297 GB\n",
      "Throughput: 2437 sequences /s  \n",
      "Total time: 123.12023329734802 \n",
      "Time per epoch: 2.5123647232444917     \n",
      "Total TimeEpoch 123.1058714389801 and Total TimeBatch: 111.8541350364685. Difference from CPU dataloader : 11.251736402511597         \n",
      "Time first epoch: 2.1629693508148193 \n",
      "Time Communication: 6.7922515869140625            \n",
      "Time forwards: 38.89456915855408 \n",
      "Time Backward: 4.156979322433472            \n",
      "------------------------------\n",
      "\n",
      "Batch size: 16\n",
      "number of total parameters: 55809\n",
      "number of trainable parameters: 55809\n",
      "\n",
      "Max GPU memory allocated: 0.2066488265991211 GB\n",
      "Max GPU memory cached: 0.302734375 GB\n",
      "Max CPU memory allocated: 3.3161048889160156 GB\n",
      "Throughput: 4542 sequences /s  \n",
      "Total time: 66.05665755271912 \n",
      "Time per epoch: 1.347802566022289     \n",
      "Total TimeEpoch 66.04232573509216 and Total TimeBatch: 59.989179849624634. Difference from CPU dataloader : 6.053145885467529         \n",
      "Time first epoch: 1.4129767417907715 \n",
      "Time Communication: 3.4564437866210938            \n",
      "Time forwards: 21.28163480758667 \n",
      "Time Backward: 2.140634536743164            \n",
      "------------------------------\n",
      "\n",
      "Batch size: 32\n",
      "number of total parameters: 55809\n",
      "number of trainable parameters: 55809\n",
      "\n",
      "Max GPU memory allocated: 0.2066488265991211 GB\n",
      "Max GPU memory cached: 0.302734375 GB\n",
      "Max CPU memory allocated: 3.3167686462402344 GB\n",
      "Throughput: 8850 sequences /s  \n",
      "Total time: 33.89673662185669 \n",
      "Time per epoch: 0.6915066242218018     \n",
      "Total TimeEpoch 33.883824586868286 and Total TimeBatch: 30.35598063468933. Difference from CPU dataloader : 3.527843952178955         \n",
      "Time first epoch: 0.9204983711242676 \n",
      "Time Communication: 1.8165655136108398            \n",
      "Time forwards: 10.421547889709473 \n",
      "Time Backward: 0.9784953594207764            \n",
      "------------------------------\n",
      "\n",
      "Batch size: 64\n",
      "number of total parameters: 55809\n",
      "number of trainable parameters: 55809\n",
      "\n",
      "Max GPU memory allocated: 0.2066488265991211 GB\n",
      "Max GPU memory cached: 0.302734375 GB\n",
      "Max CPU memory allocated: 3.318988800048828 GB\n",
      "Throughput: 16681 sequences /s  \n",
      "Total time: 17.984049081802368 \n",
      "Time per epoch: 0.36672792142751265     \n",
      "Total TimeEpoch 17.96966814994812 and Total TimeBatch: 15.791074991226196. Difference from CPU dataloader : 2.178593158721924         \n",
      "Time first epoch: 0.6061294078826904 \n",
      "Time Communication: 0.8485314846038818            \n",
      "Time forwards: 5.643615245819092 \n",
      "Time Backward: 0.5431842803955078            \n",
      "------------------------------\n",
      "\n",
      "Batch size: 128\n",
      "number of total parameters: 55809\n",
      "number of trainable parameters: 55809\n",
      "\n",
      "Max GPU memory allocated: 0.2066488265991211 GB\n",
      "Max GPU memory cached: 0.302734375 GB\n",
      "Max CPU memory allocated: 3.32025146484375 GB\n",
      "Throughput: 32676 sequences /s  \n",
      "Total time: 9.18106484413147 \n",
      "Time per epoch: 0.18717668494399714     \n",
      "Total TimeEpoch 9.17165756225586 and Total TimeBatch: 7.5964674949646. Difference from CPU dataloader : 1.5751900672912598         \n",
      "Time first epoch: 0.36867785453796387 \n",
      "Time Communication: 0.49196910858154297            \n",
      "Time forwards: 2.8869237899780273 \n",
      "Time Backward: 0.2524096965789795            \n",
      "------------------------------\n",
      "\n",
      "Batch size: 256\n",
      "number of total parameters: 55809\n",
      "number of trainable parameters: 55809\n",
      "\n",
      "Max GPU memory allocated: 0.2066488265991211 GB\n",
      "Max GPU memory cached: 0.302734375 GB\n",
      "Max CPU memory allocated: 3.32025146484375 GB\n",
      "Throughput: 49562 sequences /s  \n",
      "Total time: 6.053084373474121 \n",
      "Time per epoch: 0.1233004988456259     \n",
      "Total TimeEpoch 6.041724443435669 and Total TimeBatch: 4.400684595108032. Difference from CPU dataloader : 1.6410398483276367         \n",
      "Time first epoch: 0.24274158477783203 \n",
      "Time Communication: 0.3154778480529785            \n",
      "Time forwards: 1.5774450302124023 \n",
      "Time Backward: 0.15026545524597168            \n",
      "------------------------------\n",
      "\n",
      "Batch size: 512\n",
      "number of total parameters: 55809\n",
      "number of trainable parameters: 55809\n",
      "\n",
      "Max GPU memory allocated: 0.2177119255065918 GB\n",
      "Max GPU memory cached: 0.302734375 GB\n",
      "Max CPU memory allocated: 3.327831268310547 GB\n",
      "Throughput: 68580 sequences /s  \n",
      "Total time: 4.374453783035278 \n",
      "Time per epoch: 0.08914968432212363     \n",
      "Total TimeEpoch 4.368334531784058 and Total TimeBatch: 2.37958025932312. Difference from CPU dataloader : 1.9887542724609375         \n",
      "Time first epoch: 0.2581360340118408 \n",
      "Time Communication: 0.5049498081207275            \n",
      "Time forwards: 0.7393684387207031 \n",
      "Time Backward: 0.06641745567321777            \n",
      "------------------------------\n",
      "\n",
      "Batch size: 1024\n",
      "number of total parameters: 55809\n",
      "number of trainable parameters: 55809\n",
      "\n",
      "Max GPU memory allocated: 0.41788387298583984 GB\n",
      "Max GPU memory cached: 0.623046875 GB\n",
      "Max CPU memory allocated: 3.339672088623047 GB\n",
      "Throughput: 72254 sequences /s  \n",
      "Total time: 4.152018785476685 \n",
      "Time per epoch: 0.08453946210900132     \n",
      "Total TimeEpoch 4.1424336433410645 and Total TimeBatch: 1.5883257389068604. Difference from CPU dataloader : 2.554107904434204         \n",
      "Time first epoch: 0.23560166358947754 \n",
      "Time Communication: 0.6014654636383057            \n",
      "Time forwards: 0.47263193130493164 \n",
      "Time Backward: 0.04187464714050293            \n"
     ]
    }
   ],
   "source": [
    "# Profiling: \n",
    "max_memory = 0\n",
    "# ...\n",
    "    \n",
    "# DataLoader: \n",
    "persistent_workers = True\n",
    "pin_memory = True\n",
    "prefetch_factor = 3\n",
    "drop_last = False\n",
    "# ....\n",
    "\n",
    "# Mixed-Precision : FP16 Tesor Core\n",
    "mixed_precision = True\n",
    "# ...\n",
    "\n",
    "# Inputs : \n",
    "X,Y=  torch.randn(T,N,L),torch.randn(T,N)\n",
    "inputs = CustomDataset(X,Y)\n",
    "# ....\n",
    "\n",
    "for batch_size in [8,16,32,64,128,256,512,1024]:\n",
    "    print(f'------------------------------')\n",
    "    print(f'\\nBatch size: {batch_size}')\n",
    "    (prof,dataloader,sampler) = load_profile_dataloader(inputs,batch_size,num_workers,persistent_workers,pin_memory,prefetch_factor,drop_last)\n",
    "    (loss,model,optimizer) = load_model_loss_opt(H_dims =[64,128,64],C_outs = [64,32,1],kernel_size= kernel_size,c_in=1,device=device,dataparallel = dataparallel,model_name =model_name,input_shape = [6000,40,8],\n",
    "                                                     compile_model = compile_model,memory_format_last = memory_format_last)\n",
    "\n",
    "    train_model(model,optimizer,loss,prof,epochs,dataloader,sampler,device,T,memory_format_last,mixed_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajout de DataParallelism: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/root/anaconda3/envs/pytorch-2.0.1_py-3.10.5/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/root/anaconda3/envs/pytorch-2.0.1_py-3.10.5/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'mp_train' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/root/anaconda3/envs/pytorch-2.0.1_py-3.10.5/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/root/anaconda3/envs/pytorch-2.0.1_py-3.10.5/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'mp_train' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "ProcessExitedException",
     "evalue": "process 1 terminated with exit code 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     cleanup()\n\u001b[1;32m     29\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 30\u001b[0m \u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmp_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParallelised time: \u001b[39m\u001b[38;5;124m'\u001b[39m,time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mt0)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-2.0.1_py-3.10.5/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:239\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    235\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis method only supports start_method=spawn (got: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    236\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTo use a different start_method use:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    237\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m start_method)\n\u001b[1;32m    238\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdaemon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspawn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-2.0.1_py-3.10.5/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-2.0.1_py-3.10.5/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:149\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    141\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with signal \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    142\u001b[0m             (error_index, name),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m             signal_name\u001b[38;5;241m=\u001b[39mname\n\u001b[1;32m    147\u001b[0m         )\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    150\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with exit code \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    151\u001b[0m             (error_index, exitcode),\n\u001b[1;32m    152\u001b[0m             error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[1;32m    153\u001b[0m             error_pid\u001b[38;5;241m=\u001b[39mfailed_process\u001b[38;5;241m.\u001b[39mpid,\n\u001b[1;32m    154\u001b[0m             exit_code\u001b[38;5;241m=\u001b[39mexitcode\n\u001b[1;32m    155\u001b[0m         )\n\u001b[1;32m    157\u001b[0m original_trace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_queues[error_index]\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    158\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- Process \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m error_index\n",
      "\u001b[0;31mProcessExitedException\u001b[0m: process 1 terminated with exit code 1"
     ]
    }
   ],
   "source": [
    "# Profiling: \n",
    "max_memory = 0\n",
    "# ...\n",
    "    \n",
    "num_workers = 1\n",
    "persistent_workers = True\n",
    "pin_memory = True\n",
    "prefetch_factor = 3\n",
    "drop_last = False\n",
    "\n",
    "# DataParallel:\n",
    "dataparallel = True\n",
    "world_size = 2\n",
    "# ...\n",
    "\n",
    "# Inputs : \n",
    "X,Y=  torch.randn(T,N,L),torch.randn(T,N)\n",
    "inputs = CustomDataset(X,Y)\n",
    "# ....\n",
    "\n",
    "\n",
    "def mp_train(rank,world_size,X,Y,B,num_workers, epochs, device):\n",
    "    setup(rank, world_size)\n",
    "    \n",
    "    (prof,dataloader,sampler) = load_profile_dataloader(inputs,B,num_workers,persistent_workers,pin_memory,prefetch_factor,drop_last)\n",
    "    (loss,model,optimizer) = load_model_loss_opt(H_dims =[64,128,64],C_outs = [64,32,1],kernel_size= kernel_size,c_in=1,device=device,dataparallel = dataparallel,model_name =model_name,input_shape = [6000,40,8],\n",
    "                                                     compile_model = compile_model,memory_format_last = memory_format_last,rank = rank, world_size = world_size)\n",
    "\n",
    "    train_model(model,optimizer,loss,prof,epochs,dataloader,sampler,device,T,memory_format_last,mixed_precision)\n",
    "    \n",
    "    cleanup()\n",
    "\n",
    "t0 = time.time()\n",
    "mp.spawn(mp_train, args=(world_size, X, Y, B, num_workers, epochs, device), nprocs=world_size, join=True) \n",
    "print('Parallelised time: ',time.time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajout de Compiler: \n",
    "Inutile, voir plus mauvais ...\n",
    "L'utilisation de 'inductor' semble cependant avoir améliorer le temps de forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/pytorch-2.0.1_py-3.10.5/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1253: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/pytorch-2.0.1_py-3.10.5/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1253: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/pytorch-2.0.1_py-3.10.5/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1253: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/pytorch-2.0.1_py-3.10.5/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1253: UserWarning: Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput: 22414 sequences /s  \n",
      "Total time: 13.384491920471191 \n",
      "Time per epoch: 0.2731508527483259     \n",
      "Total TimeEpoch 13.384391784667969 and Total TimeBatch: 5.277243614196777. Difference from CPU dataloader : 8.107148170471191         \n",
      "Time first epoch: 1.5007538795471191 \n",
      "Time Communication: 6.448937892913818            \n",
      "Time forwards: 2.015937328338623 \n",
      "Time Backward: 0.0968012809753418            \n"
     ]
    }
   ],
   "source": [
    "# Profiling: \n",
    "max_memory = 0\n",
    "# ...\n",
    "    \n",
    "# Compiler :\n",
    "#torch._dynamo.config.suppress_errors = True\n",
    "torch._dynamo.reset()\n",
    "compile_model = True\n",
    "backend = 'cudagraphs'\n",
    "# ...\n",
    "\n",
    "# Inputs : \n",
    "X,Y=  torch.randn(T,N,L),torch.randn(T,N)\n",
    "inputs = CustomDataset(X,Y)\n",
    "# ....\n",
    "\n",
    "\n",
    "(prof,dataloader,sampler) = load_profile_dataloader(inputs,B,num_workers,persistent_workers,pin_memory,prefetch_factor,drop_last)\n",
    "(loss,model,optimizer) = load_model_loss_opt(H_dims =[64,128,64],C_outs = [64,32,1],kernel_size= kernel_size,c_in=1,device=device,dataparallel = False,model_name =model_name,input_shape = [6000,40,8],\n",
    "                                                 compile_model = compile_model,memory_format_last = memory_format_last,compile_backend = backend)\n",
    "\n",
    "train_model(model,optimizer,loss,prof,epochs,dataloader,sampler,device,T,memory_format_last,mixed_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-17 15:13:22,355] torch._dynamo.convert_frame: [ERROR] WON'T CONVERT forward /tmp/ipykernel_260934/332402667.py line 223 \n",
      "due to: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/pytorch-2.0.1_py-3.10.5/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 25, in <module>\n",
      "    from pkg_resources import packaging  # type: ignore[attr-defined]\n",
      "ImportError: cannot import name 'packaging' from 'pkg_resources' (/root/anaconda3/envs/pytorch-2.0.1_py-3.10.5/lib/python3.10/site-packages/pkg_resources/__init__.py)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/pytorch-2.0.1_py-3.10.5/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 675, in call_user_compiler\n",
      "    raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
      "torch._dynamo.exc.BackendCompilerFailed: debug_wrapper raised ImportError: cannot import name 'packaging' from 'pkg_resources' (/root/anaconda3/envs/pytorch-2.0.1_py-3.10.5/lib/python3.10/site-packages/pkg_resources/__init__.py)\n",
      "\n",
      "Set torch._dynamo.config.verbose=True for more information\n",
      "\n",
      "\n",
      "[2024-06-17 15:13:22,375] torch._dynamo.convert_frame: [ERROR] WON'T CONVERT __init__ /root/anaconda3/envs/pytorch-2.0.1_py-3.10.5/lib/python3.10/site-packages/torch/nn/modules/container.py line 276 \n",
      "due to: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/pytorch-2.0.1_py-3.10.5/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 71, in unimplemented\n",
      "    raise Unsupported(msg)\n",
      "torch._dynamo.exc.Unsupported: Guard setup for uninitialized class <class 'torch.nn.modules.container.ModuleList'>\n",
      "\n",
      "Set torch._dynamo.config.verbose=True for more information\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput: 24224 sequences /s  \n",
      "Total time: 12.384198427200317 \n",
      "Time per epoch: 0.25273663657052176     \n",
      "Total TimeEpoch 12.384095191955566 and Total TimeBatch: 4.791990280151367. Difference from CPU dataloader : 7.592104911804199         \n",
      "Time first epoch: 0.63791823387146 \n",
      "Time Communication: 6.234528541564941            \n",
      "Time forwards: 1.3105883598327637 \n",
      "Time Backward: 0.09683609008789062            \n"
     ]
    }
   ],
   "source": [
    "# Profiling: \n",
    "max_memory = 0\n",
    "# ...\n",
    "    \n",
    "# Compiler :\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch._dynamo.reset()\n",
    "compile_model = True\n",
    "backend = 'inductor'\n",
    "# ...\n",
    "\n",
    "# Inputs : \n",
    "X,Y=  torch.randn(T,N,L),torch.randn(T,N)\n",
    "inputs = CustomDataset(X,Y)\n",
    "# ....\n",
    "\n",
    "\n",
    "(prof,dataloader,sampler) = load_profile_dataloader(inputs,B,num_workers,persistent_workers,pin_memory,prefetch_factor,drop_last)\n",
    "(loss,model,optimizer) = load_model_loss_opt(H_dims =[64,128,64],C_outs = [64,32,1],kernel_size= kernel_size,c_in=1,device=device,dataparallel = False,model_name =model_name,input_shape = [6000,40,8],\n",
    "                                                 compile_model = compile_model,memory_format_last = memory_format_last,compile_backend = backend)\n",
    "\n",
    "train_model(model,optimizer,loss,prof,epochs,dataloader,sampler,device,T,memory_format_last,mixed_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profiling: \n",
    "max_memory = 0\n",
    "# ...\n",
    "    \n",
    "# DataLoader: \n",
    "num_workers = 2\n",
    "persistent_workers = True\n",
    "pin_memory = True\n",
    "prefetch_factor = 2\n",
    "drop_last = False\n",
    "# ....\n",
    "\n",
    "# Mixed-Precision : FP16 Tesor Core\n",
    "mixed_precision = False\n",
    "# ...\n",
    "\n",
    "# Inputs : \n",
    "X,Y=  torch.randn(T,N,L),torch.randn(T,N)\n",
    "inputs = CustomDataset(X,Y)\n",
    "# ....\n",
    "\n",
    "\n",
    "(prof,dataloader,sampler) = load_profile_dataloader(inputs,B,num_workers,persistent_workers,pin_memory,prefetch_factor,drop_last)\n",
    "(loss,model,optimizer) = load_model_loss_opt(H_dims =[64,128,64],C_outs = [64,32,1],kernel_size= kernel_size,c_in=1,device=device,dataparallel = False,model_name =model_name,input_shape = [6000,40,8],\n",
    "                                                 compile_model = compile_model,memory_format_last = memory_format_last)\n",
    "\n",
    "train_model(model,optimizer,loss,prof,epochs,dataloader,sampler,device,T,memory_format_last,mixed_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-format channel Last pour le CNN : Uniquement pour des inputs 4D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "required rank 4 tensor to use channels_last format",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m (prof,dataloader,sampler) \u001b[38;5;241m=\u001b[39m load_profile_dataloader(inputs,B,num_workers,persistent_workers,pin_memory,prefetch_factor,drop_last)\n\u001b[1;32m     17\u001b[0m (loss,model,optimizer) \u001b[38;5;241m=\u001b[39m load_model_loss_opt(H_dims \u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m64\u001b[39m],C_outs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m1\u001b[39m],kernel_size\u001b[38;5;241m=\u001b[39m kernel_size,c_in\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,device\u001b[38;5;241m=\u001b[39mdevice,dataparallel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,model_name \u001b[38;5;241m=\u001b[39mmodel_name,input_shape \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m6000\u001b[39m,\u001b[38;5;241m40\u001b[39m,\u001b[38;5;241m8\u001b[39m],\n\u001b[1;32m     18\u001b[0m                                                  compile_model \u001b[38;5;241m=\u001b[39m compile_model,memory_format_last \u001b[38;5;241m=\u001b[39m memory_format_last)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprof\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmemory_format_last\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 183\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, loss, prof, epochs, dataloader, sampler, device, T, memory_format_last, mixed_precision)\u001b[0m\n\u001b[1;32m    181\u001b[0m         (t_epochs,epochs1,t_coms,t_forwards,t_backwards) \u001b[38;5;241m=\u001b[39m training(model,optimizer,loss,prof,epochs,dataloader,device,scaler,memory_format_last,mixed_precision)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m     (t_epochs,t_batchs,epochs1,t_coms,t_forwards,t_backwards) \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprof\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmemory_format_last\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m total_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m total_t\n\u001b[1;32m    186\u001b[0m throughput \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{:.0f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(T\u001b[38;5;241m*\u001b[39mepochs\u001b[38;5;241m/\u001b[39mtotal_time)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sequences /s \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[52], line 111\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(model, optimizer, loss, prof, epochs, dataloader, device, scaler, memory_format_last, mixed_precision)\u001b[0m\n\u001b[1;32m    109\u001b[0m t_com \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m  memory_format_last:\n\u001b[0;32m--> 111\u001b[0m     x,y \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannels_last\u001b[49m\u001b[43m)\u001b[49m,x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     x,y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device),y\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: required rank 4 tensor to use channels_last format"
     ]
    }
   ],
   "source": [
    "# Profiling: \n",
    "max_memory = 0\n",
    "# ...\n",
    "    \n",
    "    \n",
    "# Memory format = channel_last:\n",
    "memory_format_last = True\n",
    "# ....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test avec different worker, sur model moyen (petit), input shape proche des miens : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1569 sequences /s '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blabla=  1568.73849\n",
    "f\"{'{:.0f}'.format(blabla)} sequences /s \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'CNN'\n",
    "\n",
    "B = 8\n",
    "T = 6000\n",
    "L = 8\n",
    "N = 40\n",
    "epochs = 300\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "c_in,h_dim1,h_dim2,c_out = L, 64, 64, 1\n",
    "\n",
    "persistent_workers = False\n",
    "pin_memory = False\n",
    "prefetch_factor = None\n",
    "drop_last = False\n",
    "\n",
    "X,Y=  torch.randn(T,N,L),torch.randn(T,N)\n",
    "#inputs = list(zip(X,Y))\n",
    "inputs = CustomDataset(X,Y)\n",
    "\n",
    "for num_workers in [0,1,2,4,6,8]:\n",
    "    print('\\nNum workers:',num_workers)\n",
    "    (prof,dataloader,sampler) = load_profile_dataloader(inputs,B,num_workers,persistent_workers,pin_memory,prefetch_factor,drop_last)\n",
    "    if model_name == 'MLP':\n",
    "        (loss,model,optimizer) = load_model_loss_opt(c_in,h_dim1,h_dim2,c_out,device,model_name =model_name)\n",
    "    if model_name == 'CNN':\n",
    "        # kernel_size= (1,2)\n",
    "        kernel_size = (2,)\n",
    "        (loss,model,optimizer) = load_model_loss_opt(H_dims =[64,128,64],C_outs = [64,32,1],kernel_size= kernel_size,c_in=1,device=device,dataparallel = False,model_name =model_name,input_shape = [6000,40,8])\n",
    "        \n",
    "    train_model(model,optimizer,loss,prof,epochs,dataloader,sampler,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essaie avec toute les données chargées initialement en mémoire : \n",
    "Ici, impossible de les charger en mémoire en amont pour du num_worker > 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Num workers: 0\n",
      "Total time: 0.634476900100708 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 0.634458065032959 \n",
      "Time Communication: 0.0035734176635742188            \n",
      "Time forwards: 0.11711645126342773 \n",
      "Time Backward: 0.038048505783081055            \n"
     ]
    }
   ],
   "source": [
    "B = 8\n",
    "T = 6000\n",
    "L = 8\n",
    "N = 40\n",
    "epochs = 300\n",
    "\n",
    "persistent_workers = False\n",
    "pin_memory = False\n",
    "prefetch_factor = None\n",
    "drop_last = False\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "c_in,h_dim1,h_dim2,c_out = L, 64, 64, 1\n",
    "\n",
    "X,Y=  torch.randn(T,N,L).to(device),torch.randn(T,N,1).to(device)\n",
    "inputs = CustomDataset(X,Y)\n",
    "\n",
    "for num_workers in [0]:\n",
    "    print('\\nNum workers:',num_workers)\n",
    "    (prof,dataloader,sampler) = load_profile_dataloader(inputs,B,num_workers,persistent_workers,pin_memory,prefetch_factor,drop_last)\n",
    "    (loss,model,optimizer) = load_model_loss_opt(c_in,h_dim1,h_dim2,c_out,device)\n",
    "    train_model(model,optimizer,loss,prof,epochs,dataloader,sampler,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajout de : persistent_worker, pin_memory, prefetch_factor = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Num workers: 0 and prefetch_factor:  None\n",
      "Total time: 0.7066452503204346 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 0.7066271305084229 \n",
      "Time Communication: 0.03134489059448242            \n",
      "Time forwards: 0.11990141868591309 \n",
      "Time Backward: 0.03914356231689453            \n",
      "\n",
      "Num workers: 0 and prefetch_factor:  2\n",
      "Total time: 0.6967222690582275 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 0.6967051029205322 \n",
      "Time Communication: 0.031247854232788086            \n",
      "Time forwards: 0.1203000545501709 \n",
      "Time Backward: 0.03901362419128418            \n",
      "\n",
      "Num workers: 0 and prefetch_factor:  4\n",
      "Total time: 0.6892411708831787 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 0.6892240047454834 \n",
      "Time Communication: 0.031081199645996094            \n",
      "Time forwards: 0.11827659606933594 \n",
      "Time Backward: 0.0383143424987793            \n",
      "\n",
      "Num workers: 0 and prefetch_factor:  8\n",
      "Total time: 0.6936221122741699 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 0.6936051845550537 \n",
      "Time Communication: 0.030835866928100586            \n",
      "Time forwards: 0.11847186088562012 \n",
      "Time Backward: 0.038513898849487305            \n",
      "\n",
      "Num workers: 1 and prefetch_factor:  None\n",
      "Total time: 1.2808942794799805 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 1.2808711528778076 \n",
      "Time Communication: 0.10613560676574707            \n",
      "Time forwards: 0.142730712890625 \n",
      "Time Backward: 0.04854464530944824            \n",
      "\n",
      "Num workers: 1 and prefetch_factor:  2\n",
      "Total time: 1.2782480716705322 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 1.278212308883667 \n",
      "Time Communication: 0.043892860412597656            \n",
      "Time forwards: 0.14061403274536133 \n",
      "Time Backward: 0.049520015716552734            \n",
      "\n",
      "Num workers: 1 and prefetch_factor:  4\n",
      "Total time: 1.2846832275390625 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 1.2846577167510986 \n",
      "Time Communication: 0.10344767570495605            \n",
      "Time forwards: 0.14364266395568848 \n",
      "Time Backward: 0.0494537353515625            \n",
      "\n",
      "Num workers: 1 and prefetch_factor:  8\n",
      "Total time: 1.3007986545562744 \n",
      "Time per epoch: 0.0         \n",
      "Time first epoch: 1.3007662296295166 \n",
      "Time Communication: 0.07886457443237305            \n",
      "Time forwards: 0.14497876167297363 \n",
      "Time Backward: 0.051529645919799805            \n"
     ]
    }
   ],
   "source": [
    "persistent_workers = True\n",
    "pin_memory = True\n",
    "drop_last = True\n",
    "\n",
    "X,Y=  torch.randn(T,N,L),torch.randn(T,N,1)\n",
    "inputs = CustomDataset(X,Y)\n",
    "\n",
    "for num_workers in [0,1]:\n",
    "    for prefetch_factor in [None,2,4,8]:\n",
    "        print('\\nNum workers:',num_workers,'and prefetch_factor: ',prefetch_factor)\n",
    "        (prof,dataloader,sampler) = load_profile_dataloader(inputs,B,num_workers,persistent_workers,pin_memory,prefetch_factor,drop_last)\n",
    "        (loss,model,optimizer) = load_model_loss_opt(c_in,h_dim1,h_dim2,c_out,device)\n",
    "        train_model(model,optimizer,loss,prof,epochs,dataloader,sampler,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of best config: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers= 1 #4\n",
    "persistent_workers = True\n",
    "pin_memory = True\n",
    "prefetch_factor = 4\n",
    "drop_last = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial with Dataparallel : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "\n",
    "def find_free_port():\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.bind(('', 0))\n",
    "    addr, port = s.getsockname()\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "free_port = find_free_port()\n",
    "os.environ['MASTER_ADDR'] = '137.121.170.69'\n",
    "os.environ['MASTER_PORT'] = 8888 #Ne fonctionne pas #str(free_port) \n",
    "print(f\"Using port {free_port} for MASTER_PORT\")\n",
    "\n",
    "dataparallel = True\n",
    "\n",
    "(prof,dataloader,sampler) = load_profile_dataloader(inputs,B,num_workers,persistent_workers,pin_memory,prefetch_factor,drop_last,dataparallel)\n",
    "(loss,model,optimizer) = load_model_loss_opt(c_in,h_dim1,h_dim2,c_out,device,dataparallel)\n",
    "train_model(model,optimizer,loss,prof,epochs,dataloader,sampler,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-2.0.1_py-3.10.5]",
   "language": "python",
   "name": "conda-env-pytorch-2.0.1_py-3.10.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
